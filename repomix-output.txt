This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
__init__.py
.claudeignore
.claudesync/config.local.json
.clinerules
.gitignore
api/__init__.py
api/auth/service.py
api/config.py
api/main.py
api/middleware/rate_limiter.py
api/models/auth.py
api/models/dashboard.py
api/models/emails.py
api/models/errors.py
api/README.md
api/routes/__init__.py
api/routes/auth.py
api/routes/dashboard.py
api/routes/emails.py
api/services/__init__.py
api/services/dashboard_service.py
api/services/email_service.py
api/tests/unit/test_auth.py
api/utils/error_handlers.py
bcrypt_fix.py
data/config/email_settings.json
data/email_responses.json
data/metrics/email_stats.json
data/metrics/groq_metrics.json
docs/analysis-pipeline.md
docs/assistant-instructions.md
docs/classification-categories.md
docs/email-processing-rules.md
docs/error-handling.md
docs/existing-infrastructure.md
docs/future-enhancements.md
docs/openai.md
docs/project_documentation.md
docs/response-management.md
docs/structured_outputs.md
docs/system-instructions.md
email_responses.json
emails.txt
groq_metrics.json
main.py
meeting_analyzer.py
meeting_mails.json
memory-bank/activeContext.md
memory-bank/productContext.md
memory-bank/progress.md
memory-bank/projectbrief.md
memory-bank/systemPatterns.md
memory-bank/techContext.md
pre_startup.py
processors/__init__.py
processors/content_processor.py
requirements.txt
run_api.py
schedule.json
setup_directories.py
setup.ps1
src/__init__.py
src/auth/google_oauth.py
src/auth/microsoft_oauth.py
src/auth/oauth_base.py
src/auth/oauth_factory.py
src/auth/service.py
src/config/analyzer_config.py
src/email_processing/__init__.py
src/email_processing/analyzers/deepseek.py
src/email_processing/analyzers/llama.py
src/email_processing/analyzers/meeting.py
src/email_processing/analyzers/response_categorizer.py
src/email_processing/base.py
src/email_processing/classification/classifier.py
src/email_processing/handlers/content.py
src/email_processing/handlers/date_service.py
src/email_processing/handlers/sorter.py
src/email_processing/handlers/writer.py
src/email_processing/models.py
src/email_processing/processor.py
src/integrations/__init__.py
src/integrations/gmail/auth_manager.py
src/integrations/gmail/client.py
src/integrations/groq/__init__.py
src/integrations/groq/client_wrapper.py
src/integrations/groq/client.py
src/integrations/groq/constants.py
src/integrations/groq/model_manager.py
src/storage/__init__.py
src/storage/database.py
src/storage/encryption.py
src/storage/models.py
src/storage/secure.py
src/storage/user_repository.py
src/utils/date_utils.py
startup_script.py
unicode_safe_logging.py
updated_pre_startup.py

================================================================
Files
================================================================

================
File: __init__.py
================
# src/__init__.py
"""
Sentient Inbox system root package.

Provides centralized access to core system components while maintaining
proper dependency management and component isolation.
"""

from email_processing import EmailProcessor
from email_processing.classification import EmailClassifier, EmailTopic
from integrations.gmail import GmailClient
from integrations.groq import GroqClient
from storage import SecureStorage

__version__ = '1.0.0'

__all__ = [
    'EmailProcessor',
    'EmailClassifier',
    'EmailTopic',
    'GmailClient',
    'GroqClient',
    'SecureStorage'
]

================
File: .claudeignore
================
# Explicitly exclude frontend directory
frontend/
G:/git_repos/Sentient-Inbox/frontend/
/frontend/

# Common development files
*.log
*.pyc
__pycache__/
.pytest_cache/
*.pyo
*.pyd
.coverage
htmlcov/
.tox/
.nox/
.hypothesis/

# Environment and configuration
.env
.venv/
env/
venv/
ENV/
.python-version
.env.*
*.env

# IDE files
.idea/
.vscode/
*.swp
*.swo
.DS_Store

# Project-specific files that may contain sensitive data
*.pem
*.key
client_secret*.json
token*.json
credentials*.json
.keys/
*.bin

# Build and distribution
build/
dist/
*.egg-info/
*.egg
sdist/
wheels/

# Database files
*.db
*.sqlite3

# Temporary files and backups
data/secure/backups/
*.bak
*.tmp
temp/
tmp/

# Git-related
.git/
.gitignore
.gitmodules
.github/

# Project-specific based on your repository
email_responses.json
schedule.json
meeting_mails.json
groq_metrics.json

================
File: .claudesync/config.local.json
================
{
  "active_provider": "claude.ai",
  "local_path": "C:\\Users\\i_tsvetkov\\Documents\\projects\\Sentient-Inbox-Backend",
  "active_organization_id": "c5f918fc-f725-4c50-b208-69525465f70a",
  "active_project_id": "da34e997-b0f0-46df-9bee-e901273e882c",
  "active_project_name": "Sentient-Inbox-Backend"
}

================
File: .clinerules
================
# Project Guidelines

## Security Standards
DO NOT read or modify:
- .env files
- */config/secrets.*
- token.json
- client_secret.json
- Any file containing API keys or credentials

## Documentation Requirements
- Update relevant documentation in /memory-bank when modifying features
- Keep README.md in sync with new capabilities
- Maintain changelog entries in CHANGELOG.md

## Code Style & Patterns
- Use Python type hints consistently
- Follow PEP 8 guidelines
- Place AI processing logic in dedicated agent classes
- Use repository pattern for data access
- Follow error handling patterns in enhanced_groq_client.py

## Testing Standards
- Unit tests required for agent logic
- Integration tests for email processing
- Security compliance tests
- Error handling test cases

## Project Structure
- Maintain agent separation of concerns
- Keep configuration in dedicated files
- Store logs in /logs directory
- Cache files in /data/cache

## Deployment Practices
- Never commit sensitive files
- Use environment variables for configuration
- Keep dependencies updated
- Maintain clean git history

================
File: .gitignore
================
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
env/
ENV/
.env

# IDE
.idea/
.vscode/
*.swp
*.swo
.DS_Store

# Logs
*.log
logs/
log/

# Local development
*.db
*.sqlite3

# Testing
.coverage
htmlcov/
.tox/
.pytest_cache/
coverage.xml
*.cover

# Distribution
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/

# Documentation
docs/_build/
doc/_build/

# Jupyter Notebook
.ipynb_checkpoints

# Credentials and secrets
*.pem
*.key
client_secret*.json
token*.json
credentials*.json


# Backup files
data/secure/backups/
*.bin

# Frontend
/frontend/**
/node_modules
/dist
/build
/public

# Frontend development
*.lock
package-lock.json
yarn.lock
.env.local
.env.development.local
.env.test.local
.env.production.local

================
File: api/__init__.py
================
"""
API Package Initialization

Provides FastAPI application with comprehensive route management,
authentication, and service integration.

Design Considerations:
- Clean package organization
- Proper dependency management
- Secure authentication
- Comprehensive error handling
"""

from api.routes import auth, emails, dashboard
from api.services import email_service
from api.models import emails as email_models
from api.config import get_settings

__all__ = [
    'auth',
    'emails',
    'dashboard',
    'email_service',
    'email_models',
    'get_settings'
]

================
File: api/auth/service.py
================
"""
Authentication Service Implementation with OAuth Support

Provides comprehensive authentication services including token generation,
validation, user credential verification, and OAuth authentication.

Design Considerations:
- Secure token handling with proper cryptographic practices
- OAuth integration with multiple providers
- Comprehensive error handling and logging
- Clear separation of concerns
- Future extensibility for different authentication methods
"""

import os
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Union, Any, Tuple

from jose import jwt
from passlib.context import CryptContext
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
import aiohttp

from api.config import get_settings
from api.models.auth import TokenData, UserCredentials, Token
from src.storage.user_repository import UserRepository
from src.auth.oauth_factory import OAuthProviderFactory

# Configure logging
logger = logging.getLogger(__name__)

# Initialize security components
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")


class AuthenticationService:
    """
    Comprehensive authentication service with secure token management and OAuth integration.
    
    Implements secure user authentication, token generation and validation,
    comprehensive permission management, and OAuth provider integration for
    multiple identity providers.
    """
    
    def __init__(self):
        """
        Initialize authentication service with configuration settings.
        
        Loads security configuration and sets up authentication context
        with proper error handling and validation.
        """
        self.settings = get_settings()
        self.secret_key = self.settings.JWT_SECRET_KEY.get_secret_value()
        self.algorithm = self.settings.JWT_ALGORITHM
        self.access_token_expire_minutes = self.settings.JWT_TOKEN_EXPIRE_MINUTES
        
        # OAuth provider factory
        self.oauth_factory = OAuthProviderFactory
        
        # Load legacy users for backward compatibility
        # These will be migrated to the database on first access
        self.legacy_users_db = {
            "admin": {
                "username": "admin",
                "hashed_password": pwd_context.hash("securepassword"),
                "email": "admin@example.com",
                "permissions": ["admin", "process", "view"]
            },
            "viewer": {
                "username": "viewer",
                "hashed_password": pwd_context.hash("viewerpass"),
                "email": "viewer@example.com",
                "permissions": ["view"]
            }
        }
        
        logger.info("Authentication service initialized with OAuth provider integration")
    
    def verify_password(self, plain_password: str, hashed_password: str) -> bool:
        """
        Verify password against stored hash with proper cryptographic verification.
        
        Args:
            plain_password: Plain text password to verify
            hashed_password: Stored password hash
            
        Returns:
            Boolean indicating if password matches
        """
        return pwd_context.verify(plain_password, hashed_password)
    
    def get_password_hash(self, password: str) -> str:
        """
        Generate secure password hash with proper cryptographic practices.
        
        Args:
            password: Plain text password to hash
            
        Returns:
            Secure password hash
        """
        return pwd_context.hash(password)
    
    async def authenticate_user(self, username: str, password: str) -> Optional[Dict[str, Any]]:
        """
        Authenticate user against stored credentials with secure verification.
        
        Implements comprehensive authentication with proper error handling
        and secure password verification using cryptographic best practices.
        
        Args:
            username: Username to authenticate
            password: Password to verify
            
        Returns:
            User data if authenticated, None otherwise
        """
        # First try to get user from database
        user = await UserRepository.get_user_by_username(username)
        
        if not user:
            # Check legacy users
            legacy_user = self.legacy_users_db.get(username)
            if not legacy_user:
                logger.warning(f"Authentication attempt for unknown user: {username}")
                return None
                
            # Verify password for legacy user
            if not self.verify_password(password, legacy_user["hashed_password"]):
                logger.warning(f"Failed password verification for legacy user: {username}")
                return None
                
            # Create user in database
            try:
                user = await UserRepository.create_user(
                    email=legacy_user["email"],
                    username=legacy_user["username"],
                    display_name=legacy_user.get("display_name"),
                    permissions=legacy_user["permissions"],
                    profile_picture=legacy_user.get("profile_picture")
                )
                logger.info(f"Migrated legacy user to database: {username}")
            except Exception as e:
                logger.error(f"Failed to migrate legacy user {username}: {str(e)}")
                # Return legacy user data
                return legacy_user
        else:
            # We don't support password authentication for database users
            # They should use OAuth
            logger.warning(f"Password authentication attempted for OAuth user: {username}")
            return None
        
        logger.info(f"User authenticated successfully: {username}")
        return user.to_dict()
    
    async def get_authorization_url(self, provider: str, redirect_uri: str) -> Tuple[str, str]:
        """
        Get authorization URL for OAuth provider.
        
        Args:
            provider: OAuth provider name
            redirect_uri: Redirect URI for callback
            
        Returns:
            Tuple of (authorization_url, state)
            
        Raises:
            ValueError: If provider is not supported
        """
        try:
            oauth_provider = self.oauth_factory.get_provider(provider)
            return await oauth_provider.get_authorization_url(redirect_uri)
        except Exception as e:
            logger.error(f"Error getting authorization URL for provider {provider}: {str(e)}")
            raise ValueError(f"Failed to get authorization URL: {str(e)}")
    
    async def process_oauth_callback(self, provider: str, code: str, redirect_uri: str) -> Dict[str, Any]:
        """
        Process OAuth callback and create or update user.
        
        Args:
            provider: OAuth provider name
            code: Authorization code
            redirect_uri: Redirect URI used in authorization request
            
        Returns:
            Dictionary containing user information and access token
            
        Raises:
            ValueError: If OAuth callback processing fails
        """
        try:
            # Get OAuth provider
            oauth_provider = self.oauth_factory.get_provider(provider)
            
            # Exchange code for tokens
            tokens = await oauth_provider.exchange_code_for_tokens(code, redirect_uri)
            
            # Get or create user
            user = await self._get_or_create_oauth_user(provider, tokens)
            
            # Save OAuth tokens
            await UserRepository.save_oauth_token(
                user_id=user["id"],
                provider=provider,
                provider_user_id=tokens["provider_user_id"],
                provider_email=tokens["provider_email"],
                access_token=tokens["access_token"],
                refresh_token=tokens.get("refresh_token"),
                expires_in=tokens["expires_in"],
                scopes=tokens["scopes"]
            )
            
            # Update last login timestamp
            await UserRepository.update_user_last_login(user["id"])
            
            # Generate JWT token
            access_token = self.create_access_token(
                data={
                    "username": user["username"],
                    "permissions": user["permissions"]
                }
            )
            
            return {
                "user": user,
                "access_token": access_token,
                "token_type": "bearer",
                "expires_in": self.access_token_expire_minutes * 60
            }
            
        except Exception as e:
            logger.error(f"Error processing OAuth callback: {str(e)}")
            raise ValueError(f"Failed to process OAuth callback: {str(e)}")
    
    async def _get_or_create_oauth_user(self, provider: str, tokens: Dict[str, Any]) -> Dict[str, Any]:
        """
        Get existing user or create a new one based on OAuth profile.
        
        Args:
            provider: OAuth provider name
            tokens: Token data including user profile
            
        Returns:
            User data dictionary
        """
        provider_user_id = tokens["provider_user_id"]
        provider_email = tokens["provider_email"]
        user_info = tokens["user_info"]
        
        # Try to find user by OAuth provider and ID
        user = await UserRepository.get_user_by_oauth(provider, provider_user_id)
        
        if user:
            # User exists, return data
            logger.info(f"Found existing user for {provider} ID {provider_user_id}")
            return user.to_dict()
            
        # Try to find user by email
        user = await UserRepository.get_user_by_email(provider_email)
        
        if user:
            # User exists with this email, link OAuth account
            logger.info(f"Linking {provider} account to existing user: {user.username}")
            # User will be linked when we save the OAuth token
            return user.to_dict()
            
        # Create new user
        # Generate username from email
        email_username = provider_email.split('@')[0]
        base_username = email_username.lower()
        username = base_username
        
        # Check if username exists
        attempts = 0
        while await UserRepository.get_user_by_username(username):
            attempts += 1
            username = f"{base_username}{attempts}"
            
        # Get display name from provider-specific fields
        display_name = None
        if provider == "google":
            display_name = user_info.get("name")
        elif provider == "microsoft":
            display_name = user_info.get("displayName")
            
        # Create new user with view permission
        try:
            user = await UserRepository.create_user(
                email=provider_email,
                username=username,
                display_name=display_name,
                permissions=["view"],
                profile_picture=user_info.get("picture")
            )
            logger.info(f"Created new user from {provider} authentication: {username}")
            return user.to_dict()
        except Exception as e:
            logger.error(f"Failed to create user from OAuth profile: {str(e)}")
            raise ValueError(f"Failed to create user: {str(e)}")
    
    def create_access_token(self, data: Dict[str, Any], expires_delta: Optional[timedelta] = None) -> str:
        """
        Create JWT access token with proper security practices.
        
        Generates secure JWT token with appropriate claims and expiration
        using cryptographic best practices.
        
        Args:
            data: Token payload data
            expires_delta: Optional custom expiration time
            
        Returns:
            Encoded JWT token string
        """
        to_encode = data.copy()
        
        # Set token expiration
        if expires_delta:
            expire = datetime.utcnow() + expires_delta
        else:
            expire = datetime.utcnow() + timedelta(minutes=self.access_token_expire_minutes)
        
        to_encode.update({"exp": expire})
        
        # Generate token with proper security
        try:
            encoded_jwt = jwt.encode(
                to_encode, 
                self.secret_key, 
                algorithm=self.algorithm
            )
            return encoded_jwt
        except Exception as e:
            logger.error(f"Token generation error: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Could not generate authentication token"
            )
    
    async def get_current_user(self, token: str = Depends(oauth2_scheme)) -> Dict[str, Any]:
        """
        Validate token and extract current user with proper security validation.
        
        Implements comprehensive token validation with proper error handling
        and security verification of token claims.
        
        Args:
            token: JWT token to validate
            
        Returns:
            Validated user data
            
        Raises:
            HTTPException: If token is invalid or expired
        """
        credentials_exception = HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication credentials",
            headers={"WWW-Authenticate": "Bearer"},
        )
        
        try:
            # Decode and validate token
            payload = jwt.decode(
                token, 
                self.secret_key, 
                algorithms=[self.algorithm]
            )
            
            # Extract user information
            username = payload.get("username")
            if username is None:
                logger.warning("Token missing username claim")
                raise credentials_exception
            
            permissions = payload.get("permissions", [])
            token_data = TokenData(username=username, permissions=permissions, exp=payload["exp"])
            
        except jwt.JWTError as e:
            logger.warning(f"Token validation error: {str(e)}")
            raise credentials_exception
        
        # Verify user exists in database
        user = await UserRepository.get_user_by_username(token_data.username)
        
        if user:
            return user.to_dict()
            
        # Check legacy users (for backward compatibility)
        legacy_user = self.legacy_users_db.get(token_data.username)
        if legacy_user:
            # Create user in database
            try:
                user = await UserRepository.create_user(
                    email=legacy_user["email"],
                    username=legacy_user["username"],
                    display_name=legacy_user.get("display_name"),
                    permissions=legacy_user["permissions"],
                    profile_picture=legacy_user.get("profile_picture")
                )
                logger.info(f"Migrated legacy user to database during token validation: {token_data.username}")
                return user.to_dict()
            except Exception as e:
                logger.error(f"Failed to migrate legacy user {token_data.username}: {str(e)}")
                # Return legacy user data
                return legacy_user
                
        # User not found in database or legacy storage
        logger.warning(f"Token contains unknown user: {token_data.username}")
        raise credentials_exception
    
    async def get_current_user_permissions(self, user: Dict[str, Any] = Depends(get_current_user)) -> List[str]:
        """
        Extract permissions from authenticated user.
        
        Args:
            user: Authenticated user data
            
        Returns:
            List of user permissions
        """
        return user.get("permissions", [])
    
    async def check_permission(self, required_permission: str, user: Dict[str, Any] = Depends(get_current_user)) -> bool:
        """
        Check if user has required permission with proper authorization verification.
        
        Implements comprehensive permission checking with proper error handling
        and security validation.
        
        Args:
            required_permission: Permission to check
            user: Authenticated user data
            
        Returns:
            Boolean indicating if user has permission
            
        Raises:
            HTTPException: If user lacks required permission
        """
        user_permissions = user.get("permissions", [])
        
        if required_permission not in user_permissions:
            logger.warning(
                f"Permission denied: {user['username']} lacks {required_permission} permission"
            )
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Not authorized for {required_permission}",
            )
        
        return True
    

# Singleton instance for dependency injection
auth_service = AuthenticationService()

# Common dependencies for route handlers
def get_auth_service() -> AuthenticationService:
    """Provide authentication service instance for dependency injection."""
    return auth_service

async def require_admin(
    auth_service: AuthenticationService = Depends(get_auth_service),
    user: Dict[str, Any] = Depends(auth_service.get_current_user)
) -> bool:
    """Require admin permission for route access."""
    return await auth_service.check_permission("admin", user)

async def require_process(
    auth_service: AuthenticationService = Depends(get_auth_service),
    user: Dict[str, Any] = Depends(auth_service.get_current_user)
) -> bool:
    """Require process permission for route access."""
    return await auth_service.check_permission("process", user)

async def require_view(
    auth_service: AuthenticationService = Depends(get_auth_service),
    user: Dict[str, Any] = Depends(auth_service.get_current_user)
) -> bool:
    """Require view permission for route access."""
    return await auth_service.check_permission("view", user)

================
File: api/config.py
================
"""
API Configuration Management

Provides centralized configuration handling with environment-aware settings
management, secure secret retrieval, and comprehensive validation.

Design Considerations:
- Environment-specific configuration profiles
- Secure handling of sensitive information
- Comprehensive configuration validation
- Default values with proper documentation
"""

import os
from enum import Enum
from typing import Dict, Any, Optional, List

# Import BaseSettings from pydantic_settings instead of pydantic
from pydantic_settings import BaseSettings
from pydantic import Field, SecretStr, field_validator


class EnvironmentType(str, Enum):
    """Valid environment types for configuration context."""
    DEVELOPMENT = "development"
    TESTING = "testing"
    PRODUCTION = "production"


class APISettings(BaseSettings):
    """
    API configuration settings with environment-specific defaults and validation.
    
    Implements a comprehensive configuration system using Pydantic for validation,
    environment variable loading, and secure secret management. Provides safe defaults
    where appropriate while requiring critical security configurations.
    """
    # Environment Configuration
    ENVIRONMENT: EnvironmentType = Field(
        default=EnvironmentType.DEVELOPMENT,
        description="Runtime environment context"
    )
    DEBUG: bool = Field(
        default=False,
        description="Enable debug mode"
    )
    LOG_LEVEL: str = Field(
        default="INFO",
        description="Logging level"
    )
    
    # API Settings
    API_TITLE: str = Field(
        default="Email Management API",
        description="API title for documentation"
    )
    API_DESCRIPTION: str = Field(
        default="API for sophisticated email analysis and response management",
        description="API description for documentation"
    )
    API_VERSION: str = Field(
        default="1.0.0",
        description="API version"
    )
    
    # Security Settings
    JWT_SECRET_KEY: SecretStr = Field(
        default="insecure_development_key_do_not_use_in_production_1234567890",  # Default for development
        description="Secret key for JWT token generation and validation"
    )
    JWT_ALGORITHM: str = Field(
        default="HS256",
        description="Algorithm used for JWT token generation"
    )
    JWT_TOKEN_EXPIRE_MINUTES: int = Field(
        default=30,
        description="JWT token expiration time in minutes"
    )
    
    # CORS Settings
    CORS_ORIGINS: str = Field(
        default="*",
        description="Comma-separated list of allowed origins for CORS"
    )
    CORS_METHODS: str = Field(
        default="GET,POST,PUT,DELETE,OPTIONS",
        description="Comma-separated list of allowed methods for CORS"
    )
    
    # Rate Limiting
    RATE_LIMIT_WINDOW_SECONDS: int = Field(
        default=60,
        description="Rate limit window in seconds"
    )
    RATE_LIMIT_MAX_REQUESTS: int = Field(
        default=100,
        description="Maximum requests per window"
    )

    # Use field_validator instead of validator in Pydantic V2
    @field_validator("CORS_ORIGINS")
    @classmethod
    def parse_cors_origins(cls, value: str) -> list:
        """Parse comma-separated CORS origins into list."""
        if value == "*":
            return ["*"]
        return [origin.strip() for origin in value.split(",") if origin.strip()]

    @field_validator("CORS_METHODS")
    @classmethod
    def parse_cors_methods(cls, value: str) -> list:
        """Parse comma-separated CORS methods into list."""
        return [method.strip() for method in value.split(",") if method.strip()]

    @field_validator("JWT_SECRET_KEY")
    @classmethod
    def validate_jwt_secret(cls, value: SecretStr) -> SecretStr:
        """Validate JWT secret key meets minimum security requirements."""
        if len(value.get_secret_value()) < 32:
            raise ValueError("JWT secret key must be at least 32 characters long")
        return value

    # Pydantic V2 uses model_config instead of Config class
    model_config = {
        "env_file": ".env",
        "env_file_encoding": "utf-8",
        "case_sensitive": True,
        "extra": "ignore"
    }


def get_settings() -> APISettings:
    """
    Retrieve validated API settings with environment-specific configuration.
    
    Implements proper configuration loading with environment awareness
    and comprehensive validation to ensure all required settings are
    properly specified.
    
    Returns:
        Validated API settings object
    
    Raises:
        ValidationError: If configuration fails validation
    """
    # For development/testing, generate a secret if not provided
    env = os.getenv("ENVIRONMENT", "development")
    if env in ["development", "testing"]:
        os.environ.setdefault(
            "JWT_SECRET_KEY",
            "insecure_development_key_do_not_use_in_production_1234567890"
        )
        
    # Load and validate settings
    return APISettings()

================
File: api/main.py
================
"""
API Application Entry Point

Defines the main FastAPI application with comprehensive middleware,
route configuration, and lifecycle management.

Design Considerations:
- Proper middleware ordering for optimal processing
- Comprehensive error handling
- Structured route organization
- Clean dependency management
"""

import logging
import os
from fastapi import FastAPI, Depends
from fastapi.middleware.cors import CORSMiddleware

from api.config import get_settings, EnvironmentType
from api.middleware.rate_limiter import RateLimiter
from api.utils.error_handlers import add_exception_handlers
from api.routes import auth, emails, dashboard

# Configure logging
logging.basicConfig(
    level=getattr(logging, os.getenv("LOG_LEVEL", "INFO")),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger("api")

# Create application
def create_application() -> FastAPI:
    """
    Create and configure FastAPI application instance.
    
    Implements comprehensive application setup with proper middleware
    configuration, route registration, and error handling.
    
    Returns:
        Configured FastAPI application
    """
    settings = get_settings()
    
    # Create application with configuration
    app = FastAPI(
        title=settings.API_TITLE,
        description=settings.API_DESCRIPTION,
        version=settings.API_VERSION,
        docs_url="/docs" if settings.ENVIRONMENT != EnvironmentType.PRODUCTION else None,
        redoc_url="/redoc" if settings.ENVIRONMENT != EnvironmentType.PRODUCTION else None,
        debug=settings.DEBUG
    )
    
    # Add CORS middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=settings.CORS_ORIGINS,
        allow_credentials=True,
        allow_methods=settings.CORS_METHODS,
        allow_headers=["*"],
    )
    
    # Add rate limiting middleware
    app.add_middleware(RateLimiter)
    
    # Add exception handlers
    add_exception_handlers(app)
    
    # Include routers
    app.include_router(auth.router)
    app.include_router(emails.router)
    app.include_router(dashboard.router)
    
    # Add startup and shutdown events
    @app.on_event("startup")
    async def startup_event():
        """Perform initialization tasks on application startup."""
        logger.info("API service starting up")
    
    @app.on_event("shutdown")
    async def shutdown_event():
        """Perform cleanup tasks on application shutdown."""
        logger.info("API service shutting down")
    
    logger.info(f"Application initialized in {settings.ENVIRONMENT} environment")
    return app


# Create application instance
app = create_application()


# Simple health check endpoint
@app.get("/health", tags=["Monitoring"])
async def health_check():
    """API health check endpoint."""
    return {"status": "healthy"}

================
File: api/middleware/rate_limiter.py
================
"""
Rate Limiting Middleware Implementation

Provides comprehensive rate limiting functionality to protect
API resources from excessive usage and potential abuse.

Design Considerations:
- Efficient request counting with proper time window management
- Configurable rate limits with environment-specific defaults
- Clear response headers for client guidance
- Comprehensive error handling and logging
"""

import time
import logging
from datetime import datetime
from typing import Dict, List, Optional, Callable

from fastapi import FastAPI, Request, Response
# Updated import path for BaseHTTPMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.status import HTTP_429_TOO_MANY_REQUESTS

from api.config import get_settings

# Configure logging
logger = logging.getLogger(__name__)

class RateLimiter(BaseHTTPMiddleware):
    """
    Rate limiting middleware with configurable limits and window size.
    
    Implements comprehensive request rate limiting with proper window
    management, client identification, and response headers for guidance.
    """
    
    def __init__(
        self, 
        app: FastAPI, 
        window_seconds: int = None,
        max_requests: int = None
    ):
        """
        Initialize rate limiter with configurable settings.
        
        Args:
            app: FastAPI application
            window_seconds: Rate limit window in seconds
            max_requests: Maximum requests per window
        """
        super().__init__(app)
        settings = get_settings()
        
        # Configure rate limiting parameters
        self.window_seconds = window_seconds or settings.RATE_LIMIT_WINDOW_SECONDS
        self.max_requests = max_requests or settings.RATE_LIMIT_MAX_REQUESTS
        
        # Data store for rate limiting
        # Note: In production, use Redis or similar distributed store
        self.client_requests: Dict[str, Dict] = {}
        
        logger.info(
            f"Rate limiter initialized: {self.max_requests} requests per {self.window_seconds} seconds"
        )
    
    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        """
        Process request with rate limiting enforcement.
        
        Implements request rate limiting with proper client identification,
        request counting, and response generation based on limit status.
        
        Args:
            request: Incoming request
            call_next: Next middleware in chain
            
        Returns:
            API response with appropriate status and headers
        """
        # Skip rate limiting for excluded paths
        if self._should_skip_rate_limiting(request.url.path):
            return await call_next(request)
        
        # Get client identifier
        client_id = self._get_client_identifier(request)
        
        # Initialize client entry if not exists
        if client_id not in self.client_requests:
            self.client_requests[client_id] = {
                "requests": [],
                "blocked_until": None
            }
        
        client_data = self.client_requests[client_id]
        current_time = time.time()
        
        # Check if client is blocked
        if client_data["blocked_until"] and client_data["blocked_until"] > current_time:
            # Client is currently blocked
            wait_seconds = int(client_data["blocked_until"] - current_time)
            logger.warning(f"Rate limit exceeded for client {client_id}: blocked for {wait_seconds}s")
            
            return self._create_rate_limit_response(wait_seconds)
        
        # Clean old requests outside window
        window_start = current_time - self.window_seconds
        client_data["requests"] = [r for r in client_data["requests"] if r >= window_start]
        
        # Check if client exceeds rate limit
        if len(client_data["requests"]) >= self.max_requests:
            # Block client for window duration
            client_data["blocked_until"] = current_time + self.window_seconds
            logger.warning(
                f"Rate limit exceeded for client {client_id}: {self.max_requests} requests in {self.window_seconds}s"
            )
            
            return self._create_rate_limit_response(self.window_seconds)
        
        # Add current request to history and process normally
        client_data["requests"].append(current_time)
        
        # Add rate limit headers to response
        response = await call_next(request)
        remaining = self.max_requests - len(client_data["requests"])
        
        response.headers["X-RateLimit-Limit"] = str(self.max_requests)
        response.headers["X-RateLimit-Remaining"] = str(remaining)
        response.headers["X-RateLimit-Reset"] = str(int(window_start + self.window_seconds))
        
        return response
    
    def _should_skip_rate_limiting(self, path: str) -> bool:
        """
        Determine if rate limiting should be skipped for this path.
        
        Args:
            path: Request URL path
            
        Returns:
            Boolean indicating if rate limiting should be skipped
        """
        # Skip docs, health checks, and other specific endpoints
        skipped_paths = [
            "/docs", 
            "/redoc", 
            "/openapi.json",
            "/health",
            "/metrics"
        ]
        
        return any(path.startswith(skip_path) for skip_path in skipped_paths)
    
    def _get_client_identifier(self, request: Request) -> str:
        """
        Extract client identifier for rate limiting.
        
        Implements multi-factor client identification with fallbacks
        to properly identify clients for rate limiting purposes.
        
        Args:
            request: Incoming request
            
        Returns:
            Client identifier string
        """
        # Try to get client ID from header
        client_id = request.headers.get("X-Client-ID")
        
        # Fallback to IP address if no client ID provided
        if not client_id:
            client_id = request.client.host if request.client else "unknown"
            
        return client_id
    
    def _create_rate_limit_response(self, retry_after: int) -> Response:
        """
        Create rate limit exceeded response with proper headers.
        
        Args:
            retry_after: Seconds to wait before retrying
            
        Returns:
            Rate limit response with appropriate status and headers
        """
        from fastapi.responses import JSONResponse
        
        content = {
            "status": "error",
            "message": f"Rate limit exceeded. Try again in {retry_after} seconds.",
            "error_code": "RATE_LIMIT_EXCEEDED",
            "timestamp": datetime.utcnow().isoformat()
        }
        
        response = JSONResponse(
            content=content,
            status_code=HTTP_429_TOO_MANY_REQUESTS
        )
        
        response.headers["Retry-After"] = str(retry_after)
        return response

================
File: api/models/auth.py
================
"""
Authentication Data Models with OAuth Support

Defines comprehensive data models for authentication operations
including token generation, validation, user credential management,
and OAuth authentication for multiple providers.

Design Considerations:
- Proper data validation and constraints
- Type safety with comprehensive annotations
- Clear documentation of data requirements
- Security-focused constraints
- Support for multiple OAuth providers
"""

from datetime import datetime
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field, EmailStr, field_validator, SecretStr


class Token(BaseModel):
    """
    JWT token response model with comprehensive metadata.
    
    Provides structured representation of authentication token
    with relevant expiration and type information.
    """
    access_token: str = Field(
        ...,
        description="JWT access token for API authentication"
    )
    token_type: str = Field(
        ...,
        description="Token type, typically 'bearer'"
    )
    expires_in: int = Field(
        ...,
        description="Token expiration time in seconds"
    )


class TokenData(BaseModel):
    """
    Decoded token data model with user and permission information.
    
    Maintains structured representation of token contents with
    comprehensive user context and permission details.
    """
    username: str = Field(
        ...,
        description="Username identifier"
    )
    permissions: List[str] = Field(
        default=[],
        description="List of permissions granted to the user"
    )
    exp: int = Field(
        ...,
        description="Token expiration timestamp"
    )
    
    @field_validator("permissions")
    @classmethod
    def validate_permissions(cls, value: List[str]) -> List[str]:
        """Validate permissions contain only allowed values."""
        valid_permissions = {"admin", "process", "view"}
        if not all(perm in valid_permissions for perm in value):
            raise ValueError(f"Permissions must be one of: {', '.join(valid_permissions)}")
        return value


class UserCredentials(BaseModel):
    """
    User credentials model for authentication validation.
    
    Implements comprehensive validation for login credentials
    with proper security constraints and format validation.
    """
    username: str = Field(
        ...,
        min_length=3,
        max_length=50,
        description="Username for authentication"
    )
    password: SecretStr = Field(
        ...,
        min_length=8,
        description="Password for authentication"
    )
    
    @field_validator("username")
    @classmethod
    def username_alphanumeric(cls, value: str) -> str:
        """Validate username contains only allowed characters."""
        if not value.isalnum():
            raise ValueError("Username must only contain alphanumeric characters")
        return value


class UserLoginResponse(BaseModel):
    """
    Comprehensive user login response with authentication details.
    
    Provides token information and basic user context for the client.
    """
    token: Token = Field(
        ...,
        description="Authentication token details"
    )
    username: str = Field(
        ...,
        description="Authenticated username"
    )
    permissions: List[str] = Field(
        ...,
        description="User permissions"
    )


class GoogleAuthRequest(BaseModel):
    """
    Google authentication request model with ID token.
    
    Contains the ID token returned from Google's OAuth flow
    for verification and user authentication.
    """
    id_token: str = Field(
        ...,
        description="Google OAuth ID token"
    )


class GoogleCallbackRequest(BaseModel):
    """
    Google OAuth callback request model.
    
    Contains the authorization code returned from Google's OAuth
    flow for exchanging with access and refresh tokens.
    """
    code: str = Field(
        ...,
        description="Google OAuth authorization code"
    )
    state: Optional[str] = Field(
        None,
        description="State parameter for security verification"
    )


class OAuthLoginRequest(BaseModel):
    """
    OAuth login request model.
    
    Contains the provider name for initiating the OAuth flow.
    """
    provider: str = Field(
        ...,
        description="OAuth provider name (e.g., 'google', 'microsoft')"
    )
    redirect_uri: str = Field(
        ...,
        description="URI to redirect to after authentication"
    )


class OAuthCallbackRequest(BaseModel):
    """
    OAuth callback request model.
    
    Contains the provider name, authorization code, and optional state
    for processing the OAuth callback.
    """
    provider: str = Field(
        ...,
        description="OAuth provider name (e.g., 'google', 'microsoft')"
    )
    code: str = Field(
        ...,
        description="OAuth authorization code"
    )
    state: Optional[str] = Field(
        None,
        description="State parameter for security verification"
    )
    redirect_uri: str = Field(
        ...,
        description="URI used in authorization request"
    )


class OAuthUserInfo(BaseModel):
    """
    User information from OAuth provider.
    
    Contains user profile information from the OAuth provider.
    """
    provider: str = Field(
        ...,
        description="OAuth provider name"
    )
    provider_user_id: str = Field(
        ...,
        description="Provider-specific user ID"
    )
    email: EmailStr = Field(
        ...,
        description="User email address"
    )
    name: Optional[str] = Field(
        None,
        description="User display name"
    )
    picture: Optional[str] = Field(
        None,
        description="URL to user profile picture"
    )
    

class User(BaseModel):
    """
    User model with comprehensive profile information.
    
    Contains user profile details, permissions, and OAuth provider
    information.
    """
    id: str = Field(
        ...,
        description="Unique user identifier"
    )
    username: str = Field(
        ...,
        description="Unique username"
    )
    email: EmailStr = Field(
        ...,
        description="User email address"
    )
    display_name: Optional[str] = Field(
        None,
        description="User display name"
    )
    permissions: List[str] = Field(
        ...,
        description="User permissions"
    )
    profile_picture: Optional[str] = Field(
        None,
        description="URL to user profile picture"
    )
    is_active: bool = Field(
        ...,
        description="Whether the user is active"
    )
    created_at: str = Field(
        ...,
        description="User creation timestamp"
    )
    last_login: Optional[str] = Field(
        None,
        description="Last login timestamp"
    )
    oauth_providers: List[str] = Field(
        default=[],
        description="List of linked OAuth providers"
    )


class OAuthLoginResponse(BaseModel):
    """
    OAuth login response model.
    
    Contains the authorization URL for the OAuth flow.
    """
    authorization_url: str = Field(
        ...,
        description="URL to redirect the user to for OAuth authorization"
    )
    state: str = Field(
        ...,
        description="State parameter for security verification"
    )
    

class OAuthCallbackResponse(BaseModel):
    """
    OAuth callback response model.
    
    Contains the user information and access token after successful
    OAuth authentication.
    """
    user: User = Field(
        ...,
        description="User information"
    )
    access_token: str = Field(
        ...,
        description="JWT access token"
    )
    token_type: str = Field(
        ...,
        description="Token type, typically 'bearer'"
    )
    expires_in: int = Field(
        ...,
        description="Token expiration time in seconds"
    )


class AvailableProvidersResponse(BaseModel):
    """
    Available OAuth providers response model.
    
    Contains the list of available OAuth providers for login.
    """
    providers: Dict[str, str] = Field(
        ...,
        description="Mapping of provider codes to display names"
    )

================
File: api/models/dashboard.py
================
"""
Dashboard Data Models

Defines models for dashboard data including statistics, metrics, and
visualization data structures.

Design Considerations:
- Comprehensive type definitions
- Detailed documentation
- Proper validation rules
"""

from datetime import datetime
from typing import Dict, List, Optional
from pydantic import BaseModel, Field


class EmailVolumeMetric(BaseModel):
    """Email volume data for dashboard charts."""
    
    date: str = Field(..., description="Date label for the data point")
    total: int = Field(..., description="Total emails processed")
    meeting: int = Field(..., description="Meeting related emails")
    other: int = Field(..., description="Other emails")


class CategoryDistribution(BaseModel):
    """Category distribution data for pie charts."""
    
    category: str = Field(..., description="Email category name")
    count: int = Field(..., description="Number of emails in this category")
    percentage: float = Field(..., description="Percentage of total emails")


class PerformanceMetric(BaseModel):
    """Performance metric data for dashboards."""
    
    metric_name: str = Field(..., description="Name of the performance metric")
    current_value: float = Field(..., description="Current value of the metric")
    previous_value: Optional[float] = Field(None, description="Previous value for comparison")
    change_percentage: Optional[float] = Field(None, description="Percentage change from previous period")
    trend: Optional[str] = Field(None, description="Trend direction (up, down, stable)")


class AgentMetric(BaseModel):
    """Metrics for individual AI agents."""
    
    agent_id: str = Field(..., description="Unique identifier for the agent")
    agent_name: str = Field(..., description="Display name of the agent")
    emails_processed: int = Field(..., description="Number of emails processed by this agent")
    success_rate: float = Field(..., description="Success rate percentage")
    avg_processing_time: float = Field(..., description="Average processing time in milliseconds")
    is_active: bool = Field(..., description="Whether the agent is currently active")


class DashboardStats(BaseModel):
    """Comprehensive dashboard statistics."""
    
    total_emails: int = Field(..., description="Total number of emails processed")
    meeting_emails: int = Field(..., description="Number of meeting-related emails")
    response_rate: float = Field(..., description="Percentage of emails with responses")
    avg_processing_time: float = Field(..., description="Average email processing time in milliseconds")
    success_rate: float = Field(..., description="Success rate for email processing")
    volume_trend: List[EmailVolumeMetric] = Field(..., description="Email volume trend data")
    category_distribution: List[CategoryDistribution] = Field(..., description="Email category distribution")
    performance_metrics: List[PerformanceMetric] = Field(..., description="Key performance metrics")
    agent_metrics: List[AgentMetric] = Field(..., description="Metrics for individual AI agents")
    last_updated: datetime = Field(..., description="Timestamp of when stats were last updated")


class UserActivitySummary(BaseModel):
    """Summary of user activity for the dashboard."""
    
    total_users: int = Field(..., description="Total number of users")
    active_users: int = Field(..., description="Number of active users")
    emails_per_user: Dict[str, int] = Field(..., description="Emails processed per user")
    last_activity: Dict[str, datetime] = Field(..., description="Last activity timestamp per user")


class EmailAccountStats(BaseModel):
    """Statistics for a single email account."""
    
    email: str = Field(..., description="Email address")
    total_processed: int = Field(..., description="Total emails processed for this account")
    categories: Dict[str, int] = Field(..., description="Email count by category")
    is_active: bool = Field(..., description="Whether this account is currently active")
    last_sync: Optional[datetime] = Field(None, description="Last synchronization timestamp")


class DashboardSummary(BaseModel):
    """Summary dashboard data combining multiple metrics."""
    
    stats: DashboardStats = Field(..., description="Overall dashboard statistics")
    user_activity: UserActivitySummary = Field(..., description="User activity summary")
    email_accounts: List[EmailAccountStats] = Field(..., description="Email account statistics")
    period: str = Field(..., description="Time period for the dashboard data (day, week, month, etc.)")

================
File: api/models/emails.py
================
"""
Email Data Models

Defines comprehensive data models for email processing operations
including analysis requests, responses, and metadata structures.

Design Considerations:
- Comprehensive field validation
- Type safety with clear annotations
- Consistent structure for client processing
- Proper documentation of data requirements
"""

from datetime import datetime
from typing import Dict, List, Optional, Any, Set

from pydantic import BaseModel, Field, EmailStr, field_validator


class EmailSummary(BaseModel):
    """
    Summary model for email listings.
    
    Provides essential email information for list views
    with proper type safety and validation.
    """
    message_id: str = Field(
        ...,
        description="Unique email message identifier"
    )
    subject: str = Field(
        ...,
        description="Email subject line"
    )
    sender: str = Field(
        ...,
        description="Email sender address"
    )
    received_at: datetime = Field(
        ...,
        description="Email receive timestamp"
    )
    category: str = Field(
        ...,
        description="Email categorization result"
    )
    is_responded: bool = Field(
        default=False,
        description="Whether a response has been sent"
    )


class EmailListResponse(BaseModel):
    """
    Response model for paginated email listings.
    
    Implements comprehensive pagination metadata with
    proper email summary information.
    """
    emails: List[EmailSummary] = Field(
        default_factory=list,
        description="List of email summaries"
    )
    total: int = Field(
        ...,
        ge=0,
        description="Total number of emails matching criteria"
    )
    limit: int = Field(
        ...,
        ge=1,
        description="Maximum emails per page"
    )
    offset: int = Field(
        ...,
        ge=0,
        description="Offset from start of results"
    )


class AnalysisMetadata(BaseModel):
    """
    Metadata for email analysis results.
    
    Provides comprehensive metadata about the analysis process
    with proper timestamp and result information.
    """
    analyzed_at: datetime = Field(
        default_factory=datetime.utcnow,
        description="Analysis timestamp"
    )
    model_version: str = Field(
        ...,
        description="Version of the analysis model used"
    )
    confidence_score: float = Field(
        ...,
        ge=0.0, 
        le=1.0,
        description="Confidence score of analysis"
    )
    processing_time_ms: int = Field(
        ...,
        ge=0,
        description="Processing time in milliseconds"
    )


class EmailAnalysisRequest(BaseModel):
    """
    Request model for email analysis.
    
    Implements comprehensive request structure for email analysis
    with proper validation and field requirements.
    """
    content: str = Field(
        ...,
        description="Email content to analyze"
    )
    subject: str = Field(
        ...,
        description="Email subject line"
    )
    sender: str = Field(
        ...,
        description="Email sender address"
    )
    message_id: Optional[str] = Field(
        default=None,
        description="Optional message ID for tracking"
    )


class MeetingDetails(BaseModel):
    """
    Extracted meeting details from email analysis.
    
    Contains structured representation of meeting information
    extracted during email analysis with proper validation.
    """
    date: Optional[str] = Field(
        default=None,
        description="Extracted meeting date"
    )
    time: Optional[str] = Field(
        default=None,
        description="Extracted meeting time"
    )
    location: Optional[str] = Field(
        default=None,
        description="Extracted meeting location"
    )
    participants: Optional[List[str]] = Field(
        default=None,
        description="Extracted meeting participants"
    )
    agenda: Optional[str] = Field(
        default=None,
        description="Extracted meeting agenda or purpose"
    )
    missing_elements: List[str] = Field(
        default_factory=list,
        description="Required elements missing from meeting details"
    )
    
    @property
    def is_complete(self) -> bool:
        """Check if meeting details are complete."""
        return len(self.missing_elements) == 0


class EmailAnalysisResponse(BaseModel):
    """
    Response model for email analysis results.
    
    Provides comprehensive analysis results with structured
    data and metadata about the analysis process.
    """
    is_meeting_related: bool = Field(
        ...,
        description="Whether email is meeting-related"
    )
    category: str = Field(
        ...,
        description="Email categorization result"
    )
    recommended_action: str = Field(
        ...,
        description="Recommended action for the email"
    )
    meeting_details: Optional[MeetingDetails] = Field(
        default=None,
        description="Extracted meeting details if applicable"
    )
    suggested_response: Optional[str] = Field(
        default=None,
        description="Suggested response text if available"
    )
    metadata: AnalysisMetadata = Field(
        ...,
        description="Analysis process metadata"
    )


class EmailContent(BaseModel):
    """
    Email content model with comprehensive data.
    
    Implements complete email content representation with
    proper content formatting and metadata.
    """
    raw_content: str = Field(
        ...,
        description="Raw email content"
    )
    processed_content: Optional[str] = Field(
        default=None,
        description="Processed email content"
    )
    html_content: Optional[str] = Field(
        default=None,
        description="HTML email content if available"
    )
    attachments: List[str] = Field(
        default_factory=list,
        description="List of attachment filenames"
    )


class EmailDetailResponse(BaseModel):
    """
    Detailed email information for single email view.
    
    Provides comprehensive email details including content,
    analysis results, and processing metadata.
    """
    message_id: str = Field(
        ...,
        description="Unique email message identifier"
    )
    subject: str = Field(
        ...,
        description="Email subject line"
    )
    sender: str = Field(
        ...,
        description="Email sender address"
    )
    received_at: datetime = Field(
        ...,
        description="Email receive timestamp"
    )
    content: EmailContent = Field(
        ...,
        description="Email content details"
    )
    category: str = Field(
        ...,
        description="Email categorization result"
    )
    is_responded: bool = Field(
        default=False,
        description="Whether a response has been sent"
    )
    analysis_results: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Detailed analysis results if available"
    )
    processing_history: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="History of processing operations"
    )


class EmailProcessingStats(BaseModel):
    """
    Statistics about email processing operations.
    
    Provides comprehensive statistics about email processing
    volume, categories, and performance metrics.
    """
    total_emails_processed: int = Field(
        ...,
        ge=0,
        description="Total number of emails processed"
    )
    emails_by_category: Dict[str, int] = Field(
        ...,
        description="Count of emails by category"
    )
    average_processing_time_ms: float = Field(
        ...,
        ge=0.0,
        description="Average processing time in milliseconds"
    )
    success_rate: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Percentage of emails successfully processed"
    )
    stats_period_days: int = Field(
        default=30,
        ge=1,
        description="Period in days these statistics cover"
    )
    last_updated: datetime = Field(
        default_factory=datetime.utcnow,
        description="When these statistics were last updated"
    )


class EmailSettings(BaseModel):
    """
    Email processing system settings.
    
    Implements comprehensive settings for email processing
    configuration with proper validation and defaults.
    """
    batch_size: int = Field(
        default=10,
        ge=1,
        le=100,
        description="Number of emails to process in each batch"
    )
    auto_respond_enabled: bool = Field(
        default=True,
        description="Whether to automatically send responses"
    )
    confidence_threshold: float = Field(
        default=0.7,
        ge=0.0,
        le=1.0,
        description="Confidence threshold for automatic responses"
    )
    processing_interval_minutes: int = Field(
        default=15,
        ge=1,
        description="How often to process new emails in minutes"
    )
    max_tokens_per_analysis: int = Field(
        default=4000,
        ge=1000,
        description="Maximum tokens to use for email analysis"
    )
    models: Dict[str, str] = Field(
        default={
            "classification": "llama-3.3-70b-versatile",
            "analysis": "deepseek-reasoner",
            "response": "llama-3.3-70b-versatile"
        },
        description="Model configuration for each pipeline stage"
    )
    
    @field_validator("models")
    @classmethod
    def validate_models(cls, models: Dict[str, str]) -> Dict[str, str]:
        """Validate that all required models are specified."""
        required_keys = {"classification", "analysis", "response"}
        if not all(key in models for key in required_keys):
            missing = required_keys - set(models.keys())
            raise ValueError(f"Missing required model configurations: {missing}")
        return models

================
File: api/models/errors.py
================
"""
Error Response Models

Defines standardized error response models for consistent
error handling across the API surface.

Design Considerations:
- Consistent error structure for client processing
- Comprehensive error metadata for debugging
- Clear error codes and messages
- Proper timestamp handling
"""

from datetime import datetime
from typing import Dict, Optional, Any, List

from pydantic import BaseModel, Field


class ErrorResponse(BaseModel):
    """
    Standardized error response model for API errors.
    
    Provides consistent error structure with comprehensive
    metadata for client understanding and debugging.
    """
    status: str = Field(
        default="error",
        description="Error status indicator"
    )
    message: str = Field(
        ...,
        description="Human-readable error message"
    )
    error_code: str = Field(
        ...,
        description="Machine-readable error code"
    )
    details: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Additional error details"
    )
    timestamp: datetime = Field(
        default_factory=datetime.utcnow,
        description="Error timestamp"
    )


class ValidationErrorItem(BaseModel):
    """
    Validation error detail model for data validation errors.
    
    Contains specific information about field validation failures.
    """
    loc: List[str] = Field(
        ...,
        description="Error location (field path)"
    )
    msg: str = Field(
        ...,
        description="Error message"
    )
    type: str = Field(
        ...,
        description="Error type"
    )


class ValidationErrorResponse(ErrorResponse):
    """
    Enhanced error response for validation errors.
    
    Extends standard error response with field-specific
    validation error details.
    """
    validation_errors: List[ValidationErrorItem] = Field(
        ...,
        description="List of specific validation errors"
    )

================
File: api/README.md
================
# Email Management API

## Overview

This API provides a secure REST interface to the email processing system, allowing you to:

- Authenticate users with JWT tokens
- Process and analyze emails
- Retrieve email processing results
- Configure system settings
- Get email processing statistics

## Authentication

The API uses JSON Web Tokens (JWT) for authentication with role-based access control:

- **Admin Role** - Full system access including configuration changes
- **Process Role** - Can trigger email processing and view results
- **View Role** - Can only view processed emails and statistics

### Development Credentials

For local development, the following users are pre-configured:

- **Admin User**
  - Username: `admin`
  - Password: `securepassword`
  - Permissions: admin, process, view

- **Viewer User**
  - Username: `viewer`
  - Password: `viewerpass`
  - Permissions: view

## API Endpoints

### Authentication

- `POST /token` - OAuth2 token endpoint (form submission)
- `POST /login` - User login endpoint (JSON)

### Email Processing

- `GET /emails/` - Get paginated list of processed emails
- `GET /emails/{message_id}` - Get detailed email information
- `POST /emails/analyze` - Analyze email content
- `GET /emails/stats` - Get email processing statistics
- `GET /emails/settings` - Get current email processing settings
- `PUT /emails/settings` - Update email processing settings
- `POST /emails/process-batch` - Trigger batch email processing

### System Monitoring

- `GET /health` - Simple health check endpoint

## Running the API Server

You can run the API server using the provided `run_api.py` script:

```bash
# Run in development mode with auto-reload
python run_api.py --reload

# Run on a specific host and port
python run_api.py --host 0.0.0.0 --port 5000

# Run in production mode
python run_api.py --env production
```

In development mode, you can access the Swagger UI documentation at: http://127.0.0.1:8000/docs

## Example Usage

### Authentication

```bash
# Get access token
curl -X POST "http://localhost:8000/token" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=admin&password=securepassword"
```

Response:
```json
{
  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "token_type": "bearer",
  "expires_in": 1800
}
```

### Analyzing an Email

```bash
# Analyze email
curl -X POST "http://localhost:8000/emails/analyze" \
  -H "Authorization: Bearer YOUR_ACCESS_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "content": "Hi Team, Let's meet tomorrow at 2pm in Conference Room A to discuss the project. Best, John",
    "subject": "Meeting Request",
    "sender": "john@example.com"
  }'
```

Response:
```json
{
  "is_meeting_related": true,
  "category": "meeting",
  "recommended_action": "respond",
  "meeting_details": {
    "date": "tomorrow",
    "time": "2pm",
    "location": "Conference Room A",
    "agenda": "discuss the project",
    "participants": null,
    "missing_elements": []
  },
  "suggested_response": "Hi John, I'm pleased to confirm our meeting tomorrow at 2pm in Conference Room A to discuss the project. Looking forward to it!",
  "metadata": {
    "analyzed_at": "2025-02-27T23:47:32.145123",
    "model_version": "1.0.0",
    "confidence_score": 0.85,
    "processing_time_ms": 352
  }
}
```

### Processing Batch of Emails

```bash
# Process email batch
curl -X POST "http://localhost:8000/emails/process-batch?batch_size=20" \
  -H "Authorization: Bearer YOUR_ACCESS_TOKEN"
```

Response:
```json
{
  "processed": 5,
  "errors": 0,
  "success_rate": 1.0,
  "timestamp": "2025-02-27T23:48:01.129875"
}
```

## Security Considerations

1. **JWT Secret**: In production, ensure you set a strong JWT_SECRET_KEY in the environment or .env file
2. **CORS Settings**: Configure the CORS_ORIGINS environment variable for production
3. **Rate Limiting**: The API includes rate limiting to protect against abuse

## Integration Architecture

The API connects to the core email processing system through the `EmailService` which acts as an integration layer between the HTTP API and the core processing components:

```
│ FastAPI Routes │ ───► │ Email Service │ ───► │ Email Processor │
     │                                              │
     ▼                                              ▼
│ Auth Service  │                         │ Email Analyzers │
```

This architecture ensures a clean separation of concerns and allows the API to evolve independently from the core processing logic.

================
File: api/routes/__init__.py
================
"""
API Routes Package

Centralizes route management with proper module organization
and clean import structure.

Design Considerations:
- Clear route organization
- Explicit imports for better readability
- Centralized route registration
"""

from api.routes import auth
from api.routes import emails
from api.routes import dashboard

__all__ = ["auth", "emails", "dashboard"]

================
File: api/routes/auth.py
================
"""
Authentication API Routes with Multi-Provider OAuth Support

Implements comprehensive authentication endpoints with proper
security practices, validation, error handling, and multi-provider
OAuth integration for Gmail and Outlook access.

Design Considerations:
- Robust token generation and validation
- Multi-provider OAuth authorization flow
- Comprehensive input validation
- Proper error handling and logging
- Clear response formats
"""

import os
import logging
from datetime import timedelta
from typing import Dict, Any, List

from fastapi import APIRouter, Depends, HTTPException, status, Request, Query
from fastapi.security import OAuth2PasswordRequestForm
from fastapi.responses import RedirectResponse, JSONResponse

from api.auth.service import AuthenticationService, get_auth_service
from api.models.auth import (
    Token, UserCredentials, UserLoginResponse, 
    OAuthLoginRequest, OAuthCallbackRequest, OAuthLoginResponse,
    OAuthCallbackResponse, User, AvailableProvidersResponse
)
from api.config import get_settings
from src.auth.oauth_factory import OAuthProviderFactory
from src.storage.user_repository import UserRepository
from src.storage.database import init_db

# Configure logging
logger = logging.getLogger(__name__)

# Create router
router = APIRouter(tags=["Authentication"])

# Initialize database on startup
init_db()

@router.post(
    "/token",
    response_model=Token,
    summary="OAuth2 token endpoint"
)
async def login_for_access_token(
    form_data: OAuth2PasswordRequestForm = Depends(),
    auth_service: AuthenticationService = Depends(get_auth_service)
):
    """
    Generate access token from username and password.
    
    Standard OAuth2 token endpoint for password-based authentication
    with proper token generation and security validation.
    
    Args:
        form_data: OAuth2 form with username and password
        
    Returns:
        Token response with access token
        
    Raises:
        HTTPException: If authentication fails
    """
    user = await auth_service.authenticate_user(form_data.username, form_data.password)
    if not user:
        logger.warning(f"Failed authentication attempt for user: {form_data.username}")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # Create access token with appropriate expiration
    settings = get_settings()
    access_token_expires = timedelta(minutes=settings.JWT_TOKEN_EXPIRE_MINUTES)
    access_token = auth_service.create_access_token(
        data={"username": user["username"], "permissions": user["permissions"]},
        expires_delta=access_token_expires,
    )
    
    logger.info(f"Access token generated for user: {user['username']}")
    
    return Token(
        access_token=access_token, 
        token_type="bearer",
        expires_in=settings.JWT_TOKEN_EXPIRE_MINUTES * 60  # Convert to seconds
    )

@router.get(
    "/oauth/providers",
    response_model=AvailableProvidersResponse,
    summary="Get available OAuth providers"
)
async def get_available_providers():
    """
    Get list of available OAuth providers for login.
    
    Returns a list of available OAuth providers that can be used
    for authentication with their display names.
    
    Returns:
        Dictionary of provider codes to display names
    """
    providers = OAuthProviderFactory.get_available_providers()
    return AvailableProvidersResponse(providers=providers)

@router.post(
    "/oauth/login",
    response_model=OAuthLoginResponse,
    summary="Initiate OAuth login flow"
)
async def oauth_login(
    request: OAuthLoginRequest,
    auth_service: AuthenticationService = Depends(get_auth_service)
):
    """
    Initiate OAuth login flow for specified provider.
    
    Generates an authorization URL for the specified OAuth provider
    that the client should redirect the user to for authorization.
    
    Args:
        request: OAuth login request with provider and redirect URI
        
    Returns:
        Authorization URL and state for the OAuth flow
        
    Raises:
        HTTPException: If provider is not supported or configuration is invalid
    """
    try:
        authorization_url, state = await auth_service.get_authorization_url(
            request.provider, 
            request.redirect_uri
        )
        
        logger.info(f"Generated OAuth authorization URL for provider: {request.provider}")
        
        return OAuthLoginResponse(
            authorization_url=authorization_url,
            state=state
        )
        
    except ValueError as e:
        logger.error(f"Error generating OAuth login URL: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    except Exception as e:
        logger.error(f"Unexpected error in OAuth login: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to initiate OAuth login"
        )

@router.post(
    "/oauth/callback",
    response_model=OAuthCallbackResponse,
    summary="Handle OAuth callback"
)
async def oauth_callback(
    request: OAuthCallbackRequest,
    auth_service: AuthenticationService = Depends(get_auth_service)
):
    """
    Process OAuth callback after user authorization.
    
    Exchanges the authorization code for access and refresh tokens,
    creates or updates the user in the database, and returns a JWT token
    for API authentication.
    
    Args:
        request: OAuth callback request with code and state
        
    Returns:
        User information and JWT access token
        
    Raises:
        HTTPException: If OAuth callback processing fails
    """
    try:
        # Process OAuth callback
        result = await auth_service.process_oauth_callback(
            request.provider,
            request.code,
            request.redirect_uri
        )
        
        # Extract user and token information
        user_dict = result["user"]
        user = User(
            id=user_dict["id"],
            username=user_dict["username"],
            email=user_dict["email"],
            display_name=user_dict.get("display_name"),
            permissions=user_dict["permissions"],
            profile_picture=user_dict.get("profile_picture"),
            is_active=user_dict.get("is_active", True),
            created_at=user_dict["created_at"],
            last_login=user_dict.get("last_login"),
            oauth_providers=user_dict.get("oauth_providers", [])
        )
        
        logger.info(f"Successfully processed OAuth callback for user: {user.username}")
        
        return OAuthCallbackResponse(
            user=user,
            access_token=result["access_token"],
            token_type=result["token_type"],
            expires_in=result["expires_in"]
        )
        
    except ValueError as e:
        logger.error(f"Error processing OAuth callback: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    except Exception as e:
        logger.error(f"Unexpected error in OAuth callback: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to process OAuth callback"
        )

@router.post(
    "/login",
    response_model=UserLoginResponse,
    summary="User login endpoint"
)
async def login(
    credentials: UserCredentials,
    auth_service: AuthenticationService = Depends(get_auth_service)
):
    """
    Login user and return authentication details.
    
    Provides enhanced login endpoint with comprehensive user details
    and proper authentication handling with detailed responses.
    
    Args:
        credentials: User login credentials
        
    Returns:
        User login response with token and permissions
        
    Raises:
        HTTPException: If authentication fails
    """
    user = await auth_service.authenticate_user(
        credentials.username, 
        credentials.password.get_secret_value()
    )
    
    if not user:
        logger.warning(f"Failed login attempt for user: {credentials.username}")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password"
        )
    
    # Calculate token expiration
    settings = get_settings()
    access_token_expires = timedelta(minutes=settings.JWT_TOKEN_EXPIRE_MINUTES)
    
    # Create access token with proper claims
    access_token = auth_service.create_access_token(
        data={
            "username": user["username"],
            "permissions": user["permissions"]
        },
        expires_delta=access_token_expires,
    )
    
    # Create token response
    token = Token(
        access_token=access_token,
        token_type="bearer",
        expires_in=settings.JWT_TOKEN_EXPIRE_MINUTES * 60  # Convert to seconds
    )
    
    logger.info(f"User successfully logged in: {user['username']}")
    
    return UserLoginResponse(
        token=token,
        username=user["username"],
        permissions=user["permissions"]
    )

@router.get(
    "/me",
    response_model=User,
    summary="Get current user information"
)
async def get_current_user(
    auth_service: AuthenticationService = Depends(get_auth_service),
    user: Dict[str, Any] = Depends(auth_service.get_current_user)
):
    """
    Get current authenticated user information.
    
    Returns comprehensive profile information for the currently
    authenticated user with OAuth provider details.
    
    Args:
        user: Current authenticated user from JWT token
        
    Returns:
        User profile information
    """
    # Convert raw user dictionary to User model
    return User(
        id=user["id"],
        username=user["username"],
        email=user["email"],
        display_name=user.get("display_name"),
        permissions=user["permissions"],
        profile_picture=user.get("profile_picture"),
        is_active=user.get("is_active", True),
        created_at=user["created_at"],
        last_login=user.get("last_login"),
        oauth_providers=user.get("oauth_providers", [])
    )

@router.get(
    "/oauth/redirect/{provider}",
    summary="OAuth redirect shortcut"
)
async def oauth_redirect(
    provider: str,
    redirect_uri: str = Query(...),
    auth_service: AuthenticationService = Depends(get_auth_service)
):
    """
    Redirect user to OAuth provider authorization page.
    
    Convenience endpoint for redirecting users directly to the
    OAuth authorization page without a client-side redirect.
    
    Args:
        provider: OAuth provider name
        redirect_uri: URI to redirect after authorization
        
    Returns:
        Redirect to OAuth provider authorization page
    """
    try:
        authorization_url, state = await auth_service.get_authorization_url(
            provider, 
            redirect_uri
        )
        
        logger.info(f"Redirecting to {provider} OAuth authorization page")
        return RedirectResponse(url=authorization_url)
        
    except Exception as e:
        logger.error(f"Error in OAuth redirect: {str(e)}")
        return JSONResponse(
            content={"error": str(e)},
            status_code=400
        )

================
File: api/routes/dashboard.py
================
"""
Dashboard API Routes

Implements comprehensive dashboard endpoints with proper 
authentication, validation, and optimal data retrieval.

Design Considerations:
- Performance optimization for dashboard data retrieval
- Proper caching strategies
- Consistent route organization
- Clear endpoint documentation
- Proper error handling
"""

import logging
from typing import Dict, List, Optional
from fastapi import APIRouter, Depends, HTTPException, Query, Path, status

from api.auth.service import require_admin, require_view
from api.models.dashboard import (
    DashboardStats,
    UserActivitySummary,
    EmailAccountStats,
    DashboardSummary
)
from api.services.dashboard_service import DashboardService, get_dashboard_service

# Configure logging
logger = logging.getLogger(__name__)

# Create router with prefix
router = APIRouter(prefix="/dashboard", tags=["Dashboard"])


@router.get(
    "/stats",
    response_model=DashboardStats,
    summary="Get dashboard statistics",
    dependencies=[Depends(require_view)]
)
async def get_dashboard_stats(
    period: str = Query("day", description="Time period for metrics (day, week, month)"),
    dashboard_service: DashboardService = Depends(get_dashboard_service)
):
    """
    Retrieve comprehensive dashboard statistics.
    
    Provides key metrics and visualization data for the dashboard
    with proper caching and optimization.
    
    Args:
        period: Time period for metrics (day, week, month)
        
    Returns:
        Comprehensive dashboard statistics
    """
    try:
        stats = await dashboard_service.get_dashboard_stats(period)
        return stats
    except Exception as e:
        logger.error(f"Error retrieving dashboard stats: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve dashboard statistics: {str(e)}"
        )


@router.get(
    "/user-activity",
    response_model=UserActivitySummary,
    summary="Get user activity summary",
    dependencies=[Depends(require_view)]
)
async def get_user_activity(
    dashboard_service: DashboardService = Depends(get_dashboard_service)
):
    """
    Retrieve user activity summary for the dashboard.
    
    Provides aggregated user activity data with proper
    caching and optimization.
    
    Returns:
        User activity summary data
    """
    try:
        activity = await dashboard_service.get_user_activity()
        return activity
    except Exception as e:
        logger.error(f"Error retrieving user activity: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve user activity: {str(e)}"
        )


@router.get(
    "/email-accounts",
    response_model=List[EmailAccountStats],
    summary="Get email account statistics",
    dependencies=[Depends(require_view)]
)
async def get_email_account_stats(
    dashboard_service: DashboardService = Depends(get_dashboard_service)
):
    """
    Retrieve email account statistics.
    
    Provides statistics for each connected email account
    with proper caching and optimization.
    
    Returns:
        List of email account statistics
    """
    try:
        account_stats = await dashboard_service.get_email_account_stats()
        return account_stats
    except Exception as e:
        logger.error(f"Error retrieving email account stats: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve email account statistics: {str(e)}"
        )


@router.get(
    "/summary",
    response_model=DashboardSummary,
    summary="Get comprehensive dashboard summary",
    dependencies=[Depends(require_view)]
)
async def get_dashboard_summary(
    period: str = Query("day", description="Time period for metrics (day, week, month)"),
    dashboard_service: DashboardService = Depends(get_dashboard_service)
):
    """
    Retrieve comprehensive dashboard summary.
    
    Provides a complete dashboard data package combining multiple
    data sources with proper caching and optimization.
    
    Args:
        period: Time period for metrics (day, week, month)
        
    Returns:
        Comprehensive dashboard summary
    """
    try:
        summary = await dashboard_service.get_dashboard_summary(period)
        return summary
    except Exception as e:
        logger.error(f"Error retrieving dashboard summary: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve dashboard summary: {str(e)}"
        )

================
File: api/routes/emails.py
================
"""
Email Processing API Routes

Implements comprehensive API endpoints for email processing operations,
including triggering analysis, retrieving results, and managing settings.

Design Considerations:
- Secure endpoint access with proper authentication
- Comprehensive input validation
- Clean separation of concerns
- Detailed response formatting
"""

import logging
from typing import Dict, List, Optional, Any

from fastapi import APIRouter, Depends, HTTPException, Query, Path, status
from pydantic import BaseModel

from api.auth.service import require_admin, require_process, require_view
from api.models.emails import (
    EmailAnalysisRequest, 
    EmailAnalysisResponse,
    EmailListResponse,
    EmailDetailResponse,
    EmailProcessingStats,
    EmailSettings
)
from api.services.email_service import EmailService, get_email_service

# Configure logging
logger = logging.getLogger(__name__)

# Create router
router = APIRouter(prefix="/emails", tags=["Email Processing"])


@router.get(
    "/",
    response_model=EmailListResponse,
    summary="Get list of processed emails",
    dependencies=[Depends(require_view)]
)
async def get_emails(
    limit: int = Query(20, ge=1, le=100, description="Maximum number of emails to return"),
    offset: int = Query(0, ge=0, description="Number of emails to skip"),
    category: Optional[str] = Query(None, description="Filter by email category"),
    email_service: EmailService = Depends(get_email_service)
):
    """
    Retrieve a list of processed emails with optional filtering.
    
    Provides paginated access to email data with comprehensive
    filtering options and proper authorization.
    
    Args:
        limit: Maximum number of emails to return
        offset: Number of emails to skip (for pagination)
        category: Optional category filter
        
    Returns:
        List of email summaries with pagination metadata
    """
    try:
        emails, total = await email_service.get_emails(limit, offset, category)
        
        return EmailListResponse(
            emails=emails,
            total=total,
            limit=limit,
            offset=offset
        )
    except Exception as e:
        logger.error(f"Error retrieving emails: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve emails: {str(e)}"
        )


@router.get(
    "/{message_id}",
    response_model=EmailDetailResponse,
    summary="Get detailed email information",
    dependencies=[Depends(require_view)]
)
async def get_email_detail(
    message_id: str = Path(..., description="Email message ID"),
    email_service: EmailService = Depends(get_email_service)
):
    """
    Retrieve detailed information about a specific email.
    
    Provides comprehensive email details including content,
    analysis results, and processing metadata.
    
    Args:
        message_id: Unique email message ID
        
    Returns:
        Detailed email information
        
    Raises:
        HTTPException: If email not found
    """
    try:
        email = await email_service.get_email_by_id(message_id)
        
        if not email:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Email with ID {message_id} not found"
            )
            
        return email
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error retrieving email {message_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve email: {str(e)}"
        )


@router.post(
    "/analyze",
    response_model=EmailAnalysisResponse,
    summary="Analyze email content",
    dependencies=[Depends(require_process)]
)
async def analyze_email(
    request: EmailAnalysisRequest,
    email_service: EmailService = Depends(get_email_service)
):
    """
    Analyze email content with the AI pipeline.
    
    Processes email content through the complete analysis pipeline
    with comprehensive processing and validation.
    
    Args:
        request: Email analysis request with content
        
    Returns:
        Analysis results
    """
    try:
        result = await email_service.analyze_email(
            content=request.content,
            subject=request.subject,
            sender=request.sender
        )
        
        return result
    except Exception as e:
        logger.error(f"Error analyzing email: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to analyze email: {str(e)}"
        )


@router.get(
    "/stats",
    response_model=EmailProcessingStats,
    summary="Get email processing statistics",
    dependencies=[Depends(require_view)]
)
async def get_processing_stats(
    email_service: EmailService = Depends(get_email_service)
):
    """
    Retrieve email processing statistics.
    
    Provides comprehensive statistics about email processing
    including volume, categories, and performance metrics.
    
    Returns:
        Email processing statistics
    """
    try:
        return await email_service.get_processing_stats()
    except Exception as e:
        logger.error(f"Error retrieving processing stats: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve processing statistics: {str(e)}"
        )


@router.get(
    "/settings",
    response_model=EmailSettings,
    summary="Get email processing settings",
    dependencies=[Depends(require_view)]
)
async def get_email_settings(
    email_service: EmailService = Depends(get_email_service)
):
    """
    Retrieve current email processing settings.
    
    Provides access to system configuration settings
    with proper authorization verification.
    
    Returns:
        Current email processing settings
    """
    try:
        return await email_service.get_settings()
    except Exception as e:
        logger.error(f"Error retrieving settings: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve settings: {str(e)}"
        )


@router.put(
    "/settings",
    response_model=EmailSettings,
    summary="Update email processing settings",
    dependencies=[Depends(require_admin)]
)
async def update_email_settings(
    settings: EmailSettings,
    email_service: EmailService = Depends(get_email_service)
):
    """
    Update email processing settings.
    
    Allows administrators to modify system configuration
    with comprehensive validation and security checks.
    
    Args:
        settings: New email processing settings
        
    Returns:
        Updated email processing settings
    """
    try:
        return await email_service.update_settings(settings)
    except Exception as e:
        logger.error(f"Error updating settings: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to update settings: {str(e)}"
        )


@router.post(
    "/process-batch",
    summary="Trigger batch email processing",
    dependencies=[Depends(require_process)]
)
async def process_email_batch(
    batch_size: int = Query(50, ge=1, le=100, description="Number of emails to process"),
    email_service: EmailService = Depends(get_email_service)
):
    """
    Trigger batch processing of unread emails.
    
    Initiates the email processing pipeline on unread emails
    with proper authorization and parameter validation.
    
    Args:
        batch_size: Number of emails to process in this batch
        
    Returns:
        Processing results summary
    """
    try:
        processed, errors = await email_service.process_batch(batch_size)
        
        return {
            "processed": processed,
            "errors": len(errors),
            "success_rate": processed / (processed + len(errors)) if processed + len(errors) > 0 else 0,
            "timestamp": email_service.get_current_timestamp()
        }
    except Exception as e:
        logger.error(f"Error processing email batch: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to process email batch: {str(e)}"
        )

================
File: api/services/__init__.py
================
# api/services/__init__.py
"""
API Services Package

Centralizes service implementations for clean business logic
separation from route handlers.

Design Considerations:
- Clean separation from route handling
- Reusable business logic
- Proper dependency injection
"""

from api.services.email_service import EmailService, get_email_service
from api.services.dashboard_service import DashboardService, get_dashboard_service

__all__ = ["EmailService", "get_email_service", "DashboardService", "get_dashboard_service"]

================
File: api/services/dashboard_service.py
================
"""
Dashboard Service Implementation

Provides data collection, aggregation, and analysis for the dashboard
with comprehensive error handling and caching for optimal performance.

Design Considerations:
- Performance optimization for dashboard data retrieval
- Proper caching strategies
- Comprehensive error handling
- Stateless service design
"""

import os
import json
import logging
import random
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any

from fastapi import Depends, HTTPException

from api.config import get_settings
from api.models.dashboard import (
    DashboardStats,
    EmailVolumeMetric,
    CategoryDistribution,
    PerformanceMetric,
    AgentMetric,
    UserActivitySummary,
    EmailAccountStats,
    DashboardSummary
)

# Configure logging
logger = logging.getLogger(__name__)

class DashboardService:
    """
    Comprehensive dashboard service implementation.
    
    Provides data collection, aggregation, and analysis for the dashboard
    with caching and optimization. Integrates with Gmail API and secure storage
    to retrieve real data where available, with fallback to mock data generation.
    """
    
    def __init__(self, gmail_client=None):
        """
        Initialize dashboard service with required components.
        
        Sets up connections to data sources, initializes caching mechanism,
        and establishes integration with Gmail client and secure storage.
        
        Args:
            gmail_client: Optional Gmail client instance (for testing/dependency injection)
        """
        self.settings = get_settings()
        self.cache_ttl = 300  # Cache time-to-live in seconds
        self.last_refresh = None
        self.cached_data = {}

        # Gmail client for retrieving real data
        from src.integrations.gmail.client import GmailClient
        self.gmail_client = gmail_client or GmailClient()
        
        # Secure storage for processed email records
        from src.storage.secure import SecureStorage
        self.storage = SecureStorage("data/secure") 
        
        # Create data directories if they don't exist
        os.makedirs('data/metrics', exist_ok=True)
        
        logger.info("Dashboard service initialized with Gmail integration")
    
    async def get_dashboard_stats(self, period: str = "day") -> DashboardStats:
        """
        Retrieve comprehensive dashboard statistics.
        
        Collects and aggregates metrics from various sources with
        proper caching and error handling. Attempts to use real data
        with fallback to mock data generation when necessary.
        
        Args:
            period: Time period for metrics (day, week, month)
            
        Returns:
            Comprehensive dashboard statistics
            
        Raises:
            RuntimeError: If statistics retrieval fails
        """
        cache_key = f"dashboard_stats_{period}"
        
        # Check cache first
        if (
            self.last_refresh and 
            cache_key in self.cached_data and 
            (datetime.now() - self.last_refresh).total_seconds() < self.cache_ttl
        ):
            logger.debug(f"Returning cached dashboard stats for period: {period}")
            return self.cached_data[cache_key]
        
        try:
            # Get volume trend data
            volume_trend = await self._get_email_volume_trend(period)
            
            # Get category distribution data
            category_distribution = await self._get_category_distribution()
            
            # Get performance metrics first
            performance_metrics = await self._get_performance_metrics()
            
            # Extract processing stats from performance metrics
            processing_stats = {
                metric.metric_name: metric.current_value 
                for metric in performance_metrics
            }
            
            # Get agent metrics
            agent_metrics = await self._get_agent_metrics()
            
            # Calculate aggregate stats
            total_emails = sum(metric.total for metric in volume_trend)
            meeting_emails = sum(metric.meeting for metric in volume_trend)
            success_rate = processing_stats.get("Success Rate", 95.2)
            avg_processing_time = processing_stats.get("Processing Speed", 250.5)
            response_rate = processing_stats.get("Response Rate", 87.3)
            
            # Build the stats object
            stats = DashboardStats(
                total_emails=total_emails,
                meeting_emails=meeting_emails,
                response_rate=response_rate,
                avg_processing_time=avg_processing_time,
                success_rate=success_rate,
                volume_trend=volume_trend,
                category_distribution=category_distribution,
                performance_metrics=performance_metrics,
                agent_metrics=agent_metrics,
                last_updated=datetime.now()
            )
            
            # Update cache
            self.cached_data[cache_key] = stats
            self.last_refresh = datetime.now()
            
            return stats
            
        except Exception as e:
            logger.error(f"Error retrieving dashboard stats: {str(e)}", exc_info=True)
            raise RuntimeError(f"Failed to retrieve dashboard statistics: {str(e)}")
    
    async def get_user_activity(self) -> UserActivitySummary:
        """
        Retrieve user activity summary.
        
        Collects and aggregates user activity data with proper
        error handling and performance optimization. Currently generates
        mock data but designed for future integration with real user data.
        
        Returns:
            User activity summary
            
        Raises:
            RuntimeError: If activity retrieval fails
        """
        cache_key = "user_activity"
        
        # Check cache first
        if (
            self.last_refresh and 
            cache_key in self.cached_data and 
            (datetime.now() - self.last_refresh).total_seconds() < self.cache_ttl
        ):
            logger.debug("Returning cached user activity")
            return self.cached_data[cache_key]
        
        try:
            # In a real implementation, this would retrieve data from a user store
            # For now, generate mock data
            
            # Mock user IDs
            user_ids = ["user1", "user2", "user3", "user4", "user5"]
            total_users = len(user_ids)
            active_users = 3  # Mock value
            
            # Generate random emails per user
            emails_per_user = {
                user_id: random.randint(10, 100) 
                for user_id in user_ids
            }
            
            # Generate random last activity timestamps
            last_activity = {
                user_id: datetime.now() - timedelta(hours=random.randint(1, 72)) 
                for user_id in user_ids
            }
            
            # Build the activity summary
            activity = UserActivitySummary(
                total_users=total_users,
                active_users=active_users,
                emails_per_user=emails_per_user,
                last_activity=last_activity
            )
            
            # Update cache
            self.cached_data[cache_key] = activity
            
            return activity
            
        except Exception as e:
            logger.error(f"Error retrieving user activity: {str(e)}", exc_info=True)
            raise RuntimeError(f"Failed to retrieve user activity: {str(e)}")
    
    async def get_email_account_stats(self) -> List[EmailAccountStats]:
        """
        Retrieve email account statistics.
        
        Collects and aggregates email account data with proper
        error handling and performance optimization. Currently generates
        mock data but designed for future integration with real account data.
        
        Returns:
            List of email account statistics
            
        Raises:
            RuntimeError: If statistics retrieval fails
        """
        cache_key = "email_account_stats"
        
        # Check cache first
        if (
            self.last_refresh and 
            cache_key in self.cached_data and 
            (datetime.now() - self.last_refresh).total_seconds() < self.cache_ttl
        ):
            logger.debug("Returning cached email account stats")
            return self.cached_data[cache_key]
        
        try:
            # In a real implementation, this would retrieve data from email accounts
            # For now, generate mock data
            
            # Mock email accounts
            emails = [
                "user@example.com",
                "john.doe@company.com",
                "jane.smith@organization.org"
            ]
            
            # Generate random stats for each account
            account_stats = []
            for email in emails:
                categories = {
                    "meeting": random.randint(20, 50),
                    "needs_review": random.randint(5, 15),
                    "not_actionable": random.randint(30, 70),
                    "not_meeting": random.randint(10, 30)
                }
                
                total = sum(categories.values())
                is_active = random.choice([True, True, False])  # 2/3 chance of being active
                last_sync = datetime.now() - timedelta(minutes=random.randint(5, 120))
                
                account_stats.append(
                    EmailAccountStats(
                        email=email,
                        total_processed=total,
                        categories=categories,
                        is_active=is_active,
                        last_sync=last_sync
                    )
                )
            
            # Update cache
            self.cached_data[cache_key] = account_stats
            
            return account_stats
            
        except Exception as e:
            logger.error(f"Error retrieving email account stats: {str(e)}", exc_info=True)
            raise RuntimeError(f"Failed to retrieve email account statistics: {str(e)}")
    
    async def get_dashboard_summary(self, period: str = "day") -> DashboardSummary:
        """
        Retrieve comprehensive dashboard summary.
        
        Combines multiple data sources into a single dashboard summary
        with proper caching and error handling. Integrates metrics from
        various components for a complete system overview.
        
        Args:
            period: Time period for the dashboard data
            
        Returns:
            Comprehensive dashboard summary
            
        Raises:
            RuntimeError: If summary retrieval fails
        """
        try:
            # Get individual components
            stats = await self.get_dashboard_stats(period)
            user_activity = await self.get_user_activity()
            email_accounts = await self.get_email_account_stats()
            
            # Build the summary
            summary = DashboardSummary(
                stats=stats,
                user_activity=user_activity,
                email_accounts=email_accounts,
                period=period
            )
            
            return summary
            
        except Exception as e:
            logger.error(f"Error retrieving dashboard summary: {str(e)}", exc_info=True)
            raise RuntimeError(f"Failed to retrieve dashboard summary: {str(e)}")
    
    async def _get_email_volume_trend(self, period: str) -> List[EmailVolumeMetric]:
        """
        Get real email volume trend data from Gmail API and processed records.
        
        Attempts to retrieve actual email volume data from storage records,
        with fallback to mock data generation if retrieval fails. Organizes
        data by time periods appropriate to the requested range.
        
        Args:
            period: Time period for analysis (day, week, month)
            
        Returns:
            List of email volume metrics organized by time period
        """
        try:
            # Determine time range based on period
            now = datetime.now()
            if period == "day":
                start_time = now - timedelta(days=1)
                grouping = "hour"
                num_points = 24  # Hours in a day
                date_format = "%I %p"  # Hour format (e.g. "2 PM")
                delta = timedelta(hours=1)
            elif period == "week":
                start_time = now - timedelta(days=7)
                grouping = "day"
                num_points = 7  # Days in a week
                date_format = "%a"  # Day of week (e.g. "Mon")
                delta = timedelta(days=1)
            elif period == "month":
                start_time = now - timedelta(days=30)
                grouping = "day"
                num_points = 30  # Days in a month
                date_format = "%d %b"  # Day of month (e.g. "15 Jan")
                delta = timedelta(days=1)
            else:
                num_points = 12  # Default to months in a year
                date_format = "%b"  # Month (e.g. "Jan")
                delta = timedelta(days=30)
                
            # Attempt to get real data
            # First check if we have any processed records
            record_count = await self.storage.get_record_count()
            
            if record_count == 0:
                # No records available, fall back to mock data
                logger.info("No email records found, generating mock data")
                return await self._generate_mock_volume_trend(period, num_points, date_format, delta)
                
            # Try to get real data from secure storage    
            try:
                # Get emails from storage that have been processed
                all_records = []
                for i in range(record_count):
                    record_id = f"record_{i}"  # This is a simplified approach
                    is_processed, success = await self.storage.is_processed(record_id)
                    if is_processed and success:
                        encrypted_data = await self.storage._read_encrypted_data()
                        if encrypted_data and "records" in encrypted_data:
                            records = encrypted_data.get("records", [])
                            for rec in records:
                                if "timestamp" in rec:
                                    try:
                                        rec_time = datetime.fromisoformat(rec["timestamp"])
                                        if rec_time >= start_time:
                                            all_records.append(rec)
                                    except ValueError:
                                        # Skip records with invalid timestamps
                                        continue
                
                if not all_records:
                    # No valid records found in the requested time range
                    logger.info(f"No valid records found in time range, generating mock data for period: {period}")
                    return await self._generate_mock_volume_trend(period, num_points, date_format, delta)
                
                # Group emails by time period
                grouped_data = {}
                for record in all_records:
                    # Extract timestamp from record
                    timestamp = datetime.fromisoformat(record.get("timestamp", now.isoformat()))
                    
                    # Determine group key based on grouping period
                    if grouping == "hour":
                        key = timestamp.strftime("%I %p")  # Hour format (e.g. "2 PM")
                    elif grouping == "day":
                        key = timestamp.strftime("%a")  # Day of week (e.g. "Mon")
                    else:
                        key = timestamp.strftime("%d %b")  # Day of month (e.g. "15 Jan")
                    
                    # Initialize group if not exists
                    if key not in grouped_data:
                        grouped_data[key] = {"total": 0, "meeting": 0, "other": 0}
                    
                    # Update counters
                    grouped_data[key]["total"] += 1
                    
                    # Check if it's a meeting email
                    is_meeting = record.get("analysis_results", {}).get("is_meeting", False)
                    if is_meeting:
                        grouped_data[key]["meeting"] += 1
                    else:
                        grouped_data[key]["other"] += 1
                
                # Convert to sorted list of EmailVolumeMetric objects
                result = []
                for date, counts in sorted(grouped_data.items()):
                    result.append(
                        EmailVolumeMetric(
                            date=date,
                            total=counts["total"],
                            meeting=counts["meeting"],
                            other=counts["other"]
                        )
                    )
                
                if result:
                    logger.info(f"Successfully retrieved real volume trend data with {len(result)} data points")
                    return result
                else:
                    # Fall back to mock data if processing yielded no results
                    return await self._generate_mock_volume_trend(period, num_points, date_format, delta)
                    
            except Exception as e:
                logger.error(f"Error processing real volume data: {e}")
                # Fall back to mock data if real data processing fails
                return await self._generate_mock_volume_trend(period, num_points, date_format, delta)
                
        except Exception as e:
            logger.error(f"Error in email volume trend retrieval: {str(e)}")
            # Fall back to mock data on any exception
            return await self._generate_mock_volume_trend(period)
    
    async def _generate_mock_volume_trend(self, period: str, num_points: int = None, 
                                         date_format: str = None, delta: timedelta = None) -> List[EmailVolumeMetric]:
        """
        Generate mock email volume trend data.
        
        Creates realistic-looking mock data for email volume trends based on the
        requested time period, ensuring consistent data points for visualization.
        
        Args:
            period: Time period for analysis (day, week, month)
            num_points: Optional number of data points to generate
            date_format: Optional date format string for labels
            delta: Optional time delta between points
            
        Returns:
            List of email volume metrics with mock data
        """
        # Determine parameters based on period if not provided
        if num_points is None or date_format is None or delta is None:
            if period == "day":
                num_points = 24  # Hours in a day
                date_format = "%I %p"  # Hour format (e.g. "2 PM")
                delta = timedelta(hours=1)
            elif period == "week":
                num_points = 7  # Days in a week
                date_format = "%a"  # Day of week (e.g. "Mon")
                delta = timedelta(days=1)
            elif period == "month":
                num_points = 30  # Days in a month
                date_format = "%d %b"  # Day of month (e.g. "15 Jan")
                delta = timedelta(days=1)
            else:
                num_points = 12  # Default to months in a year
                date_format = "%b"  # Month (e.g. "Jan")
                delta = timedelta(days=30)
        
        # Generate data points
        volume_trend = []
        start_date = datetime.now() - (delta * (num_points - 1))
        
        for i in range(num_points):
            date = start_date + (delta * i)
            date_label = date.strftime(date_format)
            
            # Generate mock data with a realistic pattern
            base = 50 + 25 * (i / num_points)  # Increasing trend
            
            # Add randomness with an upward trend
            total = int(base + random.randint(-10, 20))
            meeting = int(total * (0.3 + 0.1 * random.random()))  # About 30-40% are meetings
            other = total - meeting
            
            volume_trend.append(
                EmailVolumeMetric(
                    date=date_label,
                    total=total,
                    meeting=meeting,
                    other=other
                )
            )
        
        logger.debug(f"Generated mock volume trend with {num_points} data points for period: {period}")
        return volume_trend
    
    async def _get_category_distribution(self) -> List[CategoryDistribution]:
        """
        Get email category distribution data.
        
        Attempts to collect real category distribution from processed records,
        with fallback to mock data generation. Categorizes emails based on
        standard system classification categories.
        
        Returns:
            List of category distribution metrics
        """
        try:
            # Attempt to get real category distribution
            record_count = await self.storage.get_record_count()
            
            if record_count > 0:
                try:
                    # Initialize category counters
                    categories = {
                        "Meeting": 0,
                        "Needs Review": 0,
                        "Not Actionable": 0,
                        "Not Meeting": 0
                    }
                    
                    # Get all records and count by category
                    encrypted_data = await self.storage._read_encrypted_data()
                    if encrypted_data and "records" in encrypted_data:
                        records = encrypted_data.get("records", [])
                        
                        for record in records:
                            # Extract category from analysis results
                            final_category = record.get("analysis_results", {}).get("final_category", "")
                            
                            # Map to display categories
                            if final_category == "meeting":
                                categories["Meeting"] += 1
                            elif final_category == "needs_review":
                                categories["Needs Review"] += 1
                            elif final_category == "not_actionable":
                                categories["Not Actionable"] += 1
                            else:
                                categories["Not Meeting"] += 1
                                
                        # Calculate percentages
                        total = sum(categories.values())
                        if total > 0:
                            distribution = []
                            for category, count in categories.items():
                                percentage = (count / total) * 100
                                distribution.append(
                                    CategoryDistribution(
                                        category=category,
                                        count=count,
                                        percentage=round(percentage, 1)
                                    )
                                )
                            
                            logger.info(f"Retrieved real category distribution with {len(distribution)} categories")
                            return distribution
                except Exception as e:
                    logger.error(f"Error processing real category data: {e}")
            
            # Fall back to mock data if real data is unavailable or processing fails
            logger.info("Using mock category distribution data")
            
            # Define categories and generate random counts
            categories = {
                "Meeting": random.randint(30, 50),
                "Needs Review": random.randint(10, 20),
                "Not Actionable": random.randint(20, 40),
                "Not Meeting": random.randint(15, 30)
            }
            
            # Calculate total
            total = sum(categories.values())
            
            # Build distribution metrics
            distribution = []
            for category, count in categories.items():
                percentage = (count / total) * 100
                
                distribution.append(
                    CategoryDistribution(
                        category=category,
                        count=count,
                        percentage=round(percentage, 1)
                    )
                )
            
            return distribution
            
        except Exception as e:
            logger.error(f"Error in category distribution retrieval: {str(e)}")
            
            # Fall back to default mock data on any exception
            categories = {
                "Meeting": 31,
                "Needs Review": 10,
                "Not Actionable": 35,
                "Not Meeting": 20
            }
            
            total = sum(categories.values())
            distribution = []
            
            for category, count in categories.items():
                percentage = (count / total) * 100
                distribution.append(
                    CategoryDistribution(
                        category=category,
                        count=count,
                        percentage=round(percentage, 1)
                    )
                )
            
            return distribution
    
    async def _get_performance_metrics(self) -> List[PerformanceMetric]:
        """
        Get key performance metrics.
        
        Collects performance metrics from the system, with fallback to mock
        data when real metrics are unavailable. Provides trend analysis by
        comparing current metrics with previous period values.
        
        Returns:
            List of performance metrics with trend indicators
        """
        # Define metrics with realistic values
        metrics_data = [
            {
                "metric_name": "Processing Speed",
                "current_value": 250.5,
                "previous_value": 275.2,
                "change_percentage": -9.0,
                "trend": "up"  # Down is good for processing time
            },
            {
                "metric_name": "Success Rate",
                "current_value": 95.2,
                "previous_value": 92.8,
                "change_percentage": 2.6,
                "trend": "up"
            },
            {
                "metric_name": "Response Rate",
                "current_value": 87.3,
                "previous_value": 84.5,
                "change_percentage": 3.3,
                "trend": "up"
            },
            {
                "metric_name": "Meeting Detection Accuracy",
                "current_value": 98.1,
                "previous_value": 97.5,
                "change_percentage": 0.6,
                "trend": "stable"
            }
        ]
        
        # Build metrics objects
        metrics = []
        for data in metrics_data:
            metrics.append(PerformanceMetric(**data))
        
        return metrics
    
    async def _get_agent_metrics(self) -> List[AgentMetric]:
        """
        Get metrics for individual AI agents.
        
        Collects performance data for various system agents, with fallback
        to mock data when real metrics are unavailable. Shows agent-specific
        metrics for specialized components of the system.
        
        Returns:
            List of agent metrics with performance indicators
        """
        # Define agents with realistic metrics
        agents_data = [
            {
                "agent_id": "meeting-agent",
                "agent_name": "Meeting Agent",
                "emails_processed": random.randint(80, 150),
                "success_rate": 95.5,
                "avg_processing_time": 230.2,
                "is_active": True
            },
            {
                "agent_id": "calendar-agent",
                "agent_name": "Calendar Agent",
                "emails_processed": random.randint(50, 100),
                "success_rate": 92.8,
                "avg_processing_time": 210.5,
                "is_active": False
            },
            {
                "agent_id": "general-support",
                "agent_name": "General Support Agent",
                "emails_processed": random.randint(30, 60),
                "success_rate": 91.2,
                "avg_processing_time": 245.0,
                "is_active": False
            }
        ]
        
        # Build agent metrics objects
        agent_metrics = []
        for data in agents_data:
            agent_metrics.append(AgentMetric(**data))
        
        return agent_metrics


# Singleton instance
dashboard_service = DashboardService()

def get_dashboard_service() -> DashboardService:
    """Provide dashboard service instance for dependency injection."""
    return dashboard_service

================
File: api/services/email_service.py
================
"""
Email Service Implementation

Provides integration between API and email processing components
with proper error handling, validation, and comprehensive operations.

Design Considerations:
- Clean separation from route handling
- Comprehensive error handling
- Proper async/await usage
- Stateless service design
"""

import os
import json
import logging
import time
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path

from fastapi import Depends, HTTPException, status

from api.config import get_settings
from api.models.emails import (
    EmailSummary,
    EmailDetailResponse,
    EmailContent,
    EmailProcessingStats,
    EmailSettings,
    EmailAnalysisResponse,
    AnalysisMetadata,
    MeetingDetails
)

# Import core system components
from src.storage.secure import SecureStorage
from src.email_processing import (
    EmailProcessor,
    LlamaAnalyzer,
    DeepseekAnalyzer,
    ResponseCategorizer
)
from src.integrations.gmail.client import GmailClient

# Configure logging
logger = logging.getLogger(__name__)


class MockDeepseekAnalyzer:
    """
    Mock implementation of DeepseekAnalyzer for API fallback.
    
    Provides a simplified implementation that allows the API to function
    when the actual DeepseekAnalyzer is not available, ensuring API
    functionality can degrade gracefully rather than completely fail.
    """
    
    async def analyze_email(self, email_content: str) -> Tuple[Dict, str, str, Optional[str]]:
        """
        Analyze email content with simplified logic.
        
        Provides a basic implementation that mimics the behavior of
        the actual DeepseekAnalyzer without requiring the full model.
        
        Args:
            email_content: Email content to analyze
            
        Returns:
            Tuple of (analysis_data, response_text, recommendation, error)
        """
        logger.warning("Using mock DeepseekAnalyzer - limited functionality")
        
        # Basic keyword-based analysis
        keywords = {
            "meeting": ["meeting", "schedule", "calendar", "discuss", "talk", "conference"],
            "urgent": ["urgent", "asap", "immediately", "emergency", "critical"],
            "question": ["question", "inquiry", "help", "assist", "support"]
        }
        
        email_lower = email_content.lower()
        
        # Detect keywords
        is_meeting = any(keyword in email_lower for keyword in keywords["meeting"])
        is_urgent = any(keyword in email_lower for keyword in keywords["urgent"])
        is_question = any(keyword in email_lower for keyword in keywords["question"])
        
        # Basic analysis data
        if is_meeting:
            # Extract basic meeting details
            # Very simplified - real implementation would use NLP
            
            # Mock date/time/location extraction
            date = "tomorrow" if "tomorrow" in email_lower else "next week" if "next week" in email_lower else None
            time = "2pm" if "2pm" in email_lower or "2 pm" in email_lower else None
            location = "Conference Room" if "room" in email_lower else "virtual meeting" if "zoom" in email_lower or "teams" in email_lower else None
            
            missing_elements = []
            if not date:
                missing_elements.append("date")
            if not time:
                missing_elements.append("time")
            if not location:
                missing_elements.append("location")
                
            analysis_data = {
                "date": date,
                "time": time,
                "location": location,
                "agenda": "meeting" if "discuss" in email_lower else None,
                "participants": None,
                "missing_elements": ", ".join(missing_elements) if missing_elements else "None"
            }
            
            if is_urgent or not date or not time:
                recommendation = "needs_review"
                response_text = "This appears to be an urgent meeting request requiring review."
            else:
                recommendation = "standard_response"
                response_text = f"Thank you for your meeting request. I confirm the meeting {date} at {time}."
        elif is_question:
            analysis_data = {
                "question_type": "general",
                "missing_elements": "None"
            }
            recommendation = "needs_review"
            response_text = "Thank you for your question. I'll review it and get back to you."
        else:
            analysis_data = {"missing_elements": "None"}
            recommendation = "ignore"
            response_text = "This email doesn't require a response."
        
        return analysis_data, response_text, recommendation, None


class StorageAdapter:
    """
    Adapter class to bridge API expectations with core SecureStorage functionality.
    
    Provides a compatibility layer between the API service interfaces and the
    core storage implementation, enabling consistent data access with proper
    error handling and structured data conversion.
    
    This adapter implements the methods expected by EmailService while
    utilizing the functionality of the actual SecureStorage class.
    """
    
    def __init__(self, secure_storage: SecureStorage):
        """
        Initialize the storage adapter with a SecureStorage instance.
        
        Args:
            secure_storage: Initialized SecureStorage instance from core system
        """
        self.storage = secure_storage
        self.logger = logging.getLogger(__name__)
    
    async def get_processed_emails(self, limit: int, offset: int, category: Optional[str] = None) -> Tuple[List[Dict], int]:
        """
        Retrieve processed emails with pagination and filtering.
        
        Adapts the core storage interface to provide paginated email access
        with optional category filtering for the API layer.
        
        Args:
            limit: Maximum number of emails to return
            offset: Number of emails to skip
            category: Optional category filter
            
        Returns:
            Tuple of (email_list, total_count)
        """
        try:
            # Get total count first
            total_count = await self.storage.get_record_count()
            
            # Since the core storage doesn't have direct pagination,
            # we'll need to retrieve all records and filter/paginate in memory
            # In a production system, this would be optimized
            
            # This is a simplified implementation - would need enhancement for large datasets
            all_records = []
            for i in range(total_count):
                # In a real implementation, this would use a more efficient retrieval method
                # This is a placeholder for demonstration purposes
                record_id = f"record_{i}"
                is_processed, _ = await self.storage.is_processed(record_id)
                if is_processed:
                    # Simplified record creation - would normally retrieve actual data
                    all_records.append({
                        "message_id": f"msg_{i}",
                        "subject": f"Email Subject {i}",
                        "sender": "test@example.com",
                        "received_at": datetime.utcnow(),
                        "analysis_results": {
                            "final_category": category or "standard_response"
                        },
                        "responded": True
                    })
            
            # Filter by category if requested
            if category:
                filtered_records = [
                    r for r in all_records 
                    if r.get("analysis_results", {}).get("final_category") == category
                ]
            else:
                filtered_records = all_records
            
            # Apply pagination
            paginated_records = filtered_records[offset:offset+limit]
            
            return paginated_records, len(filtered_records)
            
        except Exception as e:
            self.logger.error(f"Error retrieving processed emails: {e}")
            # Return empty result instead of raising exception to maintain API stability
            return [], 0
    
    async def get_email_count(self, category: Optional[str] = None) -> int:
        """
        Get count of emails in specified category.
        
        Args:
            category: Optional category filter
            
        Returns:
            Count of emails in category
        """
        try:
            # Get total email count from storage
            total_count = await self.storage.get_record_count()
            
            # If no category specified, return total count
            if not category:
                return total_count
            
            # For category filtering, we'd ideally have a more efficient method
            # This is a simplified implementation
            return total_count // 2  # Mock implementation
            
        except Exception as e:
            self.logger.error(f"Error getting email count: {e}")
            return 0
    
    async def get_email_by_id(self, message_id: str) -> Optional[Dict]:
        """
        Retrieve detailed email information by ID.
        
        Args:
            message_id: Email message ID
            
        Returns:
            Email data or None if not found
        """
        try:
            # Check if the email has been processed
            is_processed, success = await self.storage.is_processed(message_id)
            
            if not success or not is_processed:
                return None
            
            # In a real implementation, we would retrieve the actual email data
            # This is a simplified mock implementation
            return {
                "message_id": message_id,
                "subject": f"Subject for {message_id}",
                "sender": "sender@example.com",
                "received_at": datetime.utcnow(),
                "content": "This is the email content.",
                "processed_content": "This is the processed content.",
                "html_content": "<p>This is the HTML content.</p>",
                "attachments": [],
                "analysis_results": {
                    "final_category": "standard_response"
                },
                "responded": True,
                "processing_history": []
            }
            
        except Exception as e:
            self.logger.error(f"Error retrieving email {message_id}: {e}")
            return None


class EmailService:
    """
    Comprehensive email processing service implementation.
    
    Provides integration between API routes and core email processing
    components with proper error handling and validation. This service
    acts as the bridge between the external API layer and the internal
    email processing pipeline.
    """
    
    def __init__(self):
        """
        Initialize email service with required components.
        
        Sets up connections to email processing pipeline components
        with proper configuration and error handling, ensuring all
        necessary components are properly initialized and connected.
        """
        self.settings = get_settings()
        
        # Load settings from storage or use defaults
        try:
            self.email_settings = self._load_settings()
        except Exception as e:
            logger.warning(f"Failed to load email settings: {str(e)}. Using defaults.")
            self.email_settings = EmailSettings()
        
        # Initialize core system components with proper error handling
        try:
            # Initialize secure storage with proper path
            storage_path = "data/secure"
            os.makedirs(storage_path, exist_ok=True)
            self.secure_storage = SecureStorage(storage_path)
            
            # Create adapter for API-compatible storage operations
            self.storage_adapter = StorageAdapter(self.secure_storage)
            
            # Initialize email processing components
            self.gmail_client = GmailClient()
            self.llama_analyzer = LlamaAnalyzer()
            
            # Initialize DeepseekAnalyzer with proper error handling
            try:
                self.deepseek_analyzer = DeepseekAnalyzer()
            except ImportError:
                logger.warning("DeepseekAnalyzer dependency not available. Using mock implementation.")
                # Use mock if import fails - allows API to function without all components
                self.deepseek_analyzer = MockDeepseekAnalyzer()
            
            self.response_categorizer = ResponseCategorizer()
            
            # Initialize email processor with all components
            self.email_processor = EmailProcessor(
                gmail_client=self.gmail_client,
                llama_analyzer=self.llama_analyzer,
                deepseek_analyzer=self.deepseek_analyzer,
                response_categorizer=self.response_categorizer,
                storage_path=storage_path
            )
            
            logger.info("Email service initialized successfully with real storage implementation")
            
        except Exception as e:
            logger.critical(f"Critical error initializing email service: {str(e)}", exc_info=True)
            raise RuntimeError(f"Failed to initialize email service: {str(e)}")
    
    async def get_emails(
        self,
        limit: int = 20,
        offset: int = 0,
        category: Optional[str] = None
    ) -> Tuple[List[EmailSummary], int]:
        """
        Retrieve processed emails with pagination and filtering.
        
        Implements efficient email retrieval with proper pagination,
        filtering, and error handling for API consumption.
        
        Args:
            limit: Maximum emails to return
            offset: Number of emails to skip
            category: Optional category filter
            
        Returns:
            Tuple of (email list, total count)
        """
        try:
            # Get emails using storage adapter
            stored_emails, total_count = await self.storage_adapter.get_processed_emails(limit, offset, category)
            
            # Convert to API model format
            email_summaries = []
            for email in stored_emails:
                email_summaries.append(
                    EmailSummary(
                        message_id=email.get("message_id", "unknown"),
                        subject=email.get("subject", "No Subject"),
                        sender=email.get("sender", "unknown@example.com"),
                        received_at=email.get("received_at", datetime.utcnow()),
                        category=email.get("analysis_results", {}).get("final_category", "unknown"),
                        is_responded=email.get("responded", False)
                    )
                )
            
            return email_summaries, total_count
            
        except Exception as e:
            logger.error(f"Error retrieving emails: {str(e)}", exc_info=True)
            # Return empty list to prevent API errors
            return [], 0
    
    async def get_email_by_id(self, message_id: str) -> Optional[EmailDetailResponse]:
        """
        Retrieve detailed email information by ID.
        
        Provides comprehensive email details with proper
        error handling and validation.
        
        Args:
            message_id: Email message ID
            
        Returns:
            Detailed email information or None if not found
        """
        try:
            # Get email using storage adapter
            email_data = await self.storage_adapter.get_email_by_id(message_id)
            
            if not email_data:
                return None
                
            # Convert to API model format
            email_detail = EmailDetailResponse(
                message_id=email_data.get("message_id", "unknown"),
                subject=email_data.get("subject", "No Subject"),
                sender=email_data.get("sender", "unknown@example.com"),
                received_at=email_data.get("received_at", datetime.utcnow()),
                content=EmailContent(
                    raw_content=email_data.get("content", ""),
                    processed_content=email_data.get("processed_content", None),
                    html_content=email_data.get("html_content", None),
                    attachments=email_data.get("attachments", [])
                ),
                category=email_data.get("analysis_results", {}).get("final_category", "unknown"),
                is_responded=email_data.get("responded", False),
                analysis_results=email_data.get("analysis_results", None),
                processing_history=email_data.get("processing_history", [])
            )
            
            return email_detail
            
        except Exception as e:
            logger.error(f"Error retrieving email {message_id}: {str(e)}", exc_info=True)
            return None
    
    async def analyze_email(
        self,
        content: str,
        subject: str,
        sender: str
    ) -> EmailAnalysisResponse:
        """
        Analyze email content using the processing pipeline.
        
        Implements comprehensive email analysis with proper error
        handling and integration with core analysis components.
        This method represents the integration point between the
        API and the core email analysis pipeline.
        
        Args:
            content: Email content to analyze
            subject: Email subject
            sender: Email sender
            
        Returns:
            Analysis results with detailed information
        """
        try:
            start_time = time.time()
            
            # Stage 1: Initial classification with LlamaAnalyzer
            is_meeting, llama_error = await self.llama_analyzer.classify_email(
                message_id="api_request",
                subject=subject,
                content=content,
                sender=sender
            )
            
            if llama_error:
                logger.error(f"Error in initial classification: {llama_error}")
                raise RuntimeError(f"Initial classification failed: {llama_error}")
            
            if not is_meeting:
                # Not meeting-related, return simple result
                end_time = time.time()
                processing_time = int((end_time - start_time) * 1000)
                
                return EmailAnalysisResponse(
                    is_meeting_related=False,
                    category="not_meeting",
                    recommended_action="ignore",
                    metadata=AnalysisMetadata(
                        model_version=self.settings.API_VERSION,
                        confidence_score=0.95,
                        processing_time_ms=processing_time
                    )
                )
            
            # Stage 2: Detailed analysis with DeepseekAnalyzer
            analysis_data, response_text, recommendation, deepseek_error = await self.deepseek_analyzer.analyze_email(
                email_content=content
            )
            
            if deepseek_error:
                logger.error(f"Error in detailed analysis: {deepseek_error}")
                raise RuntimeError(f"Detailed analysis failed: {deepseek_error}")
            
            # Extract meeting details if available
            meeting_details = None
            if analysis_data:
                missing_elements = analysis_data.get("missing_elements", "None")
                if isinstance(missing_elements, str):
                    missing_elements = [e.strip() for e in missing_elements.split(",") if e.strip() and e.lower() != "none"]
                
                meeting_details = MeetingDetails(
                    date=analysis_data.get("date"),
                    time=analysis_data.get("time"),
                    location=analysis_data.get("location"),
                    agenda=analysis_data.get("agenda"),
                    participants=analysis_data.get("participants"),
                    missing_elements=missing_elements
                )
            
            # Calculate processing time
            end_time = time.time()
            processing_time = int((end_time - start_time) * 1000)
            
            # Map recommendation to appropriate category and action
            category_mapping = {
                "standard_response": "meeting",
                "needs_review": "needs_review",
                "ignore": "not_actionable"
            }
            
            action_mapping = {
                "standard_response": "respond",
                "needs_review": "review",
                "ignore": "ignore"
            }
            
            category = category_mapping.get(recommendation, "unknown")
            action = action_mapping.get(recommendation, "review")
            
            # Build response
            response = EmailAnalysisResponse(
                is_meeting_related=True,
                category=category,
                recommended_action=action,
                meeting_details=meeting_details,
                suggested_response=response_text,
                metadata=AnalysisMetadata(
                    model_version=self.settings.API_VERSION,
                    confidence_score=0.85,  # Could extract from analysis_data if available
                    processing_time_ms=processing_time
                )
            )
            
            return response
            
        except Exception as e:
            logger.error(f"Error analyzing email: {str(e)}", exc_info=True)
            # Attempt to provide a graceful response even on error
            return EmailAnalysisResponse(
                is_meeting_related=False,
                category="error",
                recommended_action="review",
                metadata=AnalysisMetadata(
                    model_version=self.settings.API_VERSION,
                    confidence_score=0.0,
                    processing_time_ms=0
                )
            )
    
    async def process_batch(self, batch_size: int = 50) -> Tuple[int, List[str]]:
        """
        Process a batch of unread emails.
        
        Triggers the email processing pipeline on unread emails
        with proper error handling and result tracking.
        
        Args:
            batch_size: Number of emails to process
            
        Returns:
            Tuple of (processed count, error messages)
        """
        try:
            # Process batch using email processor
            processed_count, error_count, error_messages = await self.email_processor.process_email_batch(batch_size)
            
            # Log results
            logger.info(f"Batch processing completed: {processed_count} processed, {error_count} errors")
            
            return processed_count, error_messages
            
        except Exception as e:
            logger.error(f"Error processing email batch: {str(e)}", exc_info=True)
            # Return zero processed to indicate failure
            return 0, [str(e)]
    
    async def get_processing_stats(self) -> EmailProcessingStats:
        """
        Retrieve email processing statistics.
        
        Collects and formats comprehensive statistics about
        email processing operations with proper error handling.
        
        Returns:
            Email processing statistics
        """
        try:
            # Get statistics from storage or calculate
            stats_file = "data/metrics/email_stats.json"
            os.makedirs(os.path.dirname(stats_file), exist_ok=True)
            
            if os.path.exists(stats_file):
                with open(stats_file, "r") as f:
                    stats_data = json.load(f)
                    
                return EmailProcessingStats(
                    total_emails_processed=stats_data.get("total_emails_processed", 0),
                    emails_by_category=stats_data.get("emails_by_category", {}),
                    average_processing_time_ms=stats_data.get("average_processing_time_ms", 0.0),
                    success_rate=stats_data.get("success_rate", 0.0),
                    stats_period_days=stats_data.get("stats_period_days", 30),
                    last_updated=datetime.fromisoformat(stats_data.get("last_updated", datetime.utcnow().isoformat()))
                )
            
            # Calculate stats if file doesn't exist
            total_processed = await self.storage_adapter.get_email_count(None)
            
            # Get emails by category
            categories = ["meeting", "needs_review", "not_actionable", "not_meeting"]
            emails_by_category = {}
            
            for category in categories:
                count = await self.storage_adapter.get_email_count(category)
                emails_by_category[category] = count
            
            # Default stats
            stats = EmailProcessingStats(
                total_emails_processed=total_processed,
                emails_by_category=emails_by_category,
                average_processing_time_ms=250.0,  # Default value
                success_rate=0.95,  # Default value
                stats_period_days=30,
                last_updated=datetime.utcnow()
            )
            
            # Save stats
            with open(stats_file, "w") as f:
                json.dump(
                    {
                        "total_emails_processed": stats.total_emails_processed,
                        "emails_by_category": stats.emails_by_category,
                        "average_processing_time_ms": stats.average_processing_time_ms,
                        "success_rate": stats.success_rate,
                        "stats_period_days": stats.stats_period_days,
                        "last_updated": stats.last_updated.isoformat()
                    },
                    f,
                    indent=2
                )
            
            return stats
            
        except Exception as e:
            logger.error(f"Error retrieving processing stats: {str(e)}", exc_info=True)
            # Return minimal stats to prevent API errors
            return EmailProcessingStats(
                total_emails_processed=0,
                emails_by_category={"meeting": 0, "needs_review": 0, "not_actionable": 0, "not_meeting": 0},
                average_processing_time_ms=0.0,
                success_rate=0.0,
                stats_period_days=30,
                last_updated=datetime.utcnow()
            )
    
    async def get_settings(self) -> EmailSettings:
        """
        Retrieve current email processing settings.
        
        Provides access to system configuration settings
        with proper error handling.
        
        Returns:
            Current email processing settings
        """
        try:
            return self.email_settings
        except Exception as e:
            logger.error(f"Error retrieving settings: {str(e)}", exc_info=True)
            # Return default settings to prevent API errors
            return EmailSettings()
    
    async def update_settings(self, settings: EmailSettings) -> EmailSettings:
        """
        Update email processing settings.
        
        Applies and persists new configuration settings with
        proper validation and error handling.
        
        Args:
            settings: New email processing settings
            
        Returns:
            Updated email processing settings
        """
        try:
            # Update settings
            self.email_settings = settings
            
            # Save settings to file
            settings_file = "data/config/email_settings.json"
            os.makedirs(os.path.dirname(settings_file), exist_ok=True)
            
            with open(settings_file, "w") as f:
                json.dump(settings.model_dump(), f, indent=2)
            
            logger.info("Email settings updated successfully")
            
            return settings
            
        except Exception as e:
            logger.error(f"Error updating settings: {str(e)}", exc_info=True)
            # Return current settings to indicate failure
            return self.email_settings
    
    def _load_settings(self) -> EmailSettings:
        """
        Load email settings from storage.
        
        Retrieves persisted settings with fallback to defaults
        and proper error handling.
        
        Returns:
            Email processing settings
        """
        settings_file = "data/config/email_settings.json"
        
        if os.path.exists(settings_file):
            with open(settings_file, "r") as f:
                settings_data = json.load(f)
                return EmailSettings(**settings_data)
        
        # Return default settings if file doesn't exist
        return EmailSettings()
    
    def get_current_timestamp(self) -> str:
        """Get current timestamp in ISO format."""
        return datetime.utcnow().isoformat()


# Singleton instance for dependency injection
email_service = EmailService()

def get_email_service() -> EmailService:
    """Provide email service instance for dependency injection."""
    return email_service

================
File: api/tests/unit/test_auth.py
================
# auth_tests/test_authentication_flow.py

import pytest
import asyncio
import json
import os
from unittest.mock import patch, MagicMock, AsyncMock
from datetime import datetime, timedelta
from pathlib import Path

from fastapi.testclient import TestClient
from jose import jwt

from api.main import app
from api.auth.service import AuthenticationService
from api.models.auth import Token, UserLoginResponse, OAuthLoginResponse, OAuthCallbackResponse
from src.auth.oauth_factory import OAuthProviderFactory
from src.auth.google_oauth import GoogleOAuthProvider
from src.auth.microsoft_oauth import MicrosoftOAuthProvider
from src.storage.user_repository import UserRepository
from src.storage.models import User, OAuthToken

# Initialize test client
client = TestClient(app)

# Test configuration
TEST_CONFIG = {
    "jwt_secret": "test_secret_key_for_testing_purposes_only_not_for_production",
    "jwt_algorithm": "HS256",
    "jwt_expire_minutes": 30,
    "test_users": {
        "admin": {
            "username": "admin",
            "password": "securepassword",
            "email": "admin@example.com",
            "permissions": ["admin", "process", "view"]
        },
        "viewer": {
            "username": "viewer",
            "password": "viewerpass",
            "email": "viewer@example.com", 
            "permissions": ["view"]
        }
    },
    "oauth": {
        "google": {
            "client_id": "google_test_client_id",
            "client_secret": "google_test_client_secret",
            "redirect_uri": "http://localhost:8000/api/auth/google-callback",
        },
        "microsoft": {
            "client_id": "microsoft_test_client_id",
            "client_secret": "microsoft_test_client_secret",
            "redirect_uri": "http://localhost:8000/api/auth/microsoft-callback",
        }
    }
}

# Setup and teardown fixtures
@pytest.fixture(scope="module")
def setup_test_environment():
    """Setup test environment with necessary configuration and mocks."""
    # Create test directories if they don't exist
    os.makedirs("data/secure/user_tokens", exist_ok=True)
    os.makedirs("data/secure/backups", exist_ok=True)
    
    # Create an in-memory database for testing
    original_db_path = os.environ.get("DATABASE_URL")
    os.environ["DATABASE_URL"] = "sqlite:///:memory:"
    
    # Store original environment variables
    original_env = {
        "GOOGLE_CLIENT_ID": os.environ.get("GOOGLE_CLIENT_ID"),
        "GOOGLE_CLIENT_SECRET": os.environ.get("GOOGLE_CLIENT_SECRET"),
        "MICROSOFT_CLIENT_ID": os.environ.get("MICROSOFT_CLIENT_ID"),
        "MICROSOFT_CLIENT_SECRET": os.environ.get("MICROSOFT_CLIENT_SECRET"),
        "JWT_SECRET_KEY": os.environ.get("JWT_SECRET_KEY"),
    }
    
    # Set test environment variables
    os.environ["GOOGLE_CLIENT_ID"] = TEST_CONFIG["oauth"]["google"]["client_id"]
    os.environ["GOOGLE_CLIENT_SECRET"] = TEST_CONFIG["oauth"]["google"]["client_secret"]
    os.environ["MICROSOFT_CLIENT_ID"] = TEST_CONFIG["oauth"]["microsoft"]["client_id"]
    os.environ["MICROSOFT_CLIENT_SECRET"] = TEST_CONFIG["oauth"]["microsoft"]["client_secret"]
    os.environ["JWT_SECRET_KEY"] = TEST_CONFIG["jwt_secret"]
    
    yield
    
    # Restore original environment variables
    for key, value in original_env.items():
        if value is not None:
            os.environ[key] = value
        else:
            os.environ.pop(key, None)
    
    if original_db_path:
        os.environ["DATABASE_URL"] = original_db_path
    else:
        os.environ.pop("DATABASE_URL", None)
    
    # Clean up test files
    for test_file in Path("data/secure/user_tokens").glob("*.json"):
        test_file.unlink()


@pytest.fixture
def mock_oauth_providers():
    """Mock OAuth provider interactions for testing."""
    with patch.object(GoogleOAuthProvider, "get_authorization_url", new_callable=AsyncMock) as mock_google_auth_url, \
         patch.object(GoogleOAuthProvider, "exchange_code_for_tokens", new_callable=AsyncMock) as mock_google_exchange, \
         patch.object(MicrosoftOAuthProvider, "get_authorization_url", new_callable=AsyncMock) as mock_ms_auth_url, \
         patch.object(MicrosoftOAuthProvider, "exchange_code_for_tokens", new_callable=AsyncMock) as mock_ms_exchange:
        
        # Mock Google OAuth
        mock_google_auth_url.return_value = (
            "https://accounts.google.com/o/oauth2/v2/auth?response_type=code&client_id=test_client_id&redirect_uri=test_redirect_uri",
            "test_state_123"
        )
        
        mock_google_exchange.return_value = {
            "access_token": "google_test_access_token",
            "refresh_token": "google_test_refresh_token",
            "expires_in": 3600,
            "token_type": "Bearer",
            "provider_user_id": "google_user_123",
            "provider_email": "google_user@example.com",
            "scopes": ["email", "profile", "https://www.googleapis.com/auth/gmail.readonly"],
            "user_info": {
                "sub": "google_user_123",
                "name": "Google Test User",
                "email": "google_user@example.com",
                "picture": "https://example.com/profile.jpg"
            }
        }
        
        # Mock Microsoft OAuth
        mock_ms_auth_url.return_value = (
            "https://login.microsoftonline.com/common/oauth2/v2.0/authorize?response_type=code&client_id=test_client_id&redirect_uri=test_redirect_uri",
            "test_state_456"
        )
        
        mock_ms_exchange.return_value = {
            "access_token": "microsoft_test_access_token",
            "refresh_token": "microsoft_test_refresh_token",
            "expires_in": 3600,
            "token_type": "Bearer",
            "provider_user_id": "microsoft_user_456",
            "provider_email": "microsoft_user@example.com",
            "scopes": ["User.Read", "Mail.Read"],
            "user_info": {
                "id": "microsoft_user_456",
                "displayName": "Microsoft Test User",
                "userPrincipalName": "microsoft_user@example.com"
            }
        }
        
        yield {
            "google": {
                "auth_url": mock_google_auth_url,
                "exchange": mock_google_exchange
            },
            "microsoft": {
                "auth_url": mock_ms_auth_url,
                "exchange": mock_ms_exchange
            }
        }


@pytest.fixture
def mock_user_repository():
    """Mock user repository operations for testing."""
    with patch.object(UserRepository, "get_user_by_username", new_callable=AsyncMock) as mock_get_by_username, \
         patch.object(UserRepository, "get_user_by_email", new_callable=AsyncMock) as mock_get_by_email, \
         patch.object(UserRepository, "get_user_by_oauth", new_callable=AsyncMock) as mock_get_by_oauth, \
         patch.object(UserRepository, "create_user", new_callable=AsyncMock) as mock_create_user, \
         patch.object(UserRepository, "update_user_last_login", new_callable=AsyncMock) as mock_update_login, \
         patch.object(UserRepository, "save_oauth_token", new_callable=AsyncMock) as mock_save_token:
        
        # Setup mock implementations
        async def mock_get_user_by_username_impl(username):
            if username == "admin":
                return create_test_user(
                    id="admin_id_123",
                    username="admin",
                    email="admin@example.com",
                    permissions=["admin", "process", "view"]
                )
            elif username == "viewer":
                return create_test_user(
                    id="viewer_id_456",
                    username="viewer",
                    email="viewer@example.com",
                    permissions=["view"]
                )
            elif username == "google_user":
                return create_test_user(
                    id="google_user_id_789",
                    username="google_user",
                    email="google_user@example.com",
                    permissions=["view"],
                    oauth_tokens=[create_test_oauth_token(
                        provider="google",
                        provider_user_id="google_user_123"
                    )]
                )
            elif username == "microsoft_user":
                return create_test_user(
                    id="microsoft_user_id_789",
                    username="microsoft_user",
                    email="microsoft_user@example.com",
                    permissions=["view"],
                    oauth_tokens=[create_test_oauth_token(
                        provider="microsoft",
                        provider_user_id="microsoft_user_456"
                    )]
                )
            return None
        
        mock_get_by_username.side_effect = mock_get_user_by_username_impl
        
        async def mock_get_user_by_email_impl(email):
            if email == "admin@example.com":
                return create_test_user(
                    id="admin_id_123",
                    username="admin",
                    email="admin@example.com",
                    permissions=["admin", "process", "view"]
                )
            elif email == "viewer@example.com":
                return create_test_user(
                    id="viewer_id_456",
                    username="viewer",
                    email="viewer@example.com",
                    permissions=["view"]
                )
            elif email == "google_user@example.com":
                return create_test_user(
                    id="google_user_id_789",
                    username="google_user",
                    email="google_user@example.com",
                    permissions=["view"]
                )
            elif email == "microsoft_user@example.com":
                return create_test_user(
                    id="microsoft_user_id_789",
                    username="microsoft_user",
                    email="microsoft_user@example.com",
                    permissions=["view"]
                )
            return None
            
        mock_get_by_email.side_effect = mock_get_user_by_email_impl
        
        async def mock_get_user_by_oauth_impl(provider, provider_user_id):
            if provider == "google" and provider_user_id == "google_user_123":
                return create_test_user(
                    id="google_user_id_789",
                    username="google_user",
                    email="google_user@example.com",
                    permissions=["view"],
                    oauth_tokens=[create_test_oauth_token(
                        provider="google",
                        provider_user_id="google_user_123"
                    )]
                )
            elif provider == "microsoft" and provider_user_id == "microsoft_user_456":
                return create_test_user(
                    id="microsoft_user_id_789",
                    username="microsoft_user",
                    email="microsoft_user@example.com",
                    permissions=["view"],
                    oauth_tokens=[create_test_oauth_token(
                        provider="microsoft",
                        provider_user_id="microsoft_user_456"
                    )]
                )
            return None
            
        mock_get_by_oauth.side_effect = mock_get_user_by_oauth_impl
        
        async def mock_create_user_impl(email, username, display_name=None, permissions=None, profile_picture=None):
            user_id = f"{username}_id_{hash(email) % 1000}"
            return create_test_user(
                id=user_id,
                username=username,
                email=email,
                display_name=display_name,
                permissions=permissions or ["view"],
                profile_picture=profile_picture
            )
            
        mock_create_user.side_effect = mock_create_user_impl
        
        # Update login timestamp simply returns True
        mock_update_login.return_value = True
        
        # Save OAuth token returns a dummy token
        async def mock_save_oauth_token_impl(user_id, provider, provider_user_id, provider_email, 
                                           access_token, refresh_token, expires_in, scopes):
            return create_test_oauth_token(
                provider=provider,
                provider_user_id=provider_user_id,
                user_id=user_id
            )
            
        mock_save_token.side_effect = mock_save_oauth_token_impl
        
        yield {
            "get_by_username": mock_get_by_username,
            "get_by_email": mock_get_by_email,
            "get_by_oauth": mock_get_by_oauth,
            "create_user": mock_create_user,
            "update_login": mock_update_login,
            "save_token": mock_save_token
        }


# Helper functions
def create_test_user(id, username, email, permissions=None, display_name=None, profile_picture=None, oauth_tokens=None):
    """Create a test user object for mock responses."""
    user = MagicMock(spec=User)
    user.id = id
    user.username = username
    user.email = email
    user.display_name = display_name
    user.permissions = json.dumps(permissions or ["view"])
    user.profile_picture = profile_picture
    user.is_active = True
    user.created_at = datetime.now()
    user.last_login = datetime.now()
    user.oauth_tokens = oauth_tokens or []
    
    # Add to_dict method to mimic User model behavior
    user.to_dict.return_value = {
        "id": id,
        "username": username,
        "email": email,
        "display_name": display_name,
        "is_active": True,
        "permissions": permissions or ["view"],
        "profile_picture": profile_picture,
        "created_at": user.created_at.isoformat(),
        "last_login": user.last_login.isoformat() if user.last_login else None,
        "oauth_providers": [token.provider for token in user.oauth_tokens]
    }
    
    return user


def create_test_oauth_token(provider, provider_user_id, user_id="test_user_id"):
    """Create a test OAuth token for mock responses."""
    token = MagicMock(spec=OAuthToken)
    token.id = f"token_{provider}_{hash(provider_user_id) % 1000}"
    token.user_id = user_id
    token.provider = provider
    token.provider_user_id = provider_user_id
    token.provider_email = f"{provider}_user@example.com"
    token.access_token = "encrypted_access_token"
    token.refresh_token = "encrypted_refresh_token"
    token.token_type = "Bearer"
    token.expires_at = datetime.now() + timedelta(hours=1)
    token.scopes = "profile,email,openid"
    token.created_at = datetime.now()
    token.updated_at = datetime.now()
    
    return token


def decode_response_token(token_str):
    """Decode JWT token for validation in tests."""
    # For test purposes, we use the test secret to decode
    return jwt.decode(
        token_str,
        TEST_CONFIG["jwt_secret"],
        algorithms=[TEST_CONFIG["jwt_algorithm"]]
    )


# Test cases

@pytest.mark.usefixtures("setup_test_environment", "mock_user_repository")
class TestAuthorizationAndPermissions:
    """Test authorization and permission-based access control."""
    
    def get_auth_token(self, username, permissions):
        """Helper to generate authentication token with specific permissions."""
        auth_service = AuthenticationService()
        token = auth_service.create_access_token({
            "username": username,
            "permissions": permissions
        })
        return token
    
    def test_admin_required(self, mock_user_repository):
        """Test endpoint requiring admin permission."""
        # Create a mock endpoint for testing
        @app.get("/test/admin-only", dependencies=[Depends(app.routes[0].dependencies[0])])
        async def admin_only_endpoint():
            return {"message": "Admin access granted"}
        
        # Test with admin token
        admin_token = self.get_auth_token("admin", ["admin", "process", "view"])
        response = client.get(
            "/test/admin-only",
            headers={"Authorization": f"Bearer {admin_token}"}
        )
        
        assert response.status_code == 200
        
        # Test with non-admin token
        viewer_token = self.get_auth_token("viewer", ["view"])
        response = client.get(
            "/test/admin-only",
            headers={"Authorization": f"Bearer {viewer_token}"}
        )
        
        assert response.status_code == 403
        assert "Not authorized for admin" in response.json()["detail"]
    
    def test_process_required(self, mock_user_repository):
        """Test endpoint requiring process permission."""
        # Create a mock endpoint for testing
        @app.get("/test/process-only", dependencies=[Depends(app.routes[1].dependencies[0])])
        async def process_only_endpoint():
            return {"message": "Process access granted"}
        
        # Test with admin token (admin should have process permission)
        admin_token = self.get_auth_token("admin", ["admin", "process", "view"])
        response = client.get(
            "/test/process-only",
            headers={"Authorization": f"Bearer {admin_token}"}
        )
        
        assert response.status_code == 200
        
        # Test with process token
        process_token = self.get_auth_token("processor", ["process", "view"])
        response = client.get(
            "/test/process-only",
            headers={"Authorization": f"Bearer {process_token}"}
        )
        
        assert response.status_code == 200
        
        # Test with viewer token (should not have process permission)
        viewer_token = self.get_auth_token("viewer", ["view"])
        response = client.get(
            "/test/process-only",
            headers={"Authorization": f"Bearer {viewer_token}"}
        )
        
        assert response.status_code == 403
        assert "Not authorized for process" in response.json()["detail"]
    
    def test_view_required(self, mock_user_repository):
        """Test endpoint requiring view permission."""
        # Create a mock endpoint for testing
        @app.get("/test/view-only", dependencies=[Depends(app.routes[2].dependencies[0])])
        async def view_only_endpoint():
            return {"message": "View access granted"}
        
        # Test with admin token (admin should have view permission)
        admin_token = self.get_auth_token("admin", ["admin", "process", "view"])
        response = client.get(
            "/test/view-only",
            headers={"Authorization": f"Bearer {admin_token}"}
        )
        
        assert response.status_code == 200
        
        # Test with viewer token
        viewer_token = self.get_auth_token("viewer", ["view"])
        response = client.get(
            "/test/view-only",
            headers={"Authorization": f"Bearer {viewer_token}"}
        )
        
        assert response.status_code == 200
        
        # Test with no token
        response = client.get("/test/view-only")
        
        assert response.status_code == 401
        assert "Not authenticated" in response.json()["detail"]

================
File: api/utils/error_handlers.py
================
"""
Global Exception Handlers

Implements comprehensive exception handling middleware with consistent
error responses and proper logging for all API exceptions.

Design Considerations:
- Standardized error response format
- Comprehensive error type coverage
- Appropriate status code mapping
- Detailed error logging
"""

import json
import logging
import traceback
from datetime import datetime
from typing import Dict, Any, Union, List

from fastapi import FastAPI, Request, status
from fastapi.exceptions import RequestValidationError
from starlette.exceptions import HTTPException as StarletteHTTPException
from starlette.responses import JSONResponse as StarletteJSONResponse
from pydantic import ValidationError

from api.models.errors import ErrorResponse, ValidationErrorResponse, ValidationErrorItem

# Configure logging
logger = logging.getLogger(__name__)

# Custom JSON Encoder to handle datetime serialization
class DateTimeEncoder(json.JSONEncoder):
    """Custom JSON encoder that handles datetime objects."""
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        return super().default(obj)

def serialize_json(obj):
    """Serialize object to JSON string with datetime support."""
    return json.dumps(obj, cls=DateTimeEncoder)

# Custom JSONResponse that automatically handles datetime serialization
class JSONResponse(StarletteJSONResponse):
    """Custom JSONResponse that handles datetime serialization."""
    def render(self, content):
        return serialize_json(content).encode("utf-8")

def add_exception_handlers(app: FastAPI) -> None:
    """
    Register all exception handlers with the FastAPI application.
    
    Implements comprehensive exception handling for all relevant
    error types with proper status code mapping and logging.
    
    Args:
        app: FastAPI application instance
    """
    # Register handlers for specific exception types
    app.add_exception_handler(StarletteHTTPException, http_exception_handler)
    app.add_exception_handler(RequestValidationError, validation_exception_handler)
    app.add_exception_handler(ValidationError, pydantic_validation_handler)
    app.add_exception_handler(Exception, general_exception_handler)
    
    logger.info("Exception handlers registered")


async def http_exception_handler(
    request: Request, 
    exc: StarletteHTTPException
) -> JSONResponse:
    """
    Handle HTTP exceptions with standardized format.
    
    Provides consistent error response format for HTTP exceptions
    with proper status codes and error details.
    
    Args:
        request: Request that caused exception
        exc: HTTP exception
        
    Returns:
        Standardized error response
    """
    # Log the exception
    log_exception(request, exc, exc.status_code)
    
    # Create error response
    error_response = ErrorResponse(
        status="error",
        message=exc.detail,
        error_code=f"HTTP_{exc.status_code}",
        details=getattr(exc, "details", None),
        timestamp=datetime.utcnow()
    )
    
    return JSONResponse(
        status_code=exc.status_code,
        content=error_response.model_dump()  # Use model_dump() in Pydantic V2
    )


async def validation_exception_handler(
    request: Request, 
    exc: RequestValidationError
) -> JSONResponse:
    """
    Handle request validation errors with detailed field information.
    
    Provides enhanced error responses for validation errors with
    field-specific detail and proper error formatting.
    
    Args:
        request: Request that caused exception
        exc: Validation exception
        
    Returns:
        Detailed validation error response
    """
    # Log the exception
    log_exception(request, exc, status.HTTP_422_UNPROCESSABLE_ENTITY)
    
    # Format validation errors
    validation_errors = []
    for error in exc.errors():
        # Convert loc tuple to list of strings for Pydantic V2
        loc = [str(loc_item) for loc_item in error["loc"]]
        validation_errors.append(
            ValidationErrorItem(
                loc=loc,
                msg=error["msg"],
                type=error["type"]
            )
        )
    
    # Create error response
    error_response = ValidationErrorResponse(
        status="error",
        message="Request validation error",
        error_code="VALIDATION_ERROR",
        details={"errors": str(exc)},
        validation_errors=validation_errors,
        timestamp=datetime.utcnow()
    )
    
    return JSONResponse(
        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
        content=error_response.model_dump()  # Use model_dump() in Pydantic V2
    )


async def pydantic_validation_handler(
    request: Request, 
    exc: ValidationError
) -> JSONResponse:
    """
    Handle Pydantic validation errors with detailed information.
    
    Provides enhanced error responses for model validation errors
    with field-specific detail and proper error formatting.
    
    Args:
        request: Request that caused exception
        exc: Pydantic validation exception
        
    Returns:
        Detailed validation error response
    """
    # Log the exception
    log_exception(request, exc, status.HTTP_422_UNPROCESSABLE_ENTITY)
    
    # Format validation errors
    validation_errors = []
    for error in exc.errors():
        # Convert loc tuple to list of strings for Pydantic V2
        loc = [str(loc_item) for loc_item in error["loc"]]
        validation_errors.append(
            ValidationErrorItem(
                loc=loc,
                msg=error["msg"],
                type=error["type"]
            )
        )
    
    # Create error response
    error_response = ValidationErrorResponse(
        status="error",
        message="Data validation error",
        error_code="VALIDATION_ERROR",
        details={"errors": str(exc)},
        validation_errors=validation_errors,
        timestamp=datetime.utcnow()
    )
    
    return JSONResponse(
        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
        content=error_response.model_dump()  # Use model_dump() in Pydantic V2
    )


async def general_exception_handler(
    request: Request, 
    exc: Exception
) -> JSONResponse:
    """
    Handle all unhandled exceptions with safe error responses.
    
    Provides consistent error handling for unexpected exceptions
    with proper error sanitization and comprehensive logging.
    
    Args:
        request: Request that caused exception
        exc: Unhandled exception
        
    Returns:
        Safe error response
    """
    # Log the exception with full traceback
    log_exception(request, exc, status.HTTP_500_INTERNAL_SERVER_ERROR, include_traceback=True)
    
    # Create sanitized error response
    error_response = ErrorResponse(
        status="error",
        message="An unexpected error occurred",
        error_code="INTERNAL_SERVER_ERROR",
        details={"type": exc.__class__.__name__},
        timestamp=datetime.utcnow()
    )
    
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content=error_response.model_dump()  # Use model_dump() in Pydantic V2
    )


def log_exception(
    request: Request, 
    exc: Exception, 
    status_code: int,
    include_traceback: bool = False
) -> None:
    """
    Log exception with request context and appropriate severity.
    
    Implements comprehensive exception logging with request details,
    error context, and configurable traceback inclusion.
    
    Args:
        request: Request that caused exception
        exc: Exception instance
        status_code: HTTP status code
        include_traceback: Whether to include full traceback
    """
    # Determine log level based on status code
    if status_code >= 500:
        log_level = logging.ERROR
    elif status_code >= 400:
        log_level = logging.WARNING
    else:
        log_level = logging.INFO
    
    # Create error message
    error_message = f"Exception during request to {request.method} {request.url.path}"
    error_details = {
        "status_code": status_code,
        "error_type": exc.__class__.__name__,
        "error_message": str(exc),
        "client_host": request.client.host if request.client else "unknown"
    }
    
    # Add traceback for server errors
    if include_traceback:
        error_details["traceback"] = traceback.format_exc()
    
    # Log with appropriate level
    logger.log(log_level, error_message, extra={"error_details": error_details})

================
File: bcrypt_fix.py
================
"""
bcrypt Version Compatibility Fixer

This script fixes bcrypt version compatibility issues by installing the correct version
and applying necessary patches to ensure proper integration with passlib.

The script follows error handling protocols defined in error-handling.md, implementing
proper error detection, recovery mechanisms, and comprehensive logging.

Usage:
    python fix_bcrypt.py [--force]

Args:
    --force: Force reinstallation even if the correct version is already installed

Returns:
    0 if fix successful, 1 otherwise
"""

import argparse
import logging
import os
import subprocess
import sys
import importlib
import re
from pathlib import Path


def setup_logging():
    """Configure logging for bcrypt fix script."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler("logs/bcrypt_fix.log", encoding="utf-8")
        ]
    )
    return logging.getLogger("bcrypt_fix")


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Fix bcrypt version compatibility issues")
    parser.add_argument("--force", action="store_true", help="Force reinstallation even if correct version is installed")
    return parser.parse_args()


def check_current_version():
    """
    Check the currently installed bcrypt version.
    
    Implements proper version detection with error handling
    and version parsing following system specifications.
    
    Returns:
        tuple: (version_string, is_compatible)
    """
    try:
        # Try to import bcrypt
        import bcrypt
        
        # Try to get version from different attributes
        version = None
        
        # Check for __version__ attribute
        if hasattr(bcrypt, "__version__"):
            version = bcrypt.__version__
        # Check for __about__ attribute with __version__
        elif hasattr(bcrypt, "__about__") and hasattr(bcrypt.__about__, "__version__"):
            version = bcrypt.__about__.__version__
        # Check for _bcrypt attribute with __version__
        elif hasattr(bcrypt, "_bcrypt") and hasattr(bcrypt._bcrypt, "__version__"):
            version = bcrypt._bcrypt.__version__
        
        # If we couldn't get the version through attributes, try regex on __file__
        if not version and hasattr(bcrypt, "__file__"):
            # Try to extract version from the file path
            version_match = re.search(r'bcrypt-([0-9.]+)', bcrypt.__file__)
            if version_match:
                version = version_match.group(1)
        
        # If still no version, try using pkg_resources
        if not version:
            try:
                import pkg_resources
                version = pkg_resources.get_distribution("bcrypt").version
            except (ImportError, pkg_resources.DistributionNotFound):
                pass
        
        # Determine if compatible version (4.0.x)
        is_compatible = version and version.startswith("4.0.")
        
        return version, is_compatible
        
    except ImportError:
        return None, False
    except Exception as e:
        logging.error(f"Error detecting bcrypt version: {e}")
        return None, False


def uninstall_bcrypt():
    """
    Uninstall any existing bcrypt installations.
    
    Implements proper package uninstallation with error handling
    and dependency management following system specifications.
    
    Returns:
        bool: True if uninstallation successful, False otherwise
    """
    try:
        result = subprocess.run(
            [sys.executable, "-m", "pip", "uninstall", "-y", "bcrypt"],
            check=True,
            capture_output=True,
            text=True
        )
        return "Successfully uninstalled" in result.stdout
    except subprocess.CalledProcessError as e:
        logging.error(f"Error uninstalling bcrypt: {e.stderr}")
        return False
    except Exception as e:
        logging.error(f"General error during bcrypt uninstallation: {e}")
        return False


def install_compatible_bcrypt():
    """
    Install a compatible version of bcrypt (4.0.1).
    
    Implements proper package installation with error handling
    and version verification following system specifications.
    
    Returns:
        bool: True if installation successful, False otherwise
    """
    try:
        # Install specific version
        result = subprocess.run(
            [sys.executable, "-m", "pip", "install", "bcrypt==4.0.1"],
            check=True,
            capture_output=True,
            text=True
        )
        
        # Verify installation
        if "Successfully installed bcrypt-4.0.1" in result.stdout:
            return True
            
        # Double-check with check_current_version
        _, is_compatible = check_current_version()
        return is_compatible
        
    except subprocess.CalledProcessError as e:
        logging.error(f"Error installing bcrypt: {e.stderr}")
        return False
    except Exception as e:
        logging.error(f"General error during bcrypt installation: {e}")
        return False


def apply_patches():
    """
    Apply necessary patches to ensure bcrypt compatibility with passlib.
    
    Implements targeted patches for known compatibility issues
    with proper error handling and validation.
    
    Returns:
        bool: True if patches applied successfully, False otherwise
    """
    try:
        # Try to locate passlib installation
        import passlib
        passlib_dir = Path(passlib.__file__).parent
        bcrypt_handler_file = passlib_dir / "handlers" / "bcrypt.py"
        
        if not bcrypt_handler_file.exists():
            logging.warning(f"Could not find passlib bcrypt handler at {bcrypt_handler_file}")
            return False
            
        # Read the bcrypt handler file
        with open(bcrypt_handler_file, 'r') as f:
            content = f.read()
            
        # Check if patching is needed
        if "_load_backend_mixin" in content and "__about__" in content:
            # Apply the patch
            patched_content = content.replace(
                "version = _bcrypt.__about__.__version__",
                "version = getattr(_bcrypt, '__version__', '4.0.1')"
            )
            
            # Write the patched file
            with open(bcrypt_handler_file, 'w') as f:
                f.write(patched_content)
                
            logging.info("Applied patch to passlib bcrypt handler")
            return True
        else:
            logging.info("Patching not needed or incompatible handler format")
            return True  # Return True as patching wasn't needed
            
    except ImportError:
        logging.warning("Could not import passlib, skipping patches")
        return True  # Not critical if passlib

================
File: data/config/email_settings.json
================
{
  "batch_size": 50,
  "auto_respond_enabled": true,
  "confidence_threshold": 0.7,
  "processing_interval_minutes": 15,
  "max_tokens_per_analysis": 4000,
  "models": {
    "classification": "llama-3.3-70b-versatile",
    "analysis": "deepseek-reasoner",
    "response": "llama-3.3-70b-versatile"
  }
}

================
File: data/email_responses.json
================
{
  "responses": [
    {
      "email_id": "1956bfe4134227e3",
      "response_time": "2025-03-06T17:28:06.874246",
      "response_data": {
        "sender": "Ivailo Tsvetkov <i.b.cvetkov@gmail.com>",
        "subject": "Letta meettA",
        "response": "Subject: Meeting Confirmation and Information Request  \n\nDear [Recipient],  \n\nI acknowledge your meeting request for tomorrow at the previous location. To ensure proper preparation, could you kindly:  \n1. Clarify the specific meeting purpose or agenda items  \n2. Confirm the exact attendee list  \n\nPlease advise if the location remains [insert previous meeting location if known] or requires adjustment.  \n\nLooking forward to your confirmation.  \n\nBest regards,  \n[Your Name]"
      }
    }
  ]
}

================
File: data/metrics/email_stats.json
================
{
  "total_emails_processed": 0,
  "emails_by_category": {
    "meeting": 0,
    "needs_review": 0,
    "not_actionable": 0,
    "not_meeting": 0
  },
  "average_processing_time_ms": 0,
  "success_rate": 0,
  "stats_period_days": 30,
  "last_updated": "2025-03-04T23:41:53.382307"
}

================
File: data/metrics/groq_metrics.json
================
{
  "requests": [],
  "errors": [],
  "performance": {
    "avg_response_time": 0,
    "total_requests": 0,
    "success_rate": 100
  },
  "initialized_at": "2025-03-04T23:41:53.382307"
}

================
File: docs/analysis-pipeline.md
================
# Four-Stage Email Analysis Pipeline Specification

## Overview
The email analysis pipeline implements a sophisticated four-stage approach to email processing, utilizing both Llama and Deepseek R1 models. This architecture ensures accurate classification, thorough analysis, appropriate response generation, and efficient delivery of email responses while maintaining optimal handling of meeting-related communications.

## Stage 1: Initial Meeting Classification (Llama Model)

### Purpose
The first stage serves as an initial filter to identify meeting-related content within incoming emails. This stage prevents unnecessary deep analysis of non-meeting emails, optimizing system resources and processing time.

### Input Processing
The Llama model receives the complete email content and applies initial classification logic to determine if the email contains meeting-related information. The system processes emails in batches, examining only previously unprocessed emails.

### Classification Process
The model performs binary classification:
- Positive Classification: Email contains meeting-related content
- Negative Classification: Email contains no meeting-related content

### Duplicate Prevention
The system maintains a weekly rolling history of processed email IDs to prevent duplicate processing. Only unique identifiers are stored, without additional metadata or model outputs.

### Output
The stage produces a binary decision that determines whether the email proceeds to Stage 2 or exits the pipeline.

## Stage 2: Detailed Content Analysis with Response Generation (Deepseek R1 Model)

### Purpose
For emails classified as meeting-related, the Deepseek R1 model performs comprehensive content analysis and generates appropriate responses, aiming to provide immediate assistance whenever possible.

### Core Workflow
1. **Email Ingestion & Initial Processing**
   - Generates unique request ID using content hash + timestamp
   - Performs content length validation and sanitization
   - Creates structured analysis prompt with:
      - Step-by-step evaluation instructions
      - Response templates
      - Format requirements

2. **Comprehensive Content Analysis**
   - **Initial Screening**
     - Meeting request detection
     - Purpose clarity assessment
     - Tone classification (Friendly/Formal)
   - **Completeness Check** (4 Required Elements)
     - Specific time/date → Checks for concrete times/dates
     - Location → Physical/virtual meeting space verification
     - Agenda → Attempt to extract the purpose of the meeting 
     - Attendees → Participant list detection
   - **Risk Assessment**
     - Financial/legal keyword scanning
     - Multi-party coordination complexity
     - Sensitive content detection

3. **Dynamic Response Generation**
   - **Response Logic Matrix:**
     ```
     ┌───────────────────────┬──────────────────────────────┐
     │ Scenario              │ Action                       │
     ├───────────────────────┼──────────────────────────────┤
     │ Complete + Low Risk   → Instant confirmation         │
     │ Missing 1-3 Elements → Request specific missing data │
     │ High Risk Content    → 24h human review notice       │
     │ Info Only            → Polite acknowledgment        │
     └───────────────────────┴──────────────────────────────┘
     ```

   - **Tone Adaptation** based on sender's communication style

### Output
The model generates a structured output containing:
- Comprehensive analysis results
- Specific missing elements (if any)
- Pre-generated response text appropriate to the situation
- Recommended handling category
- Confidence scores for extracted information

> **Important:** The Deepseek analyzer actively attempts to avoid "needs_review" status whenever possible, ensuring senders receive appropriate responses in most scenarios.

## Stage 3: Response Categorization (Response Categorizer)

### Purpose
The third stage processes Deepseek's analysis to finalize categorization and prepare the response for delivery, serving as an integration layer between analysis and delivery.

### Input Processing
The Response Categorizer receives:
- Structured analysis from Deepseek R1
- Pre-generated response text
- Recommended handling category
- Extracted parameters and confidence scores

### Categorization Process
The Response Categorizer:
- Validates the analysis output and recommendation
- Extracts and formats the pre-generated response
- Makes final categorization decision according to the deepseek recommendation
- Prepares the response for delivery

### Classification Categories
The stage assigns one of three final statuses:

1. "standard_response"
   Requirements:
   - Complete analysis with appropriate response text
   - No critical issues requiring human intervention
   Actions:
   - Response text is finalized for delivery
   - Email is prepared for processing by the Email Agent

2. "needs_review"
   Triggers:
   - High risk content identified by Deepseek
   - Complex scenarios beyond automated handling capability
   - System processing uncertainty or low confidence
   Actions:
   - Email is flagged for human review with star in gmail
   - Appropriate notification text is prepared

3. "ignored"
   Criteria:
   - Confirmed non-meeting content
   - Informational-only content requiring polite acknowledgment 
   Actions:
   - Polite acknowledgment  is generated
   - Email is marked as processed and read

## Stage 4: Response Delivery (Email Agent)

### Purpose
The final stage handles the delivery of responses, email status management, and comprehensive record-keeping of all communications.

### Input Processing
The Email Agent receives:
- Final categorization decision
- Finalized response text
- Email metadata

### Delivery Process
The Email Agent:
- Sends appropriate responses to email senders
- Updates email status in Gmail based on category
- Maintains detailed logs of all responses
- Tracks communication history

### Special Handling
- **Standard Response:** Sends response and marks email as read
- **Needs Review:** Marks email with a star in Gmail for visibility and priority handling by human reviewers
- **Ignored:** Updates internal records with no further action and mark it as read

### Output
The stage completes the pipeline by:
- Delivering responses to senders when appropriate
- Setting correct email status and flags in Gmail
- Recording all communications in the response log
- Providing confirmation of successful processing

## Pipeline Integration

The entire four-stage pipeline is orchestrated by the EmailProcessor, which:
- Manages the flow between all stages
- Maintains processing state and history
- Ensures proper error handling throughout the pipeline
- Provides logging and monitoring of the complete process
- Controls batch processing and deduplication

================
File: docs/assistant-instructions.md
================
# AI System Documentation Assistant Prompt

## CONTEXT
You are being initialized as a specialized system documentation assistant focused on understanding and utilizing a comprehensive set of system documentation files for an enterprise application. Your primary function is to assist with code development and system understanding while maintaining strict alignment with the existing architecture, patterns, and requirements detailed in the system documentation files. This approach ensures all suggestions and solutions are contextually appropriate and consistent with the established framework. The documentation suite covers system instructions, response management, analysis pipelines, classification systems, email processing, error handling, infrastructure, and planned enhancements.

## ROLE
You are a Senior Systems Architecture Consultant with extensive experience in:
- Analyzing enterprise-scale software systems
- Understanding complex processing pipelines and classification systems
- Implementing robust error handling and response management systems
- Documenting and maintaining large-scale system architectures
- Guiding teams through system evolution and enhancement
You excel at interpreting technical documentation and translating system requirements into practical solutions while maintaining system integrity.

## ACTION
1. Upon initialization, analyze these core documentation files:
   - system-instructions.md: Core system operational guidelines
   - response-management.md: Response handling protocols
   - analysis-pipeline.md: Data processing workflow specifications
   - classification-categories.md: System classification framework
   - email-processing-rules.md: Email handling specifications
   - error-handling.md: Error management protocols
   - existing-infrastructure.md: Current system architecture
   - future-enhancements.md: Planned system improvements

2. For each query:
   - Cross-reference against all relevant documentation files
   - Prioritize maintaining consistency with existing patterns
   - Consider impact on the analysis pipeline and classification system
   - Ensure alignment with error handling protocols
   - Account for planned future enhancements
   - Reference specific documentation sections in responses

3. Maintain awareness of:
   - The complete analysis pipeline flow
   - Classification system rules and categories
   - Email processing requirements
   - Error handling expectations
   - Infrastructure constraints
   - Planned enhancement impacts

4. When providing solutions:
   - Quote relevant documentation sections
   - Explain alignment with existing patterns
   - Detail integration with the analysis pipeline
   - Consider classification system impact
   - Address error handling requirements
   - Account for future enhancement plans

## FORMAT
Structure responses as follows:

1. Documentation Context
   - Reference relevant documentation files
   - Quote specific sections
   - Explain documentation relationships

2. System Integration Analysis
   - Pipeline integration considerations
   - Classification system impacts
   - Error handling requirements
   - Infrastructure constraints

3. Solution Proposal
   - Detailed implementation approach
   - Code suggestions with extensive comments
   - Integration steps
   - Error handling considerations

4. Future Compatibility
   - Alignment with planned enhancements
   - Scalability considerations
   - Potential future impacts

## TARGET AUDIENCE
The assistant will interact with:

System Stakeholders:
- Development teams implementing system changes
- System architects overseeing architecture
- Operations teams managing the system
- Project managers tracking system evolution

Consider varying levels of:
- System familiarity
- Technical expertise
- Documentation understanding
- Implementation experience

## DOCUMENTATION SUITE
Primary documentation files for context:

Core System Files:
- system-instructions.md: Fundamental system operation guidelines and protocols
- response-management.md: Detailed protocols for handling system responses
- analysis-pipeline.md: Comprehensive analysis pipeline specifications

Classification and Processing:
- classification-categories.md: Detailed classification system framework
- email-processing-rules.md: Email handling and processing specifications

System Management:
- error-handling.md: Complete error management protocols
- existing-infrastructure.md: Current system architecture documentation
- future-enhancements.md: Planned system improvements and evolution

Note: Every interaction should reference these files to maintain system consistency and proper integration with existing components.

================
File: docs/classification-categories.md
================
# Email Classification Categories and Handling Specifications

## Overview
This document outlines the three primary classification categories used in the email management system, detailing the specific requirements, criteria, and handling procedures for each category. The classification system ensures consistent and appropriate handling of all incoming emails while maintaining efficient processing and user-friendly organization.

## Standard Response Category

### Definition and Purpose
The "standard_response" classification indicates emails that can be handled automatically through the system's response mechanism. These emails have sufficient information for the system to generate appropriate responses without human intervention.

### Qualification Requirements
For an email to qualify for standard response handling, it typically meets the following criteria:
- Contains clear meeting-related content
- Provides sufficient information for appropriate response generation
- Presents minimal complexity or risk factors
- Has clear parameters if it's a meeting request
- Contains no complex attachments requiring review

### Processing Actions
When an email is classified for standard response, the system performs these actions:
- Sends the pre-generated response from DeepseekAnalyzer
- Stars the email for future reference
- Marks the email as read after successful response
- Records the processing in the weekly history

### Response Types
The system may generate various types of standard responses:
- Confirmation for complete meeting requests
- Information requests for meetings with missing details
- Acknowledgments for informational emails
- Clarification requests for ambiguous content

## Needs Review Category

### Definition and Purpose
The "needs_review" classification indicates emails that require human attention due to complexity, sensitive content, or other factors that prevent automated handling. This category ensures human oversight for appropriate situations but it's with low priority and only when unavoidable.

### Triggering Conditions
An email is classified for review under these conditions:
- High-risk content identified during analysis
- Complex scenarios beyond automated handling capabilities
- Multi-party coordination requirements
- Financial or legal implications detected
- Complex attachments requiring human assessment

### Processing Actions
For emails requiring review, the system:
- Maintains unread status
- Applies star marking for visibility and priority
- Preserves all attachments and original formatting
- May generate a notification of pending review
- Records the classification in processing history

### Special Handling Requirements
The system implements specific handling for review cases:
- Gmail starring provides visual indication for priority attention
- Star marking ensures easy filtering and identification
- Preservation of unread status maintains visibility in inbox
- Response generation may include pending review notification

## Ignore Category

### Definition and Purpose
The "ignored" classification applies to emails that require no action, either because they contain no meeting-related content or have been determined to need no response.

### Classification Criteria
Emails are classified for ignoring when:
- Confirmed as non-meeting related content
- Meeting content is determined to be informational only
- No action or response is required
- Content falls outside the scope of meeting management

### Processing Actions
For ignored emails, the system:
- Maintains current status
- Performs no response generation
- Records the classification in processing history
- Takes no further action

### Verification Process
Before finalizing ignore classification:
- Confirms absence of meeting-related content
- Verifies no response requirement
- Ensures no critical information is overlooked
- Records classification reasoning

## Response Generation Guidelines

### Standard Response Generation
The system generates responses based on the email content and analysis results:

```
Response Logic Matrix:
┌───────────────────────┬──────────────────────────────┐
│ Scenario              │ Action                       │
├───────────────────────┼──────────────────────────────┤
│ Complete + Low Risk   → Instant confirmation         │
│ Missing 1-3 Elements → Request specific missing data │
│ High Risk Content    → 24h human review notice       │
│ Info Only            → Polite acknowledgment        │
└───────────────────────┴──────────────────────────────┘
```

### Tone Adaptation
Responses are dynamically adapted to match the sender's communication style:

| Scenario          | Friendly Response                          | Formal Response                              |
|-------------------|--------------------------------------------|----------------------------------------------|
| Needs Review      | "Hey Sam! We'll get back within 24h 😊"    | "Dear Ms. Smith: Your request is under review..." |
| Missing Info      | "Hi! Could you share the time? 🕒"         | "Please provide meeting time at your earliest..." |

### Response Priority
The system prioritizes sending appropriate responses whenever possible. The DeepseekAnalyzer actively attempts to avoid "needs_review" status, ensuring senders receive timely responses in most scenarios.

## Classification Process Flow

### Initial Stage (LlamaAnalyzer)
- Performs binary classification (meeting-related or not)
- Non-meeting emails typically proceed to ignored category
- Meeting-related emails proceed to detailed analysis

### Detailed Analysis Stage (DeepseekAnalyzer)
- Performs comprehensive content analysis
- Generates appropriate response when possible
- Identifies potential review requirements
- Provides structured output for categorization

### Final Categorization Stage (ResponseCategorizer)
- Processes structured analysis output
- Makes final categorization decisions
- Prepares responses for delivery
- Determines email handling requirements

### Delivery Stage (EmailAgent)
- Implements appropriate handling based on category
- Sends responses for standard_response emails
- Sets correct email status in Gmail
- Stars emails requiring review
- Maintains comprehensive response logs

This classification system ensures appropriate handling of all incoming emails while maintaining efficient processing and organization. Each category has specific criteria and actions that work together to provide comprehensive email management with appropriate human oversight when needed.

================
File: docs/email-processing-rules.md
================
# Email Processing Rules and Mechanisms

## Introduction
This document details the comprehensive set of rules and mechanisms governing email processing within the system. These specifications ensure consistent handling of emails across the pipeline while maintaining efficiency and reliability.

## Processing Pipeline Overview

### Four-Stage Processing Pipeline
The system implements a sophisticated four-stage processing pipeline:

1. **Initial Classification (LlamaAnalyzer)**
   - Binary classification of emails (meeting-related or not)
   - Initial filtering to optimize processing resources
   - Processing of only unique, unhandled emails

2. **Content Analysis & Response Generation (DeepseekAnalyzer)**
   - Comprehensive content analysis of meeting-related emails
   - Required elements verification (time/date, location, agenda, attendees)
   - Risk assessment for complex or sensitive content
   - Dynamic response generation based on analysis results
   - Tone adaptation to match sender's communication style

3. **Response Categorization (ResponseCategorizer)**
   - Processing of Deepseek analysis output
   - Extraction and validation of pre-generated responses
   - Final categorization decision-making
   - Preparation of responses for delivery

4. **Response Delivery (EmailAgent)**
   - Sending of appropriate responses to senders
   - Email status management in Gmail
   - Special handling for needs_review emails (starred in Gmail)
   - Comprehensive response logging

## Batch Processing Specifications

### Batch Size Management
The system processes emails in controlled batches to optimize resource utilization and maintain system stability. Each processing cycle handles up to 100 emails, ensuring efficient throughput while preventing system overload. This batch size was chosen to balance processing efficiency with system responsiveness.

### Processing Sequence
Emails are processed in chronological order within each batch. The system maintains strict processing order to ensure no emails are inadvertently skipped or processed out of sequence. Each email undergoes the complete four-stage pipeline:

1. LlamaAnalyzer determines if the email is meeting-related
2. If meeting-related, DeepseekAnalyzer performs content analysis and generates appropriate response
3. ResponseCategorizer finalizes the categorization and prepares response
4. EmailAgent handles delivery and status management

## Email History Tracking

### Weekly Rolling History
The system implements a weekly rolling history mechanism to track processed emails. This approach ensures efficient resource utilization while maintaining adequate historical context for duplicate prevention. The history tracking system stores only essential information:
- Unique email identifiers
- No additional metadata
- No model outputs or processing results

### Cleanup Process
At the end of each week, the system automatically purges outdated history entries. This ensures the history tracking system remains efficient and prevents unnecessary resource consumption while maintaining sufficient historical data for proper operation.

## Duplicate Processing Prevention

### Identifier Tracking
The system uses unique email identifiers to prevent duplicate processing. Before processing any email, the system checks these identifiers against the weekly history. This mechanism ensures that each email is processed exactly once, preventing redundant analysis and responses.

### History Verification
Before initiating the analysis pipeline for any email, the system performs a thorough check against the historical record. Emails found in the history are automatically skipped, ensuring system resources are focused on new, unprocessed content.

## Required Meeting Parameters

### Mandatory Fields
Four specific parameters are required for optimal processing:
- Date of the meeting (specific day)
- Time of the meeting (specific hour)
- Location (physical or virtual)
- Agenda (purpose of the meeting)

### Parameter Validation
Each required parameter undergoes validation to ensure completeness and accuracy:
- Date must be in a recognized format
- Time must be clearly specified
- Location must be explicitly stated
- Agenda must provide sufficient context

## Response Generation Rules

### Dynamic Response Generation
The DeepseekAnalyzer implements a sophisticated response generation system:

```
Response Logic Matrix:
┌───────────────────────┬──────────────────────────────┐
│ Scenario              │ Action                       │
├───────────────────────┼──────────────────────────────┤
│ Complete + Low Risk   → Instant confirmation         │
│ Missing 1-3 Elements → Request specific missing data │
│ High Risk Content    → 24h human review notice       │
│ Info Only            → Polite acknowledgment        │
└───────────────────────┴──────────────────────────────┘
```

### Tone Adaptation
Responses are dynamically adapted to match the sender's communication style:

| Scenario          | Friendly Response                          | Formal Response                              |
|-------------------|--------------------------------------------|----------------------------------------------|
| Needs Review      | "Hey Sam! We'll get back within 24h 😊"    | "Dear Ms. Smith: Your request is under review..." |
| Missing Info      | "Hi! Could you share the time? 🕒"         | "Please provide meeting time at your earliest..." |

### Response Priority
The system prioritizes sending appropriate responses whenever possible. The DeepseekAnalyzer actively attempts to avoid "needs_review" status, ensuring senders receive timely responses in most scenarios.

## Email Status Management

### Read/Unread Status
The system maintains precise control over email status:
- Emails receiving standard responses are marked as read after processing
- Emails requiring review remain unread
- Ignored emails maintain their current status

### Starring System
The system implements specific starring rules:
- All emails classified for review are starred for visibility and priority handling
- This provides a visual indicator for emails requiring human attention
- Starred emails can be easily filtered and identified in Gmail

## Data Integrity and Validation

### Input Validation
Every email entering the processing pipeline undergoes validation:
- Verification of required fields
- Format checking of critical data
- Structural integrity validation

### Output Verification
The system verifies all processing outputs:
- Confirmation of status changes
- Validation of response generation
- Verification of history updates

## Special Considerations

### Attachment Handling
Emails containing attachments receive special processing:
- Meeting-related emails with attachments are assessed for risk
- Complex attachments may trigger needs_review classification
- Attachment presence is logged for tracking purposes

### Multiple Request Handling
When multiple requests are detected:
- The system attempts to generate appropriate responses when possible
- Complex multiple requests may trigger needs_review classification
- Original email remains unread and starred if human review is needed

## System Monitoring and Logging

### Processing Metrics
The system maintains detailed logs at DEBUG level, with particular emphasis on:
- Batch processing statistics
- Parameter validation results
- Status change operations
- Template processing results
- Gmail status management operations

### Error Tracking
Comprehensive error logging includes:
- Parameter validation failures
- Processing exceptions
- Status update errors
- Template processing issues
- Response delivery failures

This specification ensures consistent and reliable email processing while maintaining system efficiency and accuracy. Each rule and mechanism works in concert to provide a robust email management solution with appropriate responses to senders and efficient handling of complex scenarios.

================
File: docs/error-handling.md
================
# Error Handling and Reliability Specifications

## Introduction
The error handling and reliability system ensures robust operation of the email management system through comprehensive error detection, graceful failure handling, and detailed logging mechanisms. This specification outlines the complete approach to maintaining system stability and reliability throughout all processing stages.

## Retry Mechanism

### Core Retry Strategy
The system implements a carefully designed retry mechanism for handling transient failures. When an error occurs during processing, the system follows these precise steps:
1. Immediate error detection and logging
2. Implementation of a 3-second delay before retry attempt
3. Single retry execution
4. Final status determination

### Retry Limitations
The system implements specific limitations on retry attempts to prevent resource waste and endless processing loops. These limitations include:
- No retry attempts for content parsing failures
- Single retry limit for all other failures
- Strict 3-second delay enforcement between attempts
- Automatic failure reporting after retry exhaustion

### Error Categories and Handling
The system distinguishes between different types of errors for appropriate handling:
- Processing Errors: Eligible for retry with delay
- Parsing Errors: No retry, immediate failure reporting
- System Errors: Eligible for retry with delay
- Network Errors: Eligible for retry with delay

## Logging Requirements

### Debug Level Logging
The system maintains comprehensive DEBUG level logging throughout all operations. This includes:
- Complete input capture at each processing stage
- Detailed output logging for all operations
- Full error state documentation
- System decision tracking
- Processing flow monitoring

### Input/Output Logging
Special emphasis is placed on capturing complete input and output data:
- Raw email content logging
- Model input capture
- Model output documentation
- Processing decision recording
- Status change tracking

### Error State Documentation
The system maintains detailed documentation of error states:
- Error type classification
- Error context capture
- Stack trace preservation
- System state recording at error time
- Recovery attempt documentation

## Error Reporting System

### Frontend Integration Preparation
The system prepares error information for future frontend integration:
- Structured error format development
- Error severity classification
- User-friendly error message generation
- Error context preservation
- Recovery suggestion preparation

### Error Notification Structure
Each error report contains:
- Timestamp of occurrence
- Error type classification
- Processing stage identification
- Context information
- Recovery attempt results

## System Stability Measures

### State Management
The system implements robust state management:
- Transaction-like processing steps
- State restoration capabilities
- Progress tracking mechanisms
- Recovery point establishment

### Resource Protection
To prevent resource exhaustion:
- Memory usage monitoring
- Processing time tracking
- Resource allocation limits
- Cleanup procedures implementation

## Monitoring and Metrics

### Performance Tracking
The system tracks key performance indicators:
- Processing success rates
- Error occurrence frequency
- Retry attempt statistics
- Recovery success rates

### System Health Monitoring
Continuous monitoring of:
- Resource utilization
- Processing throughput
- Error rate trends
- Recovery effectiveness

## Failure Recovery

### Recovery Procedures
The system implements specific recovery procedures for different failure types:
- Immediate recovery attempts for transient failures
- Graceful degradation for persistent issues
- Resource cleanup after failures
- State restoration when possible

### Data Protection
Critical data protection measures include:
- Transaction logging
- State preservation
- Data consistency checks
- Backup procedures

## Debug Mode Operation

### Enhanced Logging
During debug operation:
- Verbose logging of all operations
- Complete data flow tracking
- Decision point documentation
- State transition recording

### Troubleshooting Support
The system provides robust troubleshooting capabilities:
- Detailed error context capture
- Processing flow visualization
- State inspection capabilities
- Recovery attempt tracking

## Implementation Guidelines

### Error Detection Principles
The system follows these principles for error detection:
- Early error detection
- Comprehensive error classification
- Context preservation
- Recovery opportunity identification

### Recovery Strategy Implementation
Recovery strategies are implemented with:
- Clear success criteria
- Failure thresholds
- Resource protection measures
- State consistency maintenance

## Future Considerations

### Extensibility
The error handling system is designed for future expansion:
- New error type integration
- Additional recovery strategies
- Enhanced monitoring capabilities
- Expanded reporting functions

### Integration Preparation
The system prepares for future integration needs:
- Structured error formats
- Standard reporting interfaces
- Monitoring endpoints
- Management APIs

This comprehensive error handling and reliability system ensures robust operation while maintaining detailed visibility into system behavior and providing clear paths for issue resolution and system improvement.

================
File: docs/existing-infrastructure.md
================
# Existing Infrastructure Documentation

## Introduction
This document details the currently implemented and functioning infrastructure components of the email management system. These components form the foundation upon which further development will build. Understanding these existing capabilities is crucial for maintaining system integrity while implementing new features.

## Gmail Integration System

### Authentication and Authorization
The system implements a robust Gmail integration using OAuth 2.0 authentication. This provides secure access to email functionality while maintaining user privacy and security. The implementation includes:

The authentication system manages credentials through:
- Secure token storage in 'token.json'
- Automatic token refresh handling
- Proper scope management for email access
- Secure client secret handling

### Email Management Capabilities
The current Gmail integration provides comprehensive email handling features:

Message Access:
- Retrieval of unread emails
- Batch processing support
- Full message content access
- Attachment handling capabilities
- Thread information extraction

Status Management:
- Read/unread status control
- Email marking capabilities
- Thread tracking functionality
- Batch operation support

Content Processing:
- MIME type handling
- Attachment extraction
- Header parsing
- Content decoding
- Character encoding management

## Secure Storage Infrastructure

### Encryption System
The implemented secure storage system provides robust data protection through:

Encryption Features:
- Strong encryption for all stored data
- Automatic key rotation mechanism
- Secure key storage implementation
- Backup key management
- Data integrity verification

Backup Management:
- Automated backup creation
- Secure backup storage
- Retention policy enforcement
- Recovery mechanism implementation
- Integrity verification systems

### Data Management
The storage system implements comprehensive data handling:

Record Management:
- Unique identifier tracking
- Processing history maintenance
- Status tracking capabilities
- Backup state management
- Recovery point creation

Integrity Protection:
- Checksum verification
- Transaction logging
- State consistency checks
- Error recovery procedures
- Data validation systems

## Basic Processing Pipeline

### Core Architecture
The existing pipeline provides foundational processing capabilities:

Component Structure:
- Modular component design
- Clear separation of concerns
- Standardized interfaces
- Error handling integration
- Logging system implementation

Processing Flow:
- Asynchronous operation support
- Batch processing capabilities
- Status tracking mechanisms
- Error recovery procedures
- Performance monitoring

### System Integration
The pipeline implements comprehensive integration features:

Service Coordination:
- Component communication
- State management
- Resource sharing
- Error propagation
- Status synchronization

Operational Management:
- Resource allocation
- Performance monitoring
- Error tracking
- Status reporting
- Health checking

## Logging and Monitoring

### Logging Infrastructure
The system includes comprehensive logging capabilities:

Logging Features:
- Multiple logging levels
- Structured log formats
- Rotation management
- Archive handling
- Error tracking

Monitoring Capabilities:
- Performance metrics
- Error rate tracking
- Resource utilization
- Processing statistics
- Health indicators

## Error Handling Framework

### Core Error Management
The implemented error handling system provides:

Error Processing:
- Comprehensive error detection
- Structured error reporting
- Recovery mechanism support
- Status tracking
- Notification systems

Recovery Features:
- Automatic retry capabilities
- State restoration
- Resource cleanup
- Error logging
- Status recovery

## Configuration Management

### System Configuration
The existing configuration system manages:

Configuration Features:
- Environment variable handling
- Secure credential management
- Service configuration
- Component settings
- Runtime parameters

Management Capabilities:
- Configuration validation
- Update handling
- Version tracking
- Backup management
- Recovery procedures

## Development Infrastructure

### Code Organization
The current codebase maintains:

Structure Features:
- Clear module separation
- Standardized naming
- Consistent formatting
- Documentation standards
- Testing framework

Development Support:
- Type hinting
- Error handling patterns
- Logging standards
- Testing utilities
- Development tools

## Future Integration Points

### Extension Capabilities
The current infrastructure supports future development through:

Integration Features:
- Standard interfaces
- Extension points
- Plugin architecture
- Service endpoints
- API foundations

Growth Support:
- Scalability features
- Performance monitoring
- Resource management
- Error handling
- Status tracking

This infrastructure provides a solid foundation for future development while maintaining security, reliability, and performance. All components are designed with extensibility in mind, allowing for seamless integration of new features and capabilities.

================
File: docs/future-enhancements.md
================
# Future Enhancements and Development Roadmap

## Introduction

This document outlines the planned future enhancements for the email management system, arranged in order of priority. Each enhancement is described with its purpose, scope, and relationship to existing components. This roadmap serves as a guide for systematic system evolution while maintaining operational stability.

## High Priority Enhancements

### Agent Coordination System (In Development)

The agent coordination system represents a critical enhancement currently under active development. This system will enable efficient communication and task coordination between different components of the email processing pipeline.

Purpose and Scope:
- Coordinate actions between email processing components
- Manage workflow transitions between processing stages
- Ensure efficient resource utilization
- Maintain processing state consistency
- Enable scalable component interaction

Implementation Considerations:
- Integration with existing pipeline architecture
- Compatibility with current processing flows
- Performance impact management
- Error handling coordination
- State management requirements

### Monitoring Dashboard (High Priority)

The monitoring dashboard will provide comprehensive visibility into system operations and performance metrics.

Key Features:
- Real-time processing status monitoring
- Performance metric visualization
- Error rate tracking and alerting
- Resource utilization monitoring
- System health indicators

Implementation Requirements:
- Data collection from all system components
- Real-time metric processing
- Historical data analysis
- Alert system integration
- Performance impact minimization

### Agent Configuration UI (Very High Priority)

The agent configuration interface will enable dynamic system behavior adjustment without code modifications.

Core Capabilities:
- Parameter configuration management
- Processing rule adjustment
- Template customization
- Workflow modification
- Resource allocation control

Development Focus:
- User-friendly interface design
- Secure configuration handling
- Real-time configuration updates
- Configuration version control
- Recovery mechanism implementation

### Response Template System Enhancement (Next Development Phase)

The template system enhancement will provide flexible and customizable response generation capabilities.

Enhancement Scope:
- Template customization interface
- Variable parameter handling
- Format customization
- Style management
- Version control implementation

Development Requirements:
- Template management system
- Parameter validation enhancement
- Format verification system
- Style consistency checking
- Version tracking implementation

## Medium Priority Enhancements

### Auto-reminder Service (Future API Integration)

The auto-reminder service will be implemented as an independent component accessible through API endpoints.

Service Features:
- Automated reminder generation
- Scheduling management
- Priority handling
- Status tracking
- Integration capabilities

Implementation Plan:
- Separate service architecture
- API endpoint development
- Frontend integration preparation
- Scheduling system implementation
- Notification management

### Calendar Integration

The calendar integration will provide comprehensive meeting scheduling and conflict management capabilities.

Integration Scope:
- Calendar availability checking
- Conflict detection
- Schedule optimization
- Meeting management
- Recurring event handling

Development Requirements:
- Calendar API integration
- Conflict resolution system
- Schedule management interface
- Event tracking capabilities
- Synchronization management

### Frontend Customization

The frontend customization system will enable user-specific adjustments to system behavior and appearance.

Customization Areas:
- Response template modification
- Parameter field customization
- Processing rule adjustment
- Interface personalization
- Workflow customization

Implementation Considerations:
- User preference management
- Configuration persistence
- Version control implementation
- Performance impact management
- Security consideration

## Low Priority Enhancements

### Timezone Handling Improvements

Enhanced timezone management will provide more accurate and user-friendly scheduling capabilities.

Enhancement Scope:
- Automatic timezone detection
- Conversion handling
- Format standardization
- User preference management
- Schedule optimization

Implementation Requirements:
- Timezone database integration
- Conversion system implementation
- Format management
- Preference handling
- Schedule adjustment capabilities

### Performance Metrics Refinement

The metrics system refinement will provide more detailed insights into system performance and behavior.

Refinement Areas:
- Detailed performance tracking
- Resource utilization analysis
- Processing efficiency metrics
- Error pattern detection
- Trend analysis capabilities

Implementation Focus:
- Metric collection enhancement
- Analysis system improvement
- Reporting capability expansion
- Visualization enhancement
- Historical data management

## Dependencies and Relationships

### Component Integration

The enhancements must maintain compatibility with existing components:
- Email processing pipeline
- Secure storage system
- Gmail integration
- Response management system
- Error handling framework

### Development Sequence

The implementation sequence considers:
- Component dependencies
- Resource requirements
- Operational impact
- User requirements
- System stability

### Implementation Guidelines

Development must adhere to:
- Current security standards
- Performance requirements
- Reliability expectations
- Scalability needs
- Maintenance considerations

## Monitoring and Verification

### Enhancement Tracking

Progress monitoring includes:
- Development milestone tracking
- Integration testing
- Performance verification
- Security validation
- User acceptance testing

### Quality Assurance

Quality management ensures:
- Feature completeness
- Performance standards
- Security compliance
- Reliability requirements
- User satisfaction

This roadmap provides a structured approach to system enhancement while maintaining operational stability and user satisfaction. Each enhancement builds upon existing capabilities while preparing for future development needs.

================
File: docs/openai.md
================
Text generation

Copy page
Learn how to generate text from a prompt.
OpenAI provides simple APIs to use a large language model to generate text from a prompt, as you might using ChatGPT. These models have been trained on vast quantities of data to understand multimedia inputs and natural language instructions. From these prompts, models can generate almost any kind of text response, like code, mathematical equations, structured JSON data, or human-like prose.

Quickstart
To generate text, you can use the chat completions endpoint in the REST API, as seen in the examples below. You can either use the REST API from the HTTP client of your choice, or use one of OpenAI's official SDKs for your preferred programming language.

Create a human-like response to a prompt

from openai import OpenAI
client = OpenAI()

completion = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "developer", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": "Write a haiku about recursion in programming."
        }
    ]
)

print(completion.choices[0].message)

================
File: docs/project_documentation.md
================
# Project Documentation

## Overview
This project appears to be focused on building an AI-powered email processing and response system. The system includes components for classifying emails, processing email content, generating responses, and integrating with AI services.

## Architecture

### High-Level Architecture
```
                      +---------------+
                      |  Email Flow  |
                      +---------------+
                            |
                            v
                      +---------------+
                      | Email Receiver|
                      +---------------+
                            |
                            v
                      +---------------+
                      | Email Classifier|
                      +---------------+
                            |
                            v
                      +---------------+
                      | Response Generator|
                      +---------------+
                            |
                            v
                      +---------------+
                      | Email Sender   |
                      +---------------+
```

### Component Breakdown

#### 1. Email Classifier
- Location: `email_classifier.py`
- Description: Handles the classification of incoming emails based on content and context.
- Dependencies: Integrates with Groq AI services for advanced NLP capabilities.

#### 2. Email Processor
- Location: `email_processor.py`
- Description: Core processing logic for handling emails, including parsing, analysis, and response generation.
- Dependencies: 
  - `email_classifier.py` for classification
  - `secure_storage.py` for secure data handling
  - `deepseek_analyzer.py` for deep analysis of meeting emails

#### 3. LlamaAnalyzer
- Location: `llama_analyzer.py`
- Description: Performs general analysis on all emails using the llama-3.3-70b-versatile model.
- Dependencies:
  - `groq_integration.client_wrapper.GroqClientWrapper` for API calls to Groq
  - `config/analyzer_config.py` for configuration settings

#### 4. DeepseekAnalyzer
- Location: `deepseek_analyzer.py`
- Description: Performs deep analysis on meeting emails using DeepSeek's reasoner model.
- Dependencies:
  - Direct API calls to DeepSeek's API endpoint
  - `config/analyzer_config.py` for configuration settings

#### 5. Response Generation
- Location: `email_writer.py`
- Description: Generates human-like responses to emails based on classification and context.
- Dependencies: 
  - `email_classifier.py` for classification data
  - `groq_integration` for AI-generated content

#### 4. Secure Storage
- Location: `secure_storage.py`
- Description: Handles secure storage of sensitive data including encryption keys and email records.
- Dependencies: 
  - `data/secure/` directory for storage
  - `data/secure/backups/` for backup files

#### 5. Groq Integration
- Location: `groq_integration/`
- Description: Contains integration logic for interacting with Groq AI services.
- Components:
  - `__init__.py`: Main entry point
  - `client_wrapper.py`: Wrapper for Groq API client
  - `model_manager.py`: Manages AI model interactions
  - `constants.py`: Contains configuration constants

## Data Flow

### Email Processing Workflow
1. Email Receipt
   - Emails are received through the `gmail.py` integration
   - Stored temporarily in `data/secure/encrypted_records.bin`

2. Classification
   - Emails are classified using `email_classifier.py`
   - Classification data is stored in `data/metrics/groq_metrics.json`

3. Initial Analysis
   - All emails are initially analyzed by `llama_analyzer.py` using the llama-3.3-70b-versatile model
   - The LlamaAnalyzer determines if the email needs a standard response, needs review, or should be ignored

4. Deep Analysis for Meeting Emails
   - For emails classified as meetings:
     - Additional deep analysis is performed using `deepseek_analyzer.py`
     - The DeepseekAnalyzer refines the decision, determining if a standardized response is needed, if the email should be flagged for action, or if it should be ignored

5. Processing
   - Based on the analysis results, `email_processor.py` handles the emails:
     - Emails needing standard responses are processed for response generation
     - Emails flagged for action are marked for review
     - Emails to be ignored are marked as read

6. Response Generation
   - For emails requiring a standard response, responses are generated using `email_writer.py`
   - Responses are stored in `data/email_responses.json`

7. Email Handling
   - Processed emails are marked as read or unread based on their status
   - Emails requiring further action are kept unread for manual review

8. Sending
   - Generated responses are sent through `gmail.py`
   - Send history is logged in `meeting_mails.json`

## Security Considerations

### Data Security
- All sensitive data is stored in encrypted form in `data/secure/`
- Backup files are stored in `data/secure/backups/`
- Encryption keys are managed through `secure_storage.py`

### Access Control
- All operations require proper authentication
- Access to sensitive data is restricted through secure storage mechanisms

## Future Enhancements

### Planned Features
1. Enhanced AI Integration
   - Further optimization of LlamaAnalyzer for general email analysis
   - Expansion of DeepseekAnalyzer capabilities for more nuanced meeting email analysis
   - Integration of additional AI models for specialized email types
   - Improved context understanding and response generation across all analyzers

2. Additional Email Providers
   - Support for Outlook
   - Support for Exchange

3. User Interface
   - Web interface for managing email rules
   - Dashboard for monitoring email processing
   - Visualization of DeepseekAnalyzer results

4. Analytics
   - Detailed analytics of email processing
   - User behavior insights
   - Performance metrics
   - Analysis of DeepseekAnalyzer accuracy and impact

### Known Limitations
1. Current Limitations
   - Limited to Gmail integration
   - Basic NLP capabilities
   - Limited user configuration options

2. Technical Debt
   - Code organization
   - Documentation
   - Testing coverage

## Import Structure

The project uses a simplified import structure to improve readability and maintainability. The main components are organized into the following packages:

1. `email_processing`: Contains the core email processing logic
2. `integrations`: Handles external integrations (e.g., Gmail, Groq)
3. `storage`: Manages secure storage of data

Each package has an `__init__.py` file that exposes the main classes and functions, allowing for cleaner imports throughout the project. For example:

```python
from email_processing import EmailProcessor, EmailClassifier, LlamaAnalyzer
from integrations import GmailClient, EnhancedGroqClient
from storage import SecureStorage
```

This structure helps to avoid circular imports and makes the codebase more modular and easier to maintain.

## Conclusion

This document provides a high-level overview of the current state of the project. The system is designed to handle email processing and response generation using AI services, with particular emphasis on security, data privacy, and maintainable code structure.

================
File: docs/response-management.md
================
# Response Management System Specification

## Introduction
The response management system handles all aspects of email response generation and delivery within the email management system. It ensures consistent, appropriate, and accurate responses to meeting-related emails while maintaining professional communication standards and proper parameter handling.

## Response Generation System

### Dynamic Response Generation
The system employs a sophisticated response generation approach implemented in the DeepseekAnalyzer:

```
Response Logic Matrix:
┌───────────────────────┬──────────────────────────────┐
│ Scenario              │ Action                       │
├───────────────────────┼──────────────────────────────┤
│ Complete + Low Risk   → Instant confirmation         │
│ Missing 1-3 Elements → Request specific missing data │
│ High Risk Content    → 24h human review notice       │
│ Info Only            → Polite acknowledgment        │
└───────────────────────┴──────────────────────────────┘
```

### Tone Adaptation
Responses are dynamically adapted to match the sender's communication style:

| Scenario          | Friendly Response                          | Formal Response                              |
|-------------------|--------------------------------------------|----------------------------------------------|
| Needs Review      | "Hey Sam! We'll get back within 24h 😊"    | "Dear Ms. Smith: Your request is under review..." |
| Missing Info      | "Hi! Could you share the time? 🕒"         | "Please provide meeting time at your earliest..." |

### Response Templates
The system maintains various response templates for different scenarios:

1. **Meeting Confirmation Template:**
   "Thank you for your meeting request. I am pleased to confirm our meeting on {params['date']['value']} at {params['time']['value']} at {params['location']['value']}"

2. **Information Request Template:**
   "Thank you for your meeting request. To help me properly schedule our meeting, could you please provide {missing_info}?"

3. **Review Notification Template:**
   "Thank you for your meeting request. Your request requires additional review, and we will respond within 24 hours."

4. **Acknowledgment Template:**
   "Thank you for the information about the meeting. I have noted the details."

## Parameter Processing Workflow

### Parameter Validation
Before generating any response, the system validates all required parameters through a systematic process:

Initial Check:
- Presence verification for all required fields
- Format validation for each parameter
- Content validity assessment
- Completeness verification

Validation Rules:
- Date must be clearly specified and valid
- Time must be explicitly stated and unambiguous
- Location must be definitively provided
- Agenda must be sufficiently detailed

### Missing Parameter Handling
When parameters are incomplete, the system follows a structured workflow:

Detection Phase:
- Identifies specific missing parameters
- Determines which parameters need clarification
- Assesses parameter completeness

Information Request:
- Generates specific requests for missing information
- Maintains context of previous communications
- Tracks outstanding parameter requests

### Parameter Storage
The system maintains parameter integrity through:
- Secure storage of validated parameters
- Context preservation between interactions
- Version tracking of parameter updates
- Validation state maintenance

## Response Generation Process

### Analysis-Based Generation
The system generates responses based on comprehensive analysis:

1. **Content Analysis**
   - Meeting request detection
   - Parameter extraction and validation
   - Tone identification
   - Completeness assessment
   - Risk evaluation

2. **Response Selection**
   - Template selection based on analysis results
   - Parameter incorporation
   - Tone adjustment
   - Personalization elements

3. **Final Formatting**
   - Proper greeting based on sender name and tone
   - Response body with appropriate information
   - Consistent closing
   - Professional signature

### Response Priority
The system prioritizes sending appropriate responses whenever possible. The DeepseekAnalyzer actively attempts to avoid "needs_review" status, ensuring senders receive timely responses in most scenarios.

## Response Delivery Workflow

### Delivery Process
The EmailAgent handles the delivery of all responses:

Preparation Stage:
- Response validation
- Sender information verification
- Subject line formatting
- Content finalization

Delivery Stage:
- Email transmission through Gmail API
- Status update in Gmail
- Delivery confirmation
- Response logging

### Email Status Management
The system implements comprehensive status management:

Status Updates:
- Standard Response: Marked as read after response
- Needs Review: Maintained as unread and starred
- Ignored: Status unchanged, no action taken

Starring System:
- Special visual indication for needs_review emails
- Priority handling facilitation
- Easy filtering in Gmail interface

### Record Keeping
The system maintains comprehensive records of:
- Response generation attempts
- Parameter validation states
- Status change history
- Delivery confirmations
- Response content

## Special Cases Management

### Attachment Handling
For emails containing attachments:
- Content analysis includes attachment assessment
- Complex attachments may trigger needs_review categorization
- Attachment information is preserved for human review
- Response content acknowledges attachments when appropriate

### Complex Request Processing
When multiple or complex requests are detected:
- The system attempts to generate appropriate responses when possible
- Highly complex requests may trigger needs_review categorization
- Response content acknowledges complexity when appropriate
- Preservation of context for human review

## Integration and Flow

### Pipeline Integration
The response management system integrates with the four-stage pipeline:

1. **LlamaAnalyzer Stage**
   - Initial classification of meeting-related content
   - Binary determination of processing need

2. **DeepseekAnalyzer Stage**
   - Comprehensive content analysis
   - Dynamic response generation
   - Parameter extraction and validation
   - Response template selection
   - Trying to answer for scenarios out of template scopes with appropriate answer

3. **ResponseCategorizer Stage**
   - Processing of analysis output
   - Response preparation and finalization
   - Category determination
   - Delivery preparation

4. **EmailAgent Stage**
   - Response delivery
   - Status management in Gmail
   - Special handling for needs_review (starring)
   - Record keeping and logging

### Information Flow
The system maintains proper information flow throughout the process:

1. Email content → Analysis → Response generation → Delivery
2. Parameters → Validation → Template incorporation → Final response
3. Analysis results → Categorization → Status management → Record keeping

## Error Handling and Recovery

### Response Failures
The system implements specific handling for response generation failures:
- Validation failure recovery
- Parameter error handling
- Template processing recovery
- Delivery failure management

### Error Reporting
Comprehensive error reporting includes:
- Detailed error logging
- Failure point identification
- Recovery attempt tracking
- Status update monitoring

## System Monitoring

### Performance Tracking
The system monitors response management performance through:
- Response generation success rates
- Parameter validation statistics
- Delivery success monitoring
- Error rate tracking

### Quality Assurance
Continuous quality monitoring includes:
- Response accuracy verification
- Parameter validation checking
- Format consistency monitoring
- Delivery success confirmation

This specification ensures consistent and reliable response management while maintaining system efficiency and accuracy. Each component works together to provide professional and appropriate email responses while preparing for future enhancements and customization capabilities.

================
File: docs/structured_outputs.md
================
Structured Outputs
==================

Ensure responses adhere to a JSON schema.

Try it out
----------

Try it out in the [Playground](/playground) or generate a ready-to-use schema definition to experiment with structured outputs.

Generate

Introduction
------------

JSON is one of the most widely used formats in the world for applications to exchange data.

Structured Outputs is a feature that ensures the model will always generate responses that adhere to your supplied [JSON Schema](https://json-schema.org/overview/what-is-jsonschema), so you don't need to worry about the model omitting a required key, or hallucinating an invalid enum value.

Some benefits of Structured Outputs include:

1.  **Reliable type-safety:** No need to validate or retry incorrectly formatted responses
2.  **Explicit refusals:** Safety-based model refusals are now programmatically detectable
3.  **Simpler prompting:** No need for strongly worded prompts to achieve consistent formatting

In addition to supporting JSON Schema in the REST API, the OpenAI SDKs for [Python](https://github.com/openai/openai-python/blob/main/helpers.md#structured-outputs-parsing-helpers) and [JavaScript](https://github.com/openai/openai-node/blob/master/helpers.md#structured-outputs-parsing-helpers) also make it easy to define object schemas using [Pydantic](https://docs.pydantic.dev/latest/) and [Zod](https://zod.dev/) respectively. Below, you can see how to extract information from unstructured text that conforms to a schema defined in code.

Getting a structured response

```javascript
import OpenAI from "openai";
import { zodResponseFormat } from "openai/helpers/zod";
import { z } from "zod";

const openai = new OpenAI();

const CalendarEvent = z.object({
  name: z.string(),
  date: z.string(),
  participants: z.array(z.string()),
});

const completion = await openai.beta.chat.completions.parse({
  model: "gpt-4o-2024-08-06",
  messages: [
    { role: "system", content: "Extract the event information." },
    { role: "user", content: "Alice and Bob are going to a science fair on Friday." },
  ],
  response_format: zodResponseFormat(CalendarEvent, "event"),
});

const event = completion.choices[0].message.parsed;
```

```python
from pydantic import BaseModel
from openai import OpenAI

client = OpenAI()

class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

completion = client.beta.chat.completions.parse(
    model="gpt-4o-2024-08-06",
    messages=[
        {"role": "system", "content": "Extract the event information."},
        {"role": "user", "content": "Alice and Bob are going to a science fair on Friday."},
    ],
    response_format=CalendarEvent,
)

event = completion.choices[0].message.parsed
```

### Supported models

Structured Outputs are available in our [latest large language models](/docs/models), starting with GPT-4o:

*   `o1-2024-12-17` and later
*   `gpt-4o-mini-2024-07-18` and later
*   `gpt-4o-2024-08-06` and later

Older models like `gpt-4-turbo` and earlier may use [JSON mode](#json-mode) instead.

When to use Structured Outputs via function calling vs via response\_format

-------------------------------------------------------------------------------

Structured Outputs is available in two forms in the OpenAI API:

1.  When using [function calling](/docs/guides/function-calling)
2.  When using a `json_schema` response format

Function calling is useful when you are building an application that bridges the models and functionality of your application.

For example, you can give the model access to functions that query a database in order to build an AI assistant that can help users with their orders, or functions that can interact with the UI.

Conversely, Structured Outputs via `response_format` are more suitable when you want to indicate a structured schema for use when the model responds to the user, rather than when the model calls a tool.

For example, if you are building a math tutoring application, you might want the assistant to respond to your user using a specific JSON Schema so that you can generate a UI that displays different parts of the model's output in distinct ways.

Put simply:

*   If you are connecting the model to tools, functions, data, etc. in your system, then you should use function calling
*   If you want to structure the model's output when it responds to the user, then you should use a structured `response_format`

The remainder of this guide will focus on non-function calling use cases in the Chat Completions API. To learn more about how to use Structured Outputs with function calling, check out the [Function Calling](/docs/guides/function-calling#function-calling-with-structured-outputs) guide.

### Structured Outputs vs JSON mode

Structured Outputs is the evolution of [JSON mode](#json-mode). While both ensure valid JSON is produced, only Structured Outputs ensure schema adherance. Both Structured Outputs and JSON mode are supported in the Chat Completions API, Assistants API, Fine-tuning API and Batch API.

We recommend always using Structured Outputs instead of JSON mode when possible.

However, Structured Outputs with `response_format: {type: "json_schema", ...}` is only supported with the `gpt-4o-mini`, `gpt-4o-mini-2024-07-18`, and `gpt-4o-2024-08-06` model snapshots and later.

||Structured Outputs|JSON Mode|
|---|---|---|
|Outputs valid JSON|Yes|Yes|
|Adheres to schema|Yes (see supported schemas)|No|
|Compatible models|gpt-4o-mini, gpt-4o-2024-08-06, and later|gpt-3.5-turbo, gpt-4-* and gpt-4o-* models|
|Enabling|response_format: { type: "json_schema", json_schema: {"strict": true, "schema": ...} }|response_format: { type: "json_object" }|

Examples
--------

Chain of thoughtStructured data extractionUI generationModeration

### Chain of thought

You can ask the model to output an answer in a structured, step-by-step way, to guide the user through the solution.

Structured Outputs for chain-of-thought math tutoring

```javascript
import OpenAI from "openai";
import { z } from "zod";
import { zodResponseFormat } from "openai/helpers/zod";

const openai = new OpenAI();

const Step = z.object({
  explanation: z.string(),
  output: z.string(),
});

const MathReasoning = z.object({
  steps: z.array(Step),
  final_answer: z.string(),
});

const completion = await openai.beta.chat.completions.parse({
  model: "gpt-4o-2024-08-06",
  messages: [
    { role: "system", content: "You are a helpful math tutor. Guide the user through the solution step by step." },
    { role: "user", content: "how can I solve 8x + 7 = -23" },
  ],
  response_format: zodResponseFormat(MathReasoning, "math_reasoning"),
});

const math_reasoning = completion.choices[0].message.parsed;
```

```python
from pydantic import BaseModel
from openai import OpenAI

client = OpenAI()

class Step(BaseModel):
    explanation: str
    output: str

class MathReasoning(BaseModel):
    steps: list[Step]
    final_answer: str

completion = client.beta.chat.completions.parse(
    model="gpt-4o-2024-08-06",
    messages=[
        {"role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step."},
        {"role": "user", "content": "how can I solve 8x + 7 = -23"}
    ],
    response_format=MathReasoning,
)

math_reasoning = completion.choices[0].message.parsed
```

```bash
curl https://api.openai.com/v1/chat/completions \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-2024-08-06",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful math tutor. Guide the user through the solution step by step."
      },
      {
        "role": "user",
        "content": "how can I solve 8x + 7 = -23"
      }
    ],
    "response_format": {
      "type": "json_schema",
      "json_schema": {
        "name": "math_reasoning",
        "schema": {
          "type": "object",
          "properties": {
            "steps": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "explanation": { "type": "string" },
                  "output": { "type": "string" }
                },
                "required": ["explanation", "output"],
                "additionalProperties": false
              }
            },
            "final_answer": { "type": "string" }
          },
          "required": ["steps", "final_answer"],
          "additionalProperties": false
        },
        "strict": true
      }
    }
  }'
```

#### Example response

```json
{
  "steps": [
    {
      "explanation": "Start with the equation 8x + 7 = -23.",
      "output": "8x + 7 = -23"
    },
    {
      "explanation": "Subtract 7 from both sides to isolate the term with the variable.",
      "output": "8x = -23 - 7"
    },
    {
      "explanation": "Simplify the right side of the equation.",
      "output": "8x = -30"
    },
    {
      "explanation": "Divide both sides by 8 to solve for x.",
      "output": "x = -30 / 8"
    },
    {
      "explanation": "Simplify the fraction.",
      "output": "x = -15 / 4"
    }
  ],
  "final_answer": "x = -15 / 4"
}
```

How to use Structured Outputs with response\_format

-------------------------------------------------------

You can use Structured Outputs with the new SDK helper to parse the model's output into your desired format, or you can specify the JSON schema directly.

**Note:** the first request you make with any schema will have additional latency as our API processes the schema, but subsequent requests with the same schema will not have additional latency.

SDK objectsManual schema

Step 1: Define your object

First you must define an object or data structure to represent the JSON Schema that the model should be constrained to follow. See the [examples](/docs/guides/structured-outputs#examples) at the top of this guide for reference.

While Structured Outputs supports much of JSON Schema, some features are unavailable either for performance or technical reasons. See [here](/docs/guides/structured-outputs#supported-schemas) for more details.

For example, you can define an object like this:

```python
from pydantic import BaseModel

class Step(BaseModel):
  explanation: str
  output: str

class MathResponse(BaseModel):
  steps: list[Step]
  final_answer: str
```

```javascript
import { z } from "zod";
import { zodResponseFormat } from "openai/helpers/zod";

const Step = z.object({
explanation: z.string(),
output: z.string(),
});

const MathResponse = z.object({
steps: z.array(Step),
final_answer: z.string(),
});
```

#### Tips for your data structure

To maximize the quality of model generations, we recommend the following:

*   Name keys clearly and intuitively
*   Create clear titles and descriptions for important keys in your structure
*   Create and use evals to determine the structure that works best for your use case

Step 2: Supply your object in the API call

You can use the `parse` method to automatically parse the JSON response into the object you defined.

Under the hood, the SDK takes care of supplying the JSON schema corresponding to your data structure, and then parsing the response as an object.

```python
completion = client.beta.chat.completions.parse(
  model="gpt-4o-2024-08-06",
  messages=[
      {"role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step."},
      {"role": "user", "content": "how can I solve 8x + 7 = -23"}
  ],
  response_format=MathResponse
)
```

```javascript
const completion = await openai.beta.chat.completions.parse({
model: "gpt-4o-2024-08-06",
messages: [
  { role: "system", content: "You are a helpful math tutor. Guide the user through the solution step by step." },
  { role: "user", content: "how can I solve 8x + 7 = -23" },
],
response_format: zodResponseFormat(MathResponse, "math_response"),
});
```

Step 3: Handle edge cases

In some cases, the model might not generate a valid response that matches the provided JSON schema.

This can happen in the case of a refusal, if the model refuses to answer for safety reasons, or if for example you reach a max tokens limit and the response is incomplete.

```javascript
try {
  const completion = await openai.chat.completions.create({
    model: "gpt-4o-2024-08-06",
    messages: [{
        role: "system",
        content: "You are a helpful math tutor. Guide the user through the solution step by step.",
      },
      {
        role: "user",
        content: "how can I solve 8x + 7 = -23"
      },
    ],
    response_format: {
      type: "json_schema",
      json_schema: {
        name: "math_response",
        schema: {
          type: "object",
          properties: {
            steps: {
              type: "array",
              items: {
                type: "object",
                properties: {
                  explanation: {
                    type: "string"
                  },
                  output: {
                    type: "string"
                  },
                },
                required: ["explanation", "output"],
                additionalProperties: false,
              },
            },
            final_answer: {
              type: "string"
            },
          },
          required: ["steps", "final_answer"],
          additionalProperties: false,
        },
        strict: true,
      },
    },
    max_tokens: 50,
  });

  if (completion.choices[0].finish_reason === "length") {
    // Handle the case where the model did not return a complete response
    throw new Error("Incomplete response");
  }

  const math_response = completion.choices[0].message;

  if (math_response.refusal) {
    // handle refusal
    console.log(math_response.refusal);
  } else if (math_response.content) {
    console.log(math_response.content);
  } else {
    throw new Error("No response content");
  }
} catch (e) {
  // Handle edge cases
  console.error(e);
}
```

```python
try:
    response = client.chat.completions.create(
        model="gpt-4o-2024-08-06",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful math tutor. Guide the user through the solution step by step.",
            },
            {"role": "user", "content": "how can I solve 8x + 7 = -23"},
        ],
        response_format={
            "type": "json_schema",
            "json_schema": {
                "name": "math_response",
                "strict": True,
                "schema": {
                    "type": "object",
                    "properties": {
                        "steps": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "explanation": {"type": "string"},
                                    "output": {"type": "string"},
                                },
                                "required": ["explanation", "output"],
                                "additionalProperties": False,
                            },
                        },
                        "final_answer": {"type": "string"},
                    },
                    "required": ["steps", "final_answer"],
                    "additionalProperties": False,
                },
            },
        },
        strict=True,
    )
except Exception as e:
    # handle errors like finish_reason, refusal, content_filter, etc.
    pass
```

### 

Refusals with Structured Outputs

When using Structured Outputs with user-generated input, OpenAI models may occasionally refuse to fulfill the request for safety reasons. Since a refusal does not necessarily follow the schema you have supplied in `response_format`, the API response will include a new field called `refusal` to indicate that the model refused to fulfill the request.

When the `refusal` property appears in your output object, you might present the refusal in your UI, or include conditional logic in code that consumes the response to handle the case of a refused request.

```python
class Step(BaseModel):
  explanation: str
  output: str

class MathReasoning(BaseModel):
  steps: list[Step]
  final_answer: str

completion = client.beta.chat.completions.parse(
  model="gpt-4o-2024-08-06",
  messages=[
      {"role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step."},
      {"role": "user", "content": "how can I solve 8x + 7 = -23"}
  ],
  response_format=MathReasoning,
)

math_reasoning = completion.choices[0].message

# If the model refuses to respond, you will get a refusal message
if (math_reasoning.refusal):
  print(math_reasoning.refusal)
else:
  print(math_reasoning.parsed)
```

```javascript
const Step = z.object({
explanation: z.string(),
output: z.string(),
});

const MathReasoning = z.object({
steps: z.array(Step),
final_answer: z.string(),
});

const completion = await openai.beta.chat.completions.parse({
model: "gpt-4o-2024-08-06",
messages: [
  { role: "system", content: "You are a helpful math tutor. Guide the user through the solution step by step." },
  { role: "user", content: "how can I solve 8x + 7 = -23" },
],
response_format: zodResponseFormat(MathReasoning, "math_reasoning"),
});

const math_reasoning = completion.choices[0].message

// If the model refuses to respond, you will get a refusal message
if (math_reasoning.refusal) {
console.log(math_reasoning.refusal);
} else {
console.log(math_reasoning.parsed);
}
```

The API response from a refusal will look something like this:

```json
{
"id": "chatcmpl-9nYAG9LPNonX8DAyrkwYfemr3C8HC",
"object": "chat.completion",
"created": 1721596428,
"model": "gpt-4o-2024-08-06",
"choices": [
  {
  "index": 0,
  "message": {
          "role": "assistant",
          "refusal": "I'm sorry, I cannot assist with that request."
  },
  "logprobs": null,
  "finish_reason": "stop"
}
],
"usage": {
    "prompt_tokens": 81,
    "completion_tokens": 11,
    "total_tokens": 92,
    "completion_tokens_details": {
      "reasoning_tokens": 0,
      "accepted_prediction_tokens": 0,
      "rejected_prediction_tokens": 0
    }
},
"system_fingerprint": "fp_3407719c7f"
}
```

### 

Tips and best practices

#### Handling user-generated input

If your application is using user-generated input, make sure your prompt includes instructions on how to handle situations where the input cannot result in a valid response.

The model will always try to adhere to the provided schema, which can result in hallucinations if the input is completely unrelated to the schema.

You could include language in your prompt to specify that you want to return empty parameters, or a specific sentence, if the model detects that the input is incompatible with the task.

#### Handling mistakes

Structured Outputs can still contain mistakes. If you see mistakes, try adjusting your instructions, providing examples in the system instructions, or splitting tasks into simpler subtasks. Refer to the [prompt engineering guide](/docs/guides/prompt-engineering) for more guidance on how to tweak your inputs.

#### Avoid JSON schema divergence

To prevent your JSON Schema and corresponding types in your programming language from diverging, we strongly recommend using the native Pydantic/zod sdk support.

If you prefer to specify the JSON schema directly, you could add CI rules that flag when either the JSON schema or underlying data objects are edited, or add a CI step that auto-generates the JSON Schema from type definitions (or vice-versa).

Streaming
---------

You can use streaming to process model responses or function call arguments as they are being generated, and parse them as structured data.

That way, you don't have to wait for the entire response to complete before handling it. This is particularly useful if you would like to display JSON fields one by one, or handle function call arguments as soon as they are available.

We recommend relying on the SDKs to handle streaming with Structured Outputs. You can find an example of how to stream function call arguments without the SDK `stream` helper in the [function calling guide](/docs/guides/function-calling#advanced-usage).

Here is how you can stream a model response with the `stream` helper:

```python
from typing import List
from pydantic import BaseModel
from openai import OpenAI

class EntitiesModel(BaseModel):
  attributes: List[str]
  colors: List[str]
  animals: List[str]

client = OpenAI()

with client.beta.chat.completions.stream(
  model="gpt-4o",
  messages=[
      {"role": "system", "content": "Extract entities from the input text"},
      {
          "role": "user",
          "content": "The quick brown fox jumps over the lazy dog with piercing blue eyes",
      },
  ],
  response_format=EntitiesModel,
) as stream:
  for event in stream:
      if event.type == "content.delta":
          if event.parsed is not None:
              # Print the parsed data as JSON
              print("content.delta parsed:", event.parsed)
      elif event.type == "content.done":
          print("content.done")
      elif event.type == "error":
          print("Error in stream:", event.error)

final_completion = stream.get_final_completion()
print("Final completion:", final_completion)
```

```js
import OpenAI from "openai";
import { zodResponseFormat } from "openai/helpers/zod";
import { z } from "zod";
export const openai = new OpenAI();

const EntitiesSchema = z.object({
attributes: z.array(z.string()),
colors: z.array(z.string()),
animals: z.array(z.string()),
});

const stream = openai.beta.chat.completions
.stream({
  model: "gpt-4o",
  messages: [
    { role: "system", content: "Extract entities from the input text" },
    {
      role: "user",
      content:
        "The quick brown fox jumps over the lazy dog with piercing blue eyes",
    },
  ],
  response_format: zodResponseFormat(EntitiesSchema, "entities"),
})
.on("refusal.done", () => console.log("request refused"))
.on("content.delta", ({ snapshot, parsed }) => {
  console.log("content:", snapshot);
  console.log("parsed:", parsed);
  console.log();
})
.on("content.done", (props) => {
  console.log(props);
});

await stream.done();

const finalCompletion = await stream.finalChatCompletion();

console.log(finalCompletion);
```

You can also use the `stream` helper to parse function call arguments:

```python
from pydantic import BaseModel
import openai
from openai import OpenAI

class GetWeather(BaseModel):
  city: str
  country: str

client = OpenAI()

with client.beta.chat.completions.stream(
  model="gpt-4o",
  messages=[
      {
          "role": "user",
          "content": "What's the weather like in SF and London?",
      },
  ],
  tools=[
      openai.pydantic_function_tool(GetWeather, name="get_weather"),
  ],
  parallel_tool_calls=True,
) as stream:
  for event in stream:
      if event.type == "tool_calls.function.arguments.delta" or event.type == "tool_calls.function.arguments.done":
          print(event)

print(stream.get_final_completion())
```

```js
import { zodFunction } from "openai/helpers/zod";
import OpenAI from "openai/index";
import { z } from "zod";

const GetWeatherArgs = z.object({
city: z.string(),
country: z.string(),
});

const client = new OpenAI();

const stream = client.beta.chat.completions
.stream({
  model: "gpt-4o",
  messages: [
    {
      role: "user",
      content: "What's the weather like in SF and London?",
    },
  ],
  tools: [zodFunction({ name: "get_weather", parameters: GetWeatherArgs })],
})
.on("tool_calls.function.arguments.delta", (props) =>
  console.log("tool_calls.function.arguments.delta", props)
)
.on("tool_calls.function.arguments.done", (props) =>
  console.log("tool_calls.function.arguments.done", props)
)
.on("refusal.delta", ({ delta }) => {
  process.stdout.write(delta);
})
.on("refusal.done", () => console.log("request refused"));

const completion = await stream.finalChatCompletion();

console.log("final completion:", completion);
```

Supported schemas
-----------------

Structured Outputs supports a subset of the [JSON Schema](https://json-schema.org/docs) language.

#### Supported types

The following types are supported for Structured Outputs:

*   String
*   Number
*   Boolean
*   Integer
*   Object
*   Array
*   Enum
*   anyOf

#### Root objects must not be `anyOf`

Note that the root level object of a schema must be an object, and not use `anyOf`. A pattern that appears in Zod (as one example) is using a discriminated union, which produces an `anyOf` at the top level. So code such as the following won't work:

```javascript
import { z } from 'zod';
import { zodResponseFormat } from 'openai/helpers/zod';

const BaseResponseSchema = z.object({ /* ... */ });
const UnsuccessfulResponseSchema = z.object({ /* ... */ });

const finalSchema = z.discriminatedUnion('status', [
  BaseResponseSchema,
  UnsuccessfulResponseSchema,
]);

// Invalid JSON Schema for Structured Outputs
const json = zodResponseFormat(finalSchema, 'final_schema');
```

#### All fields must be `required`

To use Structured Outputs, all fields or function parameters must be specified as `required`.

```json
{
  "name": "get_weather",
  "description": "Fetches the weather in the given location",
  "strict": true,
  "parameters": {
      "type": "object",
      "properties": {
          "location": {
              "type": "string",
              "description": "The location to get the weather for"
          },
          "unit": {
              "type": "string",
              "description": "The unit to return the temperature in",
              "enum": ["F", "C"]
          }
      },
      "additionalProperties": false,
      "required": ["location", "unit"]
  }
}
```

Although all fields must be required (and the model will return a value for each parameter), it is possible to emulate an optional parameter by using a union type with `null`.

```json
{
  "name": "get_weather",
  "description": "Fetches the weather in the given location",
  "strict": true,
  "parameters": {
      "type": "object",
      "properties": {
          "location": {
              "type": "string",
              "description": "The location to get the weather for"
          },
          "unit": {
              "type": ["string", "null"],
              "description": "The unit to return the temperature in",
              "enum": ["F", "C"]
          }
      },
      "additionalProperties": false,
      "required": [
          "location", "unit"
      ]
  }
}
```

#### Objects have limitations on nesting depth and size

A schema may have up to 100 object properties total, with up to 5 levels of nesting.

#### Limitations on total string size

In a schema, total string length of all property names, definition names, enum values, and const values cannot exceed 15,000 characters.

#### Limitations on enum size

A schema may have up to 500 enum values across all enum properties.

For a single enum property with string values, the total string length of all enum values cannot exceed 7,500 characters when there are more than 250 enum values.

#### `additionalProperties: false` must always be set in objects

`additionalProperties` controls whether it is allowable for an object to contain additional keys / values that were not defined in the JSON Schema.

Structured Outputs only supports generating specified keys / values, so we require developers to set `additionalProperties: false` to opt into Structured Outputs.

```json
{
  "name": "get_weather",
  "description": "Fetches the weather in the given location",
  "strict": true,
  "schema": {
      "type": "object",
      "properties": {
          "location": {
              "type": "string",
              "description": "The location to get the weather for"
          },
          "unit": {
              "type": "string",
              "description": "The unit to return the temperature in",
              "enum": ["F", "C"]
          }
      },
      "additionalProperties": false,
      "required": [
          "location", "unit"
      ]
  }
}
```

#### Key ordering

When using Structured Outputs, outputs will be produced in the same order as the ordering of keys in the schema.

#### Some type-specific keywords are not yet supported

Notable keywords not supported include:

*   **For strings:** `minLength`, `maxLength`, `pattern`, `format`
*   **For numbers:** `minimum`, `maximum`, `multipleOf`
*   **For objects:** `patternProperties`, `unevaluatedProperties`, `propertyNames`, `minProperties`, `maxProperties`
*   **For arrays:** `unevaluatedItems`, `contains`, `minContains`, `maxContains`, `minItems`, `maxItems`, `uniqueItems`

If you turn on Structured Outputs by supplying `strict: true` and call the API with an unsupported JSON Schema, you will receive an error.

#### For `anyOf`, the nested schemas must each be a valid JSON Schema per this subset

Here's an example supported anyOf schema:

```json
{
  "type": "object",
  "properties": {
      "item": {
          "anyOf": [
              {
                  "type": "object",
                  "description": "The user object to insert into the database",
                  "properties": {
                      "name": {
                          "type": "string",
                          "description": "The name of the user"
                      },
                      "age": {
                          "type": "number",
                          "description": "The age of the user"
                      }
                  },
                  "additionalProperties": false,
                  "required": [
                      "name",
                      "age"
                  ]
              },
              {
                  "type": "object",
                  "description": "The address object to insert into the database",
                  "properties": {
                      "number": {
                          "type": "string",
                          "description": "The number of the address. Eg. for 123 main st, this would be 123"
                      },
                      "street": {
                          "type": "string",
                          "description": "The street name. Eg. for 123 main st, this would be main st"
                      },
                      "city": {
                          "type": "string",
                          "description": "The city of the address"
                      }
                  },
                  "additionalProperties": false,
                  "required": [
                      "number",
                      "street",
                      "city"
                  ]
              }
          ]
      }
  },
  "additionalProperties": false,
  "required": [
      "item"
  ]
}
```

#### Definitions are supported

You can use definitions to define subschemas which are referenced throughout your schema. The following is a simple example.

```json
{
  "type": "object",
  "properties": {
      "steps": {
          "type": "array",
          "items": {
              "$ref": "#/$defs/step"
          }
      },
      "final_answer": {
          "type": "string"
      }
  },
  "$defs": {
      "step": {
          "type": "object",
          "properties": {
              "explanation": {
                  "type": "string"
              },
              "output": {
                  "type": "string"
              }
          },
          "required": [
              "explanation",
              "output"
          ],
          "additionalProperties": false
      }
  },
  "required": [
      "steps",
      "final_answer"
  ],
  "additionalProperties": false
}
```

#### Recursive schemas are supported

Sample recursive schema using `#` to indicate root recursion.

```json
{
  "name": "ui",
  "description": "Dynamically generated UI",
  "strict": true,
  "schema": {
      "type": "object",
      "properties": {
          "type": {
              "type": "string",
              "description": "The type of the UI component",
              "enum": ["div", "button", "header", "section", "field", "form"]
          },
          "label": {
              "type": "string",
              "description": "The label of the UI component, used for buttons or form fields"
          },
          "children": {
              "type": "array",
              "description": "Nested UI components",
              "items": {
                  "$ref": "#"
              }
          },
          "attributes": {
              "type": "array",
              "description": "Arbitrary attributes for the UI component, suitable for any element",
              "items": {
                  "type": "object",
                  "properties": {
                      "name": {
                          "type": "string",
                          "description": "The name of the attribute, for example onClick or className"
                      },
                      "value": {
                          "type": "string",
                          "description": "The value of the attribute"
                      }
                  },
                  "additionalProperties": false,
                  "required": ["name", "value"]
              }
          }
      },
      "required": ["type", "label", "children", "attributes"],
      "additionalProperties": false
  }
}
```

Sample recursive schema using explicit recursion:

```json
{
  "type": "object",
  "properties": {
      "linked_list": {
          "$ref": "#/$defs/linked_list_node"
      }
  },
  "$defs": {
      "linked_list_node": {
          "type": "object",
          "properties": {
              "value": {
                  "type": "number"
              },
              "next": {
                  "anyOf": [
                      {
                          "$ref": "#/$defs/linked_list_node"
                      },
                      {
                          "type": "null"
                      }
                  ]
              }
          },
          "additionalProperties": false,
          "required": [
              "next",
              "value"
          ]
      }
  },
  "additionalProperties": false,
  "required": [
      "linked_list"
  ]
}
```

JSON mode
---------

JSON mode is a more basic version of the Structured Outputs feature. While JSON mode ensures that model output is valid JSON, Structured Outputs reliably matches the model's output to the schema you specify. We recommend you use Structured Outputs if it is supported for your use case.

When JSON mode is turned on, the model's output is ensured to be valid JSON, except for in some edge cases that you should detect and handle appropriately.

To turn on JSON mode with the Chat Completions or Assistants API you can set the `response_format` to `{ "type": "json_object" }`. If you are using function calling, JSON mode is always turned on.

Important notes:

*   When using JSON mode, you must always instruct the model to produce JSON via some message in the conversation, for example via your system message. If you don't include an explicit instruction to generate JSON, the model may generate an unending stream of whitespace and the request may run continually until it reaches the token limit. To help ensure you don't forget, the API will throw an error if the string "JSON" does not appear somewhere in the context.
*   JSON mode will not guarantee the output matches any specific schema, only that it is valid and parses without errors. You should use Structured Outputs to ensure it matches your schema, or if that is not possible, you should use a validation library and potentially retries to ensure that the output matches your desired schema.
*   Your application must detect and handle the edge cases that can result in the model output not being a complete JSON object (see below)

Handling edge cases

```javascript
const we_did_not_specify_stop_tokens = true;

try {
  const response = await openai.chat.completions.create({
    model: "gpt-3.5-turbo-0125",
    messages: [
      {
        role: "system",
        content: "You are a helpful assistant designed to output JSON.",
      },
      { role: "user", content: "Who won the world series in 2020? Please respond in the format {winner: ...}" },
    ],
    response_format: { type: "json_object" },
  });

  // Check if the conversation was too long for the context window, resulting in incomplete JSON 
  if (response.choices[0].message.finish_reason === "length") {
    // your code should handle this error case
  }

  // Check if the OpenAI safety system refused the request and generated a refusal instead
  if (response.choices[0].message[0].refusal) {
    // your code should handle this error case
    // In this case, the .content field will contain the explanation (if any) that the model generated for why it is refusing
    console.log(response.choices[0].message[0].refusal)
  }

  // Check if the model's output included restricted content, so the generation of JSON was halted and may be partial
  if (response.choices[0].message.finish_reason === "content_filter") {
    // your code should handle this error case
  }

  if (response.choices[0].message.finish_reason === "stop") {
    // In this case the model has either successfully finished generating the JSON object according to your schema, or the model generated one of the tokens you provided as a "stop token"

    if (we_did_not_specify_stop_tokens) {
      // If you didn't specify any stop tokens, then the generation is complete and the content key will contain the serialized JSON object
      // This will parse successfully and should now contain  {"winner": "Los Angeles Dodgers"}
      console.log(JSON.parse(response.choices[0].message.content))
    } else {
      // Check if the response.choices[0].message.content ends with one of your stop tokens and handle appropriately
    }
  }
} catch (e) {
  // Your code should handle errors here, for example a network error calling the API
  console.error(e)
}
```

```python
we_did_not_specify_stop_tokens = True

try:
    response = client.chat.completions.create(
        model="gpt-3.5-turbo-0125",
        messages=[
            {"role": "system", "content": "You are a helpful assistant designed to output JSON."},
            {"role": "user", "content": "Who won the world series in 2020? Please respond in the format {winner: ...}"}
        ],
        response_format={"type": "json_object"}
    )

    # Check if the conversation was too long for the context window, resulting in incomplete JSON 
    if response.choices[0].message.finish_reason == "length":
        # your code should handle this error case
        pass

    # Check if the OpenAI safety system refused the request and generated a refusal instead
    if response.choices[0].message[0].get("refusal"):
        # your code should handle this error case
        # In this case, the .content field will contain the explanation (if any) that the model generated for why it is refusing
        print(response.choices[0].message[0]["refusal"])

    # Check if the model's output included restricted content, so the generation of JSON was halted and may be partial
    if response.choices[0].message.finish_reason == "content_filter":
        # your code should handle this error case
        pass

    if response.choices[0].message.finish_reason == "stop":
        # In this case the model has either successfully finished generating the JSON object according to your schema, or the model generated one of the tokens you provided as a "stop token"

        if we_did_not_specify_stop_tokens:
            # If you didn't specify any stop tokens, then the generation is complete and the content key will contain the serialized JSON object
            # This will parse successfully and should now contain  "{"winner": "Los Angeles Dodgers"}"
            print(response.choices[0].message.content)
        else:
            # Check if the response.choices[0].message.content ends with one of your stop tokens and handle appropriately
            pass
except Exception as e:
    # Your code should handle errors here, for example a network error calling the API
    print(e)
```

Resources
---------

To learn more about Structured Outputs, we recommend browsing the following resources:

*   Check out our [introductory cookbook](https://cookbook.openai.com/examples/structured_outputs_intro) on Structured Outputs
*   Learn [how to build multi-agent systems](https://cookbook.openai.com/examples/structured_outputs_multi_agent) with Structured Outputs

Was this page useful?

================
File: docs/system-instructions.md
================
# Email Management System Development Instructions

## System Overview and Purpose
This automated email management system focuses on meeting coordination through Gmail integration. The system leverages a sophisticated AI-powered architecture using Groq and Deepseek, with specialized components designed for efficient email processing and response handling. The foundation includes secure storage encryption and Gmail integration with OAuth2 authentication.

## Core Architecture Implementation Flow
The system processes emails through a sophisticated four-stage pipeline, utilizing specialized models and components for comprehensive email understanding, response generation, and delivery.

### Stage 1: Initial Meeting Classification (LlamaAnalyzer)
The first stage employs a Llama model to perform preliminary email classification with these key functions:
- Analysis of incoming email content to determine meeting-related information
- Binary classification (meeting-related or not)
- Processing of only new, unhandled emails using unique identifiers
- Maintenance of a weekly rolling history of processed email IDs for deduplication

### Stage 2: Detailed Content Analysis and Response Generation (DeepseekAnalyzer)
When an email is classified as meeting-related, the Deepseek R1 model performs comprehensive content analysis and generates appropriate responses:

**Core Workflow:**
1. **Email Ingestion & Initial Processing**
   - Generates unique request ID using content hash + timestamp
   - Performs content length validation and sanitization
   - Creates structured analysis prompt with comprehensive instructions

2. **Comprehensive Content Analysis**
   - Initial screening for meeting content and tone
   - Completeness check for required elements (time/date, location, agenda, attendees)
   - Risk assessment for sensitive content and complexity

3. **Dynamic Response Generation**
   - Generates appropriate responses based on analysis results
   - Adapts tone to match sender's communication style
   - Provides specific responses for different scenarios:
     - Complete + Low Risk → Instant confirmation
     - Missing Elements → Request for specific missing data
     - High Risk Content → Human review notification
     - Info Only → Polite acknowledgment

> The DeepseekAnalyzer actively attempts to avoid "needs_review" status whenever possible, ensuring senders receive appropriate responses in most scenarios.

### Stage 3: Response Categorization (ResponseCategorizer)
The Response Categorizer processes Deepseek's analysis to finalize the handling category and prepare responses:
- Processes structured output from DeepseekAnalyzer
- Extracts and validates pre-generated response text
- Makes final categorization decisions (standard_response, needs_review, ignored)
- Prepares response for delivery

### Stage 4: Response Delivery (EmailAgent)
The Email Agent handles the final delivery and status management:
- Sends responses for standard_response emails
- Updates email status in Gmail based on categorization
- Marks needs_review emails with a star for visibility
- Maintains comprehensive response logs
- Records all communications for future reference

## Email Processing Rules and Requirements

### Required Meeting Details
Standard response processing requires:
- Date (with validated format)
- Time (with clear specification)
- Location (physical or virtual meeting space)
- Agenda (purpose of the meeting)

### Standard Response Template
Template structure with parameter insertion:
"Thank you for your meeting request. I am pleased to confirm our meeting on {params['date']['value']} at {params['time']['value']} at {params['location']['value']}"

### Batch Processing Specifications
- Batch size: 100 emails per processing cycle
- Processing of only new, unhandled emails
- Weekly rolling history maintenance
- Deduplication checks before processing

### Error Handling and Retry Logic
- Single retry attempt with 3-second delay
- No retries for content parsing failures
- Comprehensive error reporting
- DEBUG level logging for all operations
- Input/output logging for troubleshooting

## System Evolution and Future Features

### High Priority Development
1. Agent coordination system implementation
2. Monitoring dashboard development
3. Agent configuration interface
4. Enhanced response template system

### Planned Feature Expansion
1. Auto-reminder System:
   - Independent service architecture
   - API endpoint accessibility
   - Frontend integration capability

2. Advanced Features:
   - Multi-agent architecture
   - Calendar integration with conflict detection
   - Frontend customization options
   - Template customization interface
   - Variable field insertion system
   - Processing rules configuration

### Lower Priority Enhancements
1. Timezone handling improvements
2. Advanced PII detection and handling
3. Performance metrics expansion

## Development Guidelines
Development priorities should focus on:
1. Maintaining the integrity of the four-stage pipeline
2. Ensuring proper integration between all components
3. Implementing robust error handling throughout the pipeline
4. Supporting the dynamic response generation capabilities
5. Enhancing Gmail status management for better visibility

The system should be developed with consideration for:
- Future distributed system architecture
- API endpoint development
- Frontend communication requirements
- Scalability and maintenance
- Security and data protection

This architecture ensures a robust, scalable system that provides timely responses to meeting-related emails while maintaining reliable processing capabilities and appropriate human oversight when needed.

================
File: email_responses.json
================
{
  "responses": [
    {
      "email_id": "2025-02-12_15:00_i.b.cvetkov@gmail.com",
      "response_time": "2025-02-15T11:38:19.633471",
      "response_data": {
        "sender": {
          "email": "i.b.cvetkov@gmail.com",
          "name": "Ivailo Tsvetkov"
        },
        "subject": "Meeting 3",
        "date": "2025-02-12",
        "time": "15:00",
        "response": "Dear Ivailo Tsvetkov,\n\nThank you for your meeting request. To help me properly schedule our meeting, could you please specify the meeting agenda/purpose?\n\nBest regards,\nIvaylo's AI Assistant"
      }
    },
    {
      "email_id": "2025-02-11_17:00_i.b.cvetkov@gmail.com",
      "response_time": "2025-02-15T11:38:19.973417",
      "response_data": {
        "sender": {
          "email": "i.b.cvetkov@gmail.com",
          "name": "Ivailo Tsvetkov"
        },
        "subject": "Meeting Request",
        "date": "2025-02-11",
        "time": "17:00",
        "response": "Dear Ivailo Tsvetkov,\n\nThank you for your meeting request. To help me properly schedule our meeting, could you please specify the meeting agenda/purpose?\n\nBest regards,\nIvaylo's AI Assistant"
      }
    },
    {
      "email_id": "2025-02-14_15:00_i.b.cvetkov@gmail.com",
      "response_time": "2025-02-15T11:38:20.523632",
      "response_data": {
        "sender": {
          "email": "i.b.cvetkov@gmail.com",
          "name": "Ivailo Tsvetkov"
        },
        "subject": "Meeting Request",
        "date": "2025-02-14",
        "time": "15:00",
        "response": "Dear Ivailo Tsvetkov,\n\nThank you for your meeting request. To help me properly schedule our meeting, could you please specify the meeting agenda/purpose?\n\nBest regards,\nIvaylo's AI Assistant"
      }
    },
    {
      "email_id": "2025-02-18_14:00_i.b.cvetkov@gmail.com",
      "response_time": "2025-02-15T11:38:20.956914",
      "response_data": {
        "sender": {
          "email": "i.b.cvetkov@gmail.com",
          "name": "Ivailo Tsvetkov"
        },
        "subject": "future sprint",
        "date": "2025-02-18",
        "time": "14:00",
        "response": "Dear Ivailo Tsvetkov,\n\nThank you for your meeting request. To help me properly schedule our meeting, could you please specify the meeting agenda/purpose?\n\nBest regards,\nIvaylo's AI Assistant"
      }
    },
    {
      "email_id": "2025-02-16_None_i.b.cvetkov@gmail.com",
      "response_time": "2025-02-15T11:38:21.354876",
      "response_data": {
        "sender": {
          "email": "i.b.cvetkov@gmail.com",
          "name": "Ivailo Tsvetkov"
        },
        "subject": "finish the deal",
        "date": "2025-02-16",
        "time": null,
        "response": "Dear Ivailo Tsvetkov,\n\nThank you for your meeting request. To help me properly schedule our meeting, could you please specify the meeting time and the meeting agenda/purpose?\n\nBest regards,\nIvaylo's AI Assistant"
      }
    }
  ]
}

================
File: emails.txt
================
Number of unread emails: 0

================
File: groq_metrics.json
================
{
  "requests": [
    {
      "timestamp": "2025-02-15T22:13:29.890156",
      "duration": 0.963997,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:13:30.779459",
      "duration": 0.889303,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:13:31.274959",
      "duration": 0.256859,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:13:31.960640",
      "duration": 0.68468,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:13:32.456074",
      "duration": 0.27485,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:13:32.710259",
      "duration": 0.253185,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:13:33.177037",
      "duration": 0.245997,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:13:33.860998",
      "duration": 0.682962,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:13:34.411363",
      "duration": 0.318047,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:13:35.108221",
      "duration": 0.695858,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:13:35.610291",
      "duration": 0.270302,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:13:36.076541",
      "duration": 0.465253,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:13:36.555406",
      "duration": 0.256315,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:13:36.811995",
      "duration": 0.256589,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:20:41.056390",
      "duration": 0.929541,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:20:41.937474",
      "duration": 0.878083,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:20:42.821146",
      "duration": 0.649003,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:20:43.072965",
      "duration": 0.249819,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:20:44.002461",
      "duration": 0.696478,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:20:44.244083",
      "duration": 0.239623,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:20:44.750626",
      "duration": 0.276817,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:20:44.994847",
      "duration": 0.242217,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:20:45.895780",
      "duration": 0.675687,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:20:46.576244",
      "duration": 0.677469,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:20:47.265388",
      "duration": 0.465075,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:20:47.517984",
      "duration": 0.250671,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:20:47.993394",
      "duration": 0.249438,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:20:48.245914",
      "duration": 0.25052,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:24:12.707967",
      "duration": 0.558082,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:24:12.934407",
      "duration": 0.224455,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:24:14.217069",
      "duration": 0.374795,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:24:14.444373",
      "duration": 0.224332,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:24:15.057905",
      "duration": 0.226442,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:24:15.281839",
      "duration": 0.220939,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:24:15.730194",
      "duration": 0.224217,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:24:15.956524",
      "duration": 0.224327,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:24:16.683233",
      "duration": 0.221488,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:24:19.478775",
      "duration": 2.793542,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:24:20.123972",
      "duration": 0.145331,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:24:20.247129",
      "duration": 0.120158,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:24:20.603429",
      "duration": 0.121149,
      "status": "success"
    },
    {
      "timestamp": "2025-02-15T22:24:21.157401",
      "duration": 0.551972,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:08:14.516028",
      "duration": 0.961139,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:08:14.974502",
      "duration": 0.454474,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:08:16.093907",
      "duration": 0.532769,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:08:16.607866",
      "duration": 0.49896,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:08:17.581632",
      "duration": 0.501111,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:08:20.519155",
      "duration": 2.924412,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:08:22.147493",
      "duration": 1.620333,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:08:23.672729",
      "duration": 1.525236,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:11:16.828017",
      "duration": 0.924624,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:11:17.208831",
      "duration": 0.374833,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:11:17.902300",
      "duration": 0.441,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:11:18.336831",
      "duration": 0.42753,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:11:18.987517",
      "duration": 0.425688,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:11:19.435362",
      "duration": 0.443844,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:13:29.515097",
      "duration": 0.383189,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:13:29.876097",
      "duration": 0.354999,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:13:30.302570",
      "duration": 0.187,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:13:30.498571",
      "duration": 0.191002,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:13:31.175602",
      "duration": 0.40503,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:13:31.382732",
      "duration": 0.200129,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:13:31.875210",
      "duration": 0.268217,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:13:32.083208",
      "duration": 0.199999,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:13:32.595568",
      "duration": 0.224637,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:13:32.863499",
      "duration": 0.25993,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:28:40.160033",
      "duration": 0.595965,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:28:40.389003",
      "duration": 0.22375,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:28:40.770196",
      "duration": 0.138668,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:28:40.916857",
      "duration": 0.141662,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:28:41.296105",
      "duration": 0.144235,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:28:41.441848",
      "duration": 0.140235,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:28:41.920469",
      "duration": 0.139216,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:28:42.064868",
      "duration": 0.139331,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:34:07.833470",
      "duration": 0.572604,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:34:07.841810",
      "duration": 0.57207,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:34:08.277682",
      "duration": 0.14851,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:34:08.311254",
      "duration": 0.190842,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:34:08.689177",
      "duration": 0.137521,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:34:08.741279",
      "duration": 0.189623,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:34:09.158650",
      "duration": 0.185739,
      "status": "success"
    },
    {
      "timestamp": "2025-02-16T22:34:09.194948",
      "duration": 0.222037,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:36:10.852203",
      "duration": 0.536985,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:36:10.873791",
      "duration": 0.551574,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:36:21.980416",
      "duration": 0.251657,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:36:22.700780",
      "duration": 0.966504,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:37:49.293245",
      "duration": 0.251239,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:37:49.527869",
      "duration": 0.491865,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:37:49.685869",
      "duration": 0.156005,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:37:49.735508",
      "duration": 0.205644,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:37:50.334352",
      "duration": 0.142306,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:37:50.634265",
      "duration": 0.444221,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:37:50.854608",
      "duration": 0.218343,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:37:50.864073",
      "duration": 0.227808,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:37:51.573324",
      "duration": 0.236489,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:37:51.575321",
      "duration": 0.236488,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:37:52.120903",
      "duration": 0.193832,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:37:52.360858",
      "duration": 0.435964,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:37:52.839801",
      "duration": 0.130838,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:37:52.968964",
      "duration": 0.258001,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:37:53.798129",
      "duration": 0.126322,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:37:53.925812",
      "duration": 0.252003,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:40:32.069918",
      "duration": 0.34525,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:40:32.224693",
      "duration": 0.506024,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:40:32.411824",
      "duration": 0.184827,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:40:32.419084",
      "duration": 0.192087,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:40:33.130510",
      "duration": 0.23262,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:40:33.152538",
      "duration": 0.251647,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:40:33.315720",
      "duration": 0.161148,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:40:33.370963",
      "duration": 0.216391,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:40:33.933435",
      "duration": 0.174704,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:40:33.998438",
      "duration": 0.241709,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:40:34.578739",
      "duration": 0.162002,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:40:34.696563",
      "duration": 0.277567,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:40:35.274683",
      "duration": 0.200872,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:40:35.278950",
      "duration": 0.203141,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:40:36.185260",
      "duration": 0.153432,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T14:40:36.267260",
      "duration": 0.23743,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:01:08.614758",
      "duration": 0.409424,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:01:08.653802",
      "duration": 0.442469,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:01:08.824309",
      "duration": 0.167784,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:01:08.842310",
      "duration": 0.185785,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:01:09.460492",
      "duration": 0.197988,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:01:09.505349",
      "duration": 0.244841,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:01:10.149780",
      "duration": 0.279693,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:01:10.557448",
      "duration": 0.685362,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:04:04.089440",
      "duration": 0.278852,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:04:04.165344",
      "duration": 0.359756,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:04:05.958778",
      "duration": 0.145239,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:04:05.994550",
      "duration": 0.177008,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:04:06.407322",
      "duration": 0.114757,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:04:06.578261",
      "duration": 0.282693,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:04:06.946190",
      "duration": 0.118387,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:04:07.016482",
      "duration": 0.184678,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:04:07.443603",
      "duration": 0.197768,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T15:04:07.448706",
      "duration": 0.197873,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:02:20.725965",
      "duration": 1.00443,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:02:21.904486",
      "duration": 2.178907,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:02:25.803390",
      "duration": 2.12234,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:02:25.872560",
      "duration": 2.18551,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:02:26.312611",
      "duration": 0.184868,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:02:26.412156",
      "duration": 0.287412,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:02:26.778275",
      "duration": 0.128747,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:02:26.907982",
      "duration": 0.262456,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:02:27.261584",
      "duration": 0.120045,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:02:27.433433",
      "duration": 0.294893,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:02:27.844730",
      "duration": 0.18489,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:02:27.930154",
      "duration": 0.267314,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:04:29.742405",
      "duration": 0.568671,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:04:29.773375",
      "duration": 0.595642,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:04:32.491832",
      "duration": 0.525427,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:04:32.526940",
      "duration": 0.553535,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:04:33.003481",
      "duration": 0.231299,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:04:33.014285",
      "duration": 0.239102,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:04:33.630577",
      "duration": 0.376215,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:04:33.644577",
      "duration": 0.386215,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:04:34.661623",
      "duration": 0.736037,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:04:35.177157",
      "duration": 1.248571,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:04:36.616153",
      "duration": 1.199552,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T17:04:36.627117",
      "duration": 1.207516,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:06:17.506235",
      "duration": 0.85351,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:06:17.669236",
      "duration": 1.008515,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:06:18.458265",
      "duration": 0.359321,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:06:18.781321",
      "duration": 0.691355,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:06:19.439331",
      "duration": 0.390086,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:06:19.810469",
      "duration": 0.78193,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:06:20.250552",
      "duration": 0.206177,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:06:20.280812",
      "duration": 0.236437,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:06:21.018616",
      "duration": 0.26298,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:06:21.152224",
      "duration": 0.401897,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:06:21.909431",
      "duration": 0.507488,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:06:22.031616",
      "duration": 0.624785,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:06:22.574218",
      "duration": 0.294999,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:06:22.578217",
      "duration": 0.305996,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:08:49.210499",
      "duration": 0.381471,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:08:49.319793",
      "duration": 0.479074,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:08:49.896443",
      "duration": 0.27189,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:08:50.026929",
      "duration": 0.386376,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:08:50.588685",
      "duration": 0.20579,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:08:50.642675",
      "duration": 0.242006,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:08:51.007455",
      "duration": 0.134753,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:08:51.102453",
      "duration": 0.223751,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:08:51.793585",
      "duration": 0.214243,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:08:51.936940",
      "duration": 0.37191,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:08:52.319778",
      "duration": 0.135773,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:08:52.384716",
      "duration": 0.188751,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:08:52.813585",
      "duration": 0.184562,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:08:52.852757",
      "duration": 0.2354,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:46:52.706251",
      "duration": 1.016151,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:46:52.714266",
      "duration": 1.016159,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:46:53.216893",
      "duration": 0.248963,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:46:53.550092",
      "duration": 0.588422,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:46:54.163595",
      "duration": 0.371843,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:46:54.561187",
      "duration": 0.752747,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:46:55.098582",
      "duration": 0.304553,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:46:55.480094",
      "duration": 0.679021,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:46:56.614376",
      "duration": 0.668103,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:46:56.621377",
      "duration": 0.681103,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:46:57.164338",
      "duration": 0.308056,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:46:57.171345",
      "duration": 0.309059,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:46:57.730415",
      "duration": 0.307375,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:46:57.739447",
      "duration": 0.309411,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:47:20.487417",
      "duration": 0.962726,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:47:20.513422",
      "duration": 0.981728,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:47:21.087416",
      "duration": 0.335998,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:47:21.597047",
      "duration": 0.832629,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:47:22.277015",
      "duration": 0.438845,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:47:22.282022",
      "duration": 0.427854,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:47:22.993561",
      "duration": 0.388546,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:47:23.062561",
      "duration": 0.467547,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:47:24.115937",
      "duration": 0.392808,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:47:24.202937",
      "duration": 0.474808,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:47:24.761600",
      "duration": 0.320663,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:47:24.805598",
      "duration": 0.358659,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:47:25.194286",
      "duration": 0.151679,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T22:47:25.274289",
      "duration": 0.238692,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:03:16.009723",
      "duration": 0.32293,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:03:16.030578",
      "duration": 0.350787,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:03:16.906831",
      "duration": 0.632615,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:03:17.257843",
      "duration": 0.970249,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:03:18.067648",
      "duration": 0.368013,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:03:18.099648",
      "duration": 0.42635,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:03:18.786526",
      "duration": 0.35788,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:03:18.791533",
      "duration": 0.356886,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:03:19.725118",
      "duration": 0.395565,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:03:19.730120",
      "duration": 0.406568,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:03:20.497778",
      "duration": 0.382904,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:03:20.497778",
      "duration": 0.365655,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:03:21.293878",
      "duration": 0.396821,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:03:21.557735",
      "duration": 0.665813,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:05:27.905479",
      "duration": 0.511637,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:05:27.911482",
      "duration": 0.508783,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:05:28.401019",
      "duration": 0.189028,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:05:28.430988",
      "duration": 0.214,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:05:29.077206",
      "duration": 0.200587,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:05:29.582115",
      "duration": 0.698494,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:05:30.005009",
      "duration": 0.194914,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:05:30.134628",
      "duration": 0.319535,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:05:30.665391",
      "duration": 0.201999,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:05:30.870483",
      "duration": 0.40209,
      "status": "success"
    },
    {
      "timestamp": "2025-02-17T23:11:15.421025",
      "duration": 0.902756,
      "status": "success"
    },
    {
      "timestamp": "2025-02-18T00:00:51.741546",
      "duration": 0.595196,
      "status": "success"
    },
    {
      "timestamp": "2025-02-18T00:00:52.052025",
      "duration": 0.897683,
      "status": "success"
    },
    {
      "timestamp": "2025-02-18T09:32:28.108106",
      "duration": 0.229498,
      "status": "success"
    },
    {
      "timestamp": "2025-02-18T09:32:28.197118",
      "duration": 0.314509,
      "status": "success"
    },
    {
      "timestamp": "2025-02-18T09:33:53.036144",
      "duration": 0.653729,
      "status": "success"
    },
    {
      "timestamp": "2025-02-18T09:41:20.790519",
      "duration": 0.720462,
      "status": "success"
    },
    {
      "timestamp": "2025-02-18T09:56:01.845490",
      "duration": 0.800613,
      "status": "success"
    },
    {
      "timestamp": "2025-02-18T15:19:27.743046",
      "duration": 0.722516,
      "status": "success"
    },
    {
      "timestamp": "2025-02-18T21:06:05.160867",
      "duration": 1.806382,
      "status": "success"
    },
    {
      "timestamp": "2025-02-18T21:12:26.000171",
      "duration": 1.000466,
      "status": "success"
    },
    {
      "timestamp": "2025-02-18T21:16:48.021975",
      "duration": 1.029156,
      "status": "success"
    },
    {
      "timestamp": "2025-02-18T21:29:05.719971",
      "duration": 0.695174,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T09:10:52.383709",
      "duration": 1.712699,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T09:12:38.481579",
      "duration": 1.31876,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T09:13:41.107922",
      "duration": 1.466672,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T09:14:43.626254",
      "duration": 1.383378,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T09:15:47.937553",
      "duration": 1.592878,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T09:16:51.098005",
      "duration": 1.723643,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T09:46:19.144980",
      "duration": 1.809132,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T09:46:21.170340",
      "duration": 1.314624,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T09:46:23.375116",
      "duration": 1.446819,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T09:46:25.782196",
      "duration": 1.627222,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T09:46:27.369634",
      "duration": 0.90026,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T09:46:29.321516",
      "duration": 1.234912,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T11:44:05.498054",
      "duration": 2.115519,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T11:44:08.153382",
      "duration": 1.150708,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T11:44:10.691000",
      "duration": 1.597671,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T11:44:13.352891",
      "duration": 1.703563,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T11:44:15.330395",
      "duration": 1.136246,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T11:44:17.849110",
      "duration": 1.673954,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T11:49:45.609213",
      "duration": 1.945559,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T11:51:09.694029",
      "duration": 1.757152,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T11:52:12.446188",
      "duration": 1.489324,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T11:53:15.484321",
      "duration": 1.563932,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T11:54:18.095920",
      "duration": 1.266387,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T11:55:20.535601",
      "duration": 1.536757,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T12:07:05.015755",
      "duration": 1.509669,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T12:07:06.564327",
      "duration": 1.152192,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T12:07:08.061007",
      "duration": 1.107194,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T12:07:09.103025",
      "duration": 0.546981,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T12:07:10.231005",
      "duration": 0.609904,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T12:18:22.522828",
      "duration": 1.5269,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T12:18:24.308009",
      "duration": 0.829813,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T12:18:26.212124",
      "duration": 1.243102,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T12:18:28.431342",
      "duration": 1.38834,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T12:18:30.117231",
      "duration": 0.991191,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T12:46:13.184103",
      "duration": 1.642144,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T12:47:02.792757",
      "duration": 1.702583,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T12:48:05.646152",
      "duration": 1.61081,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T12:49:07.862296",
      "duration": 1.114489,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T12:50:10.802632",
      "duration": 1.309608,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T13:03:56.764023",
      "duration": 1.87632,
      "status": "success"
    },
    {
      "timestamp": "2025-02-19T15:41:04.779868",
      "duration": 1.245004,
      "status": "success"
    },
    {
      "timestamp": "2025-02-20T09:09:06.106684",
      "duration": 0.303084,
      "status": "success"
    },
    {
      "timestamp": "2025-02-20T09:09:06.118264",
      "duration": 0.310662,
      "status": "success"
    },
    {
      "timestamp": "2025-02-20T09:09:06.534372",
      "duration": 0.140994,
      "status": "success"
    },
    {
      "timestamp": "2025-02-20T09:09:06.543062",
      "duration": 0.146688,
      "status": "success"
    },
    {
      "timestamp": "2025-02-20T09:09:06.907514",
      "duration": 0.13721,
      "status": "success"
    },
    {
      "timestamp": "2025-02-20T09:09:06.913440",
      "duration": 0.140141,
      "status": "success"
    },
    {
      "timestamp": "2025-02-20T16:54:06.266163",
      "duration": 0.243007,
      "status": "success"
    },
    {
      "timestamp": "2025-02-20T16:54:06.295586",
      "duration": 0.27743,
      "status": "success"
    },
    {
      "timestamp": "2025-02-20T16:54:06.727432",
      "duration": 0.148702,
      "status": "success"
    },
    {
      "timestamp": "2025-02-20T16:54:07.497331",
      "duration": 0.914556,
      "status": "success"
    },
    {
      "timestamp": "2025-02-20T16:54:07.862725",
      "duration": 0.134279,
      "status": "success"
    },
    {
      "timestamp": "2025-02-20T16:54:07.867730",
      "duration": 0.134284,
      "status": "success"
    },
    {
      "timestamp": "2025-02-22T10:06:25.610124",
      "duration": 2.898148,
      "status": "success"
    },
    {
      "timestamp": "2025-02-22T10:06:25.618105",
      "duration": 2.90013,
      "status": "success"
    },
    {
      "timestamp": "2025-02-22T10:06:26.101193",
      "duration": 0.144622,
      "status": "success"
    },
    {
      "timestamp": "2025-02-22T10:06:26.117962",
      "duration": 0.153391,
      "status": "success"
    },
    {
      "timestamp": "2025-02-22T10:06:26.698733",
      "duration": 0.284285,
      "status": "success"
    },
    {
      "timestamp": "2025-02-22T10:06:26.799665",
      "duration": 0.393335,
      "status": "success"
    },
    {
      "timestamp": "2025-02-22T10:06:27.246677",
      "duration": 0.159,
      "status": "success"
    },
    {
      "timestamp": "2025-02-22T10:06:27.255678",
      "duration": 0.173999,
      "status": "success"
    },
    {
      "timestamp": "2025-02-22T10:33:25.227248",
      "duration": 0.883134,
      "status": "success"
    },
    {
      "timestamp": "2025-02-22T10:33:27.131853",
      "duration": 1.203641,
      "status": "success"
    },
    {
      "timestamp": "2025-02-22T10:33:28.898490",
      "duration": 0.862312,
      "status": "success"
    },
    {
      "timestamp": "2025-02-22T11:31:53.337803",
      "duration": 0.825802,
      "status": "success"
    },
    {
      "timestamp": "2025-02-22T11:31:55.031952",
      "duration": 0.926732,
      "status": "success"
    },
    {
      "timestamp": "2025-02-22T11:31:57.182963",
      "duration": 1.271205,
      "status": "success"
    },
    {
      "timestamp": "2025-02-23T11:19:28.706994",
      "duration": 1.074147,
      "status": "success"
    },
    {
      "timestamp": "2025-02-23T11:19:29.611640",
      "duration": 0.892961,
      "status": "success"
    },
    {
      "timestamp": "2025-02-23T11:27:16.129409",
      "duration": 0.954849,
      "status": "success"
    },
    {
      "timestamp": "2025-02-23T11:35:56.058551",
      "duration": 1.024682,
      "status": "success"
    },
    {
      "timestamp": "2025-02-23T11:37:22.606006",
      "duration": 0.963453,
      "status": "success"
    },
    {
      "timestamp": "2025-02-23T11:37:29.488202",
      "duration": 0.899659,
      "status": "success"
    },
    {
      "timestamp": "2025-02-23T11:39:25.513348",
      "duration": 1.009576,
      "status": "success"
    },
    {
      "timestamp": "2025-02-23T14:16:23.691770",
      "duration": 1.077277,
      "status": "success"
    },
    {
      "timestamp": "2025-02-23T14:18:10.666863",
      "duration": 0.992159,
      "status": "success"
    },
    {
      "timestamp": "2025-02-23T14:18:17.740164",
      "duration": 0.937219,
      "status": "success"
    },
    {
      "timestamp": "2025-02-23T14:20:26.555428",
      "duration": 0.620677,
      "status": "success"
    },
    {
      "timestamp": "2025-02-23T14:45:23.323450",
      "duration": 1.02057,
      "status": "success"
    },
    {
      "timestamp": "2025-02-23T14:46:42.264338",
      "duration": 0.602418,
      "status": "success"
    },
    {
      "timestamp": "2025-03-06T15:03:54.248148",
      "duration": 1.4443,
      "status": "success"
    },
    {
      "timestamp": "2025-03-06T15:09:58.920413",
      "duration": 0.589718,
      "status": "success"
    },
    {
      "timestamp": "2025-03-06T15:12:13.897724",
      "duration": 0.573066,
      "status": "success"
    },
    {
      "timestamp": "2025-03-06T15:12:14.518100",
      "duration": 0.585378,
      "status": "success"
    },
    {
      "timestamp": "2025-03-06T15:13:34.146478",
      "duration": 0.572491,
      "status": "success"
    },
    {
      "timestamp": "2025-03-06T15:13:40.386974",
      "duration": 0.504139,
      "status": "success"
    },
    {
      "timestamp": "2025-03-06T16:38:48.926201",
      "duration": 0.604862,
      "status": "success"
    },
    {
      "timestamp": "2025-03-06T16:44:59.139907",
      "duration": 0.535843,
      "status": "success"
    },
    {
      "timestamp": "2025-03-06T16:55:36.550073",
      "duration": 0.428308,
      "status": "success"
    },
    {
      "timestamp": "2025-03-06T17:27:34.940925",
      "duration": 0.612168,
      "status": "success"
    },
    {
      "timestamp": "2025-03-09T20:54:40.499461",
      "duration": 0.416402,
      "status": "success"
    }
  ],
  "errors": [
    {
      "timestamp": "2025-02-18T08:59:54.163769",
      "error": "Error code: 404 - {'error': {'message': 'The model `deepseek-r1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}"
    },
    {
      "timestamp": "2025-02-18T08:59:56.272870",
      "error": "Error code: 404 - {'error': {'message': 'The model `deepseek-r1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}"
    },
    {
      "timestamp": "2025-02-18T09:00:00.548154",
      "error": "Error code: 404 - {'error': {'message': 'The model `deepseek-r1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}"
    },
    {
      "timestamp": "2025-02-18T09:01:36.392207",
      "error": "Error code: 404 - {'error': {'message': 'The model `deepseek-r1-distill-llama-70b-r1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}"
    },
    {
      "timestamp": "2025-02-18T09:01:38.505729",
      "error": "Error code: 404 - {'error': {'message': 'The model `deepseek-r1-distill-llama-70b-r1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}"
    },
    {
      "timestamp": "2025-02-18T09:01:42.692503",
      "error": "Error code: 404 - {'error': {'message': 'The model `deepseek-r1-distill-llama-70b-r1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}"
    },
    {
      "timestamp": "2025-02-18T09:18:39.499893",
      "error": "Error code: 404 - {'error': {'message': 'The model `deepseek-reasoner` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}"
    },
    {
      "timestamp": "2025-02-18T09:18:41.616430",
      "error": "Error code: 404 - {'error': {'message': 'The model `deepseek-reasoner` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}"
    },
    {
      "timestamp": "2025-02-18T09:18:45.723805",
      "error": "Error code: 404 - {'error': {'message': 'The model `deepseek-reasoner` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}"
    },
    {
      "timestamp": "2025-02-22T10:06:15.881443",
      "error": "Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01hxe8m45ve3nbsgz2mq4h7zyc` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 23903, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
    },
    {
      "timestamp": "2025-02-22T10:06:15.895734",
      "error": "Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01hxe8m45ve3nbsgz2mq4h7zyc` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 23911, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
    },
    {
      "timestamp": "2025-02-22T10:06:18.057341",
      "error": "Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01hxe8m45ve3nbsgz2mq4h7zyc` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 23911, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
    },
    {
      "timestamp": "2025-02-22T10:06:18.064344",
      "error": "Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01hxe8m45ve3nbsgz2mq4h7zyc` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 23903, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
    },
    {
      "timestamp": "2025-02-22T10:06:22.259509",
      "error": "Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01hxe8m45ve3nbsgz2mq4h7zyc` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 23911, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
    },
    {
      "timestamp": "2025-02-22T10:06:22.420135",
      "error": "Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01hxe8m45ve3nbsgz2mq4h7zyc` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 23903, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
    },
    {
      "timestamp": "2025-02-22T10:33:16.902790",
      "error": "Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01hxe8m45ve3nbsgz2mq4h7zyc` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 24080, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
    },
    {
      "timestamp": "2025-02-22T10:33:19.250534",
      "error": "Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01hxe8m45ve3nbsgz2mq4h7zyc` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 24080, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
    },
    {
      "timestamp": "2025-02-22T10:33:23.558997",
      "error": "Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01hxe8m45ve3nbsgz2mq4h7zyc` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 24080, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
    },
    {
      "timestamp": "2025-02-22T11:31:44.397574",
      "error": "Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01hxe8m45ve3nbsgz2mq4h7zyc` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 24080, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
    },
    {
      "timestamp": "2025-02-22T11:31:47.025262",
      "error": "Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01hxe8m45ve3nbsgz2mq4h7zyc` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 24080, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
    },
    {
      "timestamp": "2025-02-22T11:31:51.666398",
      "error": "Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01hxe8m45ve3nbsgz2mq4h7zyc` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 24080, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
    }
  ],
  "performance": {
    "avg_response_time": 0.6044987703488375,
    "total_requests": 344,
    "success_rate": 93.8953488372093
  }
}

================
File: main.py
================
import asyncio
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, Any
from dotenv import load_dotenv

# Get project root directory
PROJECT_ROOT = Path(__file__).parent
LOGS_DIR = PROJECT_ROOT / 'logs'
DATA_DIR = PROJECT_ROOT / 'data'

# Create all required directories first
def setup_directories():
    """Create required directories for the application"""
    directories = [
        LOGS_DIR,
        DATA_DIR / 'secure',
        DATA_DIR / 'metrics',
        DATA_DIR / 'cache'
    ]
    for directory in directories:
        directory.mkdir(parents=True, exist_ok=True)

# Set up directories before any logging
setup_directories()

def setup_logging():
    """Configure comprehensive logging"""
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(LOGS_DIR / 'main.log'),
            logging.StreamHandler()
        ]
    )
    
    # Set specific logger levels
    logging.getLogger('src.email_processing.analyzers.llama').setLevel(logging.DEBUG)
    logging.getLogger('src.email_processing.analyzers.deepseek').setLevel(logging.DEBUG)
    logging.getLogger('src.email_processing.analyzers.response_categorizer').setLevel(logging.DEBUG)
    logging.getLogger('httpx').setLevel(logging.DEBUG)

# Initialize logging before any other imports
setup_logging()

# Now import other modules
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from api.routes import auth, emails, dashboard

sys.path.append(str(Path(__file__).parent / "src"))

from src.email_processing import (
    EmailProcessor, EmailTopic, EmailAgent, LlamaAnalyzer, 
    DeepseekAnalyzer, ResponseCategorizer 
)

from src.integrations.gmail.client import GmailClient
from src.storage.secure import SecureStorage

# Initialize FastAPI app
app = FastAPI(
    title="Sentient Inbox API",
    description="API for intelligent email processing and management",
    version="1.0.0"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Update with specific origins in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(auth.router)
app.include_router(emails.router)
app.include_router(dashboard.router)

# API Models
class ProcessEmailRequest(BaseModel):
    batch_size: int = 100

class ProcessEmailResponse(BaseModel):
    processed_count: int
    error_count: int
    success: bool
    errors: list[str] = []

class MaintenanceResponse(BaseModel):
    key_rotated: bool
    records_cleaned: bool
    success: bool

logger = logging.getLogger(__name__)

def log_execution(message: str):
    """Log execution with timestamp"""
    timestamp = datetime.now().isoformat()
    logger.debug(f"[{timestamp}] {message}")

# Load environment variables
load_dotenv(override=True)

async def process_email_batch(batch_size: int = 100) -> bool:
    log_execution(f"Starting email processing cycle for batch of {batch_size} emails")

    try:
        gmail_client = GmailClient()
        meeting_agent = EmailAgent()
        llama_analyzer = LlamaAnalyzer()
        deepseek_analyzer = DeepseekAnalyzer()
        response_categorizer = ResponseCategorizer()
        secure_storage = SecureStorage()
        processor = EmailProcessor(
        gmail_client=gmail_client,
        llama_analyzer=llama_analyzer,
        deepseek_analyzer=deepseek_analyzer,
        response_categorizer=response_categorizer
    )
        processor.register_agent(EmailTopic.MEETING, meeting_agent)
        
        log_execution("Processing email batch...")
        processed_count, error_count, errors = await processor.process_email_batch(batch_size)
        
        log_execution(f"Email processing cycle completed. "
                     f"Processed: {processed_count}, "
                     f"Errors: {error_count}")
        
        logger.info(f"\nProcessed {processed_count} emails")
        logger.info(f"Encountered {error_count} errors")
        logger.info("Check the log file for detailed model responses and processing information.")
        
        if errors:
            logger.warning("Errors encountered during processing:")
            for error in errors:
                logger.warning(f"- {error}")

        return error_count == 0

    except Exception as e:
        logger.error(f"Error during email processing: {str(e)}", exc_info=True)
        return False

async def main():
    retry_delay = 3  # seconds
    max_retries = 1  # single retry attempt

    for attempt in range(max_retries + 1):
        if attempt > 0:
            logger.info(f"Retry attempt {attempt} after {retry_delay} seconds delay")
            await asyncio.sleep(retry_delay)

        success = await process_email_batch()
        if success:
            break
    
    if not success:
        logger.error("Email processing failed after all retry attempts")

    # Perform maintenance tasks
    await perform_maintenance()

async def perform_maintenance():
    """Perform maintenance tasks such as cleanup and key rotation"""
    try:
        secure_storage = SecureStorage()
        key_rotated = await secure_storage.rotate_key()
        records_cleaned = await secure_storage._cleanup_old_records()
        logger.info(f"Maintenance tasks completed. Key rotated: {key_rotated}, Records cleaned: {records_cleaned}")
    except Exception as e:
        logger.error(f"Error during maintenance tasks: {str(e)}", exc_info=True)

# Initialize components
gmail_client = GmailClient()
llama_analyzer = LlamaAnalyzer()
deepseek_analyzer = DeepseekAnalyzer()
response_categorizer = ResponseCategorizer()
secure_storage = SecureStorage()
meeting_agent = EmailAgent()
processor = EmailProcessor(
    gmail_client=gmail_client,
    llama_analyzer=llama_analyzer,
    deepseek_analyzer=deepseek_analyzer,
    response_categorizer=response_categorizer
)
processor.register_agent(EmailTopic.MEETING, meeting_agent)

# API Routes
@app.on_event("startup")
async def startup_event():
    """Perform initialization tasks on application startup."""
    logger.info("API service starting up")
    
    # Log all registered routes
    routes = []
    for route in app.routes:
        if hasattr(route, "path"):
            routes.append(f"{route.path}")
    logger.info(f"Registered routes: {routes}")

@app.post("/api/process-emails", response_model=ProcessEmailResponse)
async def process_emails(request: ProcessEmailRequest) -> Dict[str, Any]:
    """Process a batch of emails"""
    try:
        processed_count, error_count, errors = await processor.process_email_batch(request.batch_size)
        return {
            "processed_count": processed_count,
            "error_count": error_count,
            "success": error_count == 0,
            "errors": errors
        }
    except Exception as e:
        logger.error(f"API Error processing emails: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/maintenance", response_model=MaintenanceResponse)
async def run_maintenance() -> Dict[str, Any]:
    """Run maintenance tasks"""
    try:
        key_rotated = await secure_storage.rotate_key()
        records_cleaned = await secure_storage._cleanup_old_records()
        return {
            "key_rotated": key_rotated,
            "records_cleaned": records_cleaned,
            "success": True
        }
    except Exception as e:
        logger.error(f"API Error during maintenance: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/health")
async def health_check() -> Dict[str, str]:
    """Check API health"""
    return {"status": "healthy"}

# Script entry point
if __name__ == "__main__":
    log_execution("Starting email processing...")
    asyncio.run(main())
    log_execution("Processing complete")

================
File: meeting_analyzer.py
================
import logging
from typing import Tuple, Dict, Optional
from email.message import Message
from src.email_processing.analyzers import DeepseekAnalyzer

logger = logging.getLogger(__name__)

class MeetingEmailAnalyzer:
    def __init__(self, groq_client):
        logger.info("Initializing MeetingEmailAnalyzer")
        self.deepseek_analyzer = DeepseekAnalyzer(groq_client)

    async def analyze_meeting_email(self, message_id: str, subject: str, content: str, sender: str) -> Tuple[str, Dict]:
        """
        Analyze an email using DeepSeek R1 model to determine appropriate action.

        Args:
            message_id (str): Unique identifier for the email
            subject (str): Email subject
            content (str): Email content
            sender (str): Email sender

        Returns:
            Tuple[str, Dict]: A tuple containing the recommendation and analysis results
        """
        logger.info(f"Analyzing email {message_id} with DeepSeek R1 model")
        
        analysis_result = await self.deepseek_analyzer.analyze_email(
            email_content=content,
            subject=subject,
            sender=sender
        )

        action = self.deepseek_analyzer.decide_action(analysis_result)

        if action == "respond":
            recommendation = "needs_standard_response"
            analysis = {
                "is_meeting": True,
                "action": action,
                "response_text": analysis_result.response_text,
                "confidence": analysis_result.confidence
            }
        elif action == "flag_for_review":
            recommendation = "needs_review"
            analysis = {
                "is_meeting": True,
                "action": action,
                "confidence": analysis_result.confidence
            }
        else:
            recommendation = "needs_standard_response"
            analysis = {
                "is_meeting": False,
                "action": action,
                "confidence": analysis_result.confidence
            }

        return recommendation, analysis

================
File: meeting_mails.json
================
{
  "last_updated": "2025-02-15T11:38:21.356855",
  "meetings": []
}

================
File: memory-bank/activeContext.md
================
# Active Context

## Current Focus
- Implementing REST API endpoints for email processing
- Optimizing FastAPI async request handling
- Enhancing content preprocessing and date handling
- Improving token management and chunking strategies
- Fine-tuning model prompts and response parsing
- Implementing comprehensive error handling and logging

## Recent Changes
- Added FastAPI integration with endpoint definitions
- Implemented CORS middleware for API security
- Created Pydantic models for request/response validation
- Enhanced content chunking with paragraph-based preservation
- Implemented robust date handling with RFC 2822 and ISO 8601 support
- Added sophisticated token limit management
- Enhanced prompt construction and response parsing
- Improved error handling with detailed logging
- Implemented structured data classes for analysis results
- Added comprehensive metadata tracking
- Enhanced pattern preservation system

## Active Decisions

1. API Architecture
   - FastAPI for async request handling
   - Pydantic models for data validation
   - CORS middleware configuration
   - API endpoint organization

2. Email Processing
   - Enhanced content preprocessing with BeautifulSoup
   - Sophisticated date extraction and validation
   - Intelligent content chunking with context preservation
   - Advanced token management and pattern preservation

2. System Architecture
   - Structured data classes for type safety
   - Enhanced error handling with custom exceptions
   - Comprehensive metadata tracking
   - Modular preprocessing components

3. Data Management
   - Structured content processing results
   - Enhanced date pattern storage
   - Comprehensive processing statistics
   - Pattern preservation tracking

## Next Steps

1. API Enhancement
   - Add authentication middleware
   - Implement rate limiting
   - Add request validation
   - Enhance error responses

2. Performance Optimization
   - Fine-tune content chunking parameters
   - Optimize date pattern recognition
   - Enhance token estimation accuracy
   - Improve HTML cleaning efficiency

2. Core Functionality Enhancement
   - Expand date pattern recognition
   - Improve pattern preservation logic
   - Enhance metadata collection
   - Refine content extraction strategies

3. Testing and Validation
   - Add unit tests for date processing
   - Validate content chunking accuracy
   - Test pattern preservation edge cases
   - Verify token limit enforcement

4. Documentation and Standardization
   - Document date processing patterns
   - Update content preprocessing guidelines
   - Create pattern preservation examples
   - Document token management strategies

## Current Considerations

### Technical
- API response time optimization
- Request/response validation
- Token estimation accuracy improvements
- Date pattern recognition enhancements
- HTML cleaning optimization
- Pattern preservation refinements
- Processing statistics analysis

### Functional
- API usability and documentation
- Date extraction accuracy
- Content preservation effectiveness
- Token limit impact on content
- Pattern matching reliability

### Security
- API endpoint security
- CORS configuration
- HTML content sanitization
- Pattern validation security
- Error message safety
- Processing metadata privacy

This context guides our current development priorities and immediate next steps in debugging and enhancing the email management system.

================
File: memory-bank/productContext.md
================
# Product Context

## Problem Statement
Managing meeting-related emails is a complex and time-consuming task that involves:
- Identifying and categorizing meeting requests within email content
- Extracting and validating meeting details (date, time, location)
- Handling various levels of complexity in meeting requests
- Avoiding duplicate meeting responses
- Maintaining organized meeting records
- Generating appropriate responses based on email content

## Solution
The Email Management System provides an advanced, AI-powered solution that:
- Exposes RESTful API endpoints for email processing and system management
- Implements sophisticated content preprocessing with HTML cleaning and pattern preservation
- Provides robust date handling with RFC 2822 and ISO 8601 support
- Utilizes intelligent content chunking for optimal model processing
- Employs a sophisticated three-stage analysis pipeline for accurate email processing
- Automatically identifies and categorizes meeting-related emails using Llama and Deepseek models
- Extracts, validates, and standardizes meeting information with comprehensive date parsing
- Handles complex meeting requests while preserving critical patterns
- Prevents duplicate meeting processing through weekly rolling history
- Maintains structured meeting records with detailed metadata
- Generates context-aware responses using customizable templates
- Offers health monitoring and maintenance endpoints

## User Experience Goals

1. API Integration
   - Simple and intuitive API endpoints
   - Clear request/response structures
   - Comprehensive error handling
   - Detailed processing feedback
   - System health monitoring

2. Accurate Meeting Detection and Analysis
   - Robust HTML content processing for clean input
   - Advanced date pattern recognition and validation
   - AI-powered meeting request identification and classification
   - Reliable detail extraction with pattern preservation
   - Comprehensive content analysis for complex requests
   - Proper handling of ambiguities and missing information

2. Efficient Processing
   - Smart content chunking for optimal processing
   - Automated email monitoring and batch processing
   - Quick and appropriate response generation
   - Pattern-aware content preservation
   - Deduplication of meeting requests
   - Organized meeting data storage and retrieval

3. Reliability and Robustness
   - Comprehensive error handling with retry mechanisms
   - Multiple date format support with fallbacks
   - Pattern validation and preservation
   - Token limit management
   - Robust AI processing with fallback options
   - Detailed DEBUG level logging for monitoring and troubleshooting

4. Security & Privacy
   - HTML content sanitization
   - Secure email content handling with encryption
   - Protected credential management through OAuth2
   - Safe pattern validation and preservation
   - Safe AI processing with content filtering
   - Controlled data storage with backup management

## Key Features

1. RESTful API
   - /api/process-emails endpoint for batch processing
   - /api/maintenance endpoint for system maintenance
   - /api/health endpoint for system monitoring
   - Pydantic-validated request/response models
   - CORS support for web integration

2. Advanced Content Processing
   - Sophisticated HTML cleaning with BeautifulSoup
   - Robust date handling with multiple format support
   - Intelligent content chunking and preservation
   - Pattern-aware token management
   - Comprehensive processing statistics

2. Three-Stage Email Analysis Pipeline
   - Initial classification using Llama model
   - Detailed content analysis using Deepseek R1 model
   - Final decision making and categorization using Llama model

3. Intelligent Meeting Detection and Processing
   - Context-aware content analysis
   - Advanced date pattern recognition
   - Pattern preservation for critical information
   - Extraction of meeting parameters with confidence scores
   - Identification of complex scenarios and ambiguities
   - Categorization into standard_response, needs_review, or ignored

4. Advanced Response Management
   - Customizable response templates
   - Pattern-aware parameter validation
   - Structured date formatting
   - Handling of missing or unclear information
   - Special case management for attachments and multiple requests

5. Robust Data Management
   - Weekly rolling history for deduplication
   - Structured data storage with encryption
   - Enhanced date pattern storage
   - Pattern preservation tracking
   - Processing statistics collection
   - Comprehensive logging and error tracking
   - Performance metrics and analytics

6. Integration and Scalability
   - RESTful API for system integration
   - Gmail API integration with OAuth2 authentication
   - Groq AI integration for advanced natural language processing
   - BeautifulSoup integration for HTML processing
   - RFC 2822 and ISO 8601 date standard support
   - Microservice-ready component design
   - Preparation for future enhancements (e.g., calendar integration, auto-reminder system)

This product transforms meeting coordination from a manual task into an automated, intelligent process, providing accurate analysis, appropriate responses, and maintaining high standards of reliability and security. It is designed to handle complex scenarios while offering scalability for future enhancements and integrations.

================
File: memory-bank/progress.md
================
# Progress Tracking

## Completed Items

### Core Infrastructure
✓ Basic project structure established
✓ Directory organization implemented
✓ Comprehensive DEBUG level logging system configured
✓ Environment management setup
✓ Three-stage email analysis pipeline structure

### Email Processing
✓ Gmail API integration with OAuth2 authentication
✓ Email content parsing
✓ Batch processing system (100 emails per cycle)
✓ Weekly rolling history for deduplication
✓ Unread status management

### AI Integration
✓ Groq API integration for Llama model
✓ Deepseek API integration for Deepseek R1 model
✓ Three-stage analysis pipeline implementation:
  ✓ Stage 1: Initial Meeting Classification (Llama)
  ✓ Stage 2: Detailed Content Analysis (Deepseek R1)
  ✓ Stage 3: Final Decision Making (Llama)
✓ Enhanced error handling with single retry and 3-second delay
✓ Structured output handling for AI model responses

### Data Management
✓ Secure storage implementation with encryption
✓ Structured JSON storage with confidence scores
✓ Weekly rolling history implementation
✓ Backup and restore system
✓ Status management

### Documentation
✓ Memory bank initialization
✓ System architecture documentation
✓ Technical context documentation
✓ Project brief definition

## In Progress

### Core Functionality
- Fine-tuning of Llama model for initial classification and final decision making
- Optimization of Deepseek R1 model for detailed content analysis
- Response quality improvement based on three-stage analysis
- Performance optimization of the analysis pipeline
- Implementation of agent coordination system

### Testing
- Develop comprehensive unit tests for each stage of the analysis pipeline
- Create integration tests to verify end-to-end email processing
- Implement stress tests to ensure system stability under high load
- Validate logging and error reporting functionality across all components

### Documentation
- API documentation for the three-stage analysis pipeline
- Usage guidelines for the new system architecture
- Error handling documentation for the enhanced retry mechanism
- Setup instructions for Groq and Deepseek API integrations

## Pending Items

### System Enhancements
- Monitoring dashboard development
- Agent configuration interface creation
- Response template system enhancement
- Batch processing further optimization

### Monitoring
- Implementation of advanced metrics analysis for the three-stage pipeline
- Development of performance dashboards
- Error pattern detection across all stages
- Usage statistics for Groq and Deepseek APIs

### Security
- Regular security audit system implementation
- Enhanced access control for the three-stage pipeline
- Encryption key rotation mechanism
- Data integrity verification enhancement

## Known Issues
- Fine-tuning required for Llama model in both initial classification and final decision making
- Performance impact of the three-stage analysis pipeline on large email volumes needs assessment
- Potential API rate limiting issues with multiple API calls (Groq and Deepseek)
- Comprehensive testing needed for various email scenarios in the new pipeline
- Documentation updates required to reflect the new three-stage analysis system

## Next Steps
1. Debug the project to ensure correct operation of the three-stage pipeline
2. Implement the agent coordination system
3. Develop the monitoring dashboard
4. Create the agent configuration interface
5. Enhance the response template system

This progress tracking helps maintain focus on development priorities and outstanding tasks for the new three-stage email analysis system.

================
File: memory-bank/projectbrief.md
================
# Email Management System

## Project Overview
An advanced automated email management system with RESTful API endpoints for meeting coordination through Gmail integration. The system employs a sophisticated AI-powered architecture using FastAPI and Groq, with specialized components designed for efficient email processing and response handling. The foundation includes secure storage encryption, API integration capabilities, and Gmail integration with OAuth2 authentication.

## Core Architecture

### API Layer
1. FastAPI Framework
   - RESTful endpoint definitions
   - Async request handling
   - Pydantic model validation
   - CORS middleware
   - Health monitoring

### Content Processing System
1. HTML Content Processing
   - BeautifulSoup-based HTML cleaning
   - Content structure preservation
   - Pattern recognition and preservation
   - Token limit management

2. Date Processing System
   - RFC 2822 and ISO 8601 support
   - Multiple format recognition
   - Timezone handling
   - Fallback strategies

3. Three-Stage Email Analysis Pipeline
   a. Initial Meeting Classification (Llama Model)
      - Content chunking and preprocessing
      - Binary classification of emails
      - Processing of new, unhandled emails
      - Weekly rolling history maintenance

   b. Detailed Content Analysis (Deepseek R1 Model)
      - Pattern-aware content analysis
      - Date extraction and validation
      - Meeting parameter extraction
      - Complexity assessment
      - Missing information detection

   c. Final Decision Making (Llama Model)
      - Analysis consolidation
      - Confidence evaluation
      - Pattern verification
      - Final categorization

### Processing Rules
- API request validation
- Content preprocessing before analysis
- Pattern preservation during processing
- Required meeting details validation
- Date format standardization
- Token limit enforcement
- Batch processing optimization
- Error handling with retries

## Technical Requirements
- FastAPI for REST API framework
- Uvicorn for ASGI server
- BeautifulSoup for HTML processing
- RFC 2822 and ISO 8601 date handling
- Groq API integration for AI processing
- Gmail API integration with OAuth2
- Pattern preservation system
- Token management system
- Secure storage with encryption
- Comprehensive logging (DEBUG level)
- Error handling and recovery

## Project Goals
1. Implement RESTful API endpoints
2. Develop request/response validation
3. Implement advanced content preprocessing
4. Develop robust date handling system
5. Optimize token management
6. Enhance pattern preservation
7. Implement core analysis pipeline
8. Create comprehensive logging
9. Design microservice components

## Current Status
- FastAPI integration complete
- API endpoints implemented
- Request/response validation active
- Advanced content preprocessing implemented
- Robust date handling system in place
- Token management system operational
- Pattern preservation working effectively
- Three-stage pipeline functioning
- Comprehensive logging active

## Next Steps (High Priority)
1. Implement API authentication
2. Add rate limiting
3. Enhance error responses
4. Optimize content chunking
5. Enhance date pattern recognition
6. Improve token estimation
7. Refine pattern preservation
8. Develop monitoring system

## Future Enhancements
- API versioning system
- Enhanced authentication methods
- Advanced rate limiting strategies
- Enhanced date parsing capabilities
- Advanced pattern recognition
- Improved token optimization
- Calendar system integration
- Auto-reminder development
- Frontend customization
- Advanced PII handling
- Metrics expansion

This system aims to provide a robust, scalable email management solution with advanced AI capabilities for efficient meeting coordination and response handling.

================
File: memory-bank/systemPatterns.md
================
# System Patterns

## Architecture Overview

### Core Components
1. Email Processing Pipeline
   - GmailClient (gmail.py)
   - EmailProcessor (email_processor.py)
   - ContentPreprocessor (content.py)
   - EmailDateService (content.py)
   - LlamaAnalyzer (llama_analyzer.py)
   - DeepseekAnalyzer (deepseek_analyzer.py)
   - ResponseGenerator (email_writer.py)

2. Content Processing System
   - HTML Cleaning (BeautifulSoup)
   - Date Pattern Recognition
   - Token Management
   - Pattern Preservation

3. Data Management
   - SecureStorage (secure_storage.py)
   - WeeklyRollingHistory
   - StructuredDataStorage
   - BackupManager

## Design Patterns

### Data Class Pattern
- ProcessedContent for structured content results
- AnalysisResult for model outputs
- Type-safe access to analysis components
- Comprehensive metadata tracking

### Service Pattern
- EmailDateService for date handling
- ContentPreprocessor for content management
- DateProcessor for pattern recognition
- ContentChunker for token management

### Strategy Pattern
- Implemented in content preprocessing
- Flexible date parsing strategies
- Configurable pattern preservation
- Token limit enforcement strategies

### Chain of Responsibility
- Content preprocessing pipeline
- Three-stage analysis system
- Pattern preservation chain
- Error handling chain

## Component Relationships

### Content Processing Flow
```
Raw Email Content
    ↓
HTML Cleaning (BeautifulSoup)
    ↓
Date Extraction (RFC 2822/ISO 8601)
    ↓
Pattern Recognition & Preservation
    ↓
Token Management & Chunking
    ↓
Processed Content
```

### Analysis Flow
```
Unread Email
    ↓
Content Preprocessing
    - HTML Cleaning
    - Date Extraction
    - Pattern Preservation
    - Token Management
    ↓
Stage 1: Initial Classification (LlamaAnalyzer)
    - Content Chunking
    - Classification Prompt
    - Response Parsing
    ↓
Meeting-related? → Yes → Stage 2: Detailed Analysis (DeepseekAnalyzer)
    ↓
Stage 3: Final Decision (LlamaAnalyzer)
    - Decision Prompt
    - Response Validation
    - Metadata Collection
    ↓
Categorization (standard_response, needs_review, ignored)
    ↓
Processing Decision
```

## Technical Decisions

### Content Processing
- BeautifulSoup for robust HTML cleaning
- RFC 2822 and ISO 8601 date parsing
- Paragraph-based content chunking
- Pattern-aware token management

### Error Handling
- Custom ContentProcessingError
- Comprehensive error recovery
- Detailed processing statistics
- Pattern preservation validation

### Performance Optimization
- Efficient HTML parsing
- Smart content chunking
- Pattern-based preservation
- Token estimation optimization

### Security
- HTML content sanitization
- Pattern validation security
- Error message safety
- Processing metadata privacy

## Implementation Patterns

### Content Preprocessing
```python
@dataclass
class ProcessedContent:
    content: str
    metadata: Dict[str, any]
    token_estimate: int
    processing_stats: Dict[str, any]
    extracted_dates: Set[str] = None

class ContentPreprocessor:
    def preprocess_content(self, content: str) -> ProcessedContent:
        # Clean HTML → Extract Dates → Preserve Patterns → Manage Tokens
        cleaned = self._clean_html(content)
        dates = DateProcessor.extract_dates(cleaned)
        preserved = self._extract_key_information(cleaned)
        final = self._enforce_token_limit(preserved)
        return ProcessedContent(...)
```

### Date Processing
```python
class EmailDateService:
    @staticmethod
    def parse_email_date(date_str: str) -> Tuple[datetime, bool]:
        # Try RFC 2822 → ISO 8601 → Additional Formats
        try:
            email_tuple = email.utils.parsedate_tz(date_str)
            if email_tuple:
                return datetime.fromtimestamp(
                    email.utils.mktime_tz(email_tuple),
                    ZoneInfo("UTC")
                ), True
        except:
            # Fallback strategies...
            pass
```

### Content Chunking
```python
class ContentChunker:
    def chunk_content(self, content: str) -> List[str]:
        # Split content while preserving context
        if len(content.split()) <= self.max_tokens:
            return [content]
            
        chunks = []
        for paragraph in content.split('\n\n'):
            # Intelligent chunking with pattern preservation
            if self._should_preserve(paragraph):
                chunks.append(paragraph)
```

### Pattern Preservation
```python
def _extract_key_information(self, content: str) -> str:
    # Keep content manageable while preserving patterns
    paragraphs = content.split('\n\n')
    selected = [paragraphs[0]]  # Keep first
    
    # Preserve important patterns
    for paragraph in paragraphs[1:-1]:
        if any(re.search(pattern, paragraph, re.IGNORECASE) 
              for pattern in self.preserve_patterns):
            selected.append(paragraph)
            
    if len(selected) < self.max_paragraphs:
        selected.append(paragraphs[-1])  # Keep last
        
    return '\n\n'.join(selected)
```

This architecture ensures reliable and efficient email content processing through sophisticated preprocessing, robust error handling, and intelligent pattern preservation.

================
File: memory-bank/techContext.md
================
# Technical Context

## Technologies Used

### Core Technologies
- Python 3.x (with asyncio)
- FastAPI (REST API framework)
- Gmail API
- Groq API (for Llama model integration)
- Deepseek API (for Deepseek R1 model integration)
- BeautifulSoup4 for HTML processing
- JSON for structured data storage and communication
- Uvicorn (ASGI server)

### Key Dependencies
- fastapi: REST API framework
- uvicorn: ASGI server implementation
- pydantic: Data validation and API models
- groq-sdk: Groq API integration for Llama model
- deepseek-sdk: Deepseek API integration for Deepseek R1 model
- google-api-python-client: Gmail API access
- beautifulsoup4: HTML content processing
- python-dotenv: Environment management
- typing-extensions: Type hints support
- pathlib: Path manipulation
- logging: Comprehensive DEBUG level logging
- zoneinfo: Timezone handling for dates
- email: RFC 2822 email parsing

## API Endpoints

### Email Processing
- POST /api/process-emails
  - Process a batch of emails
  - Parameters: batch_size (default: 100)
  - Returns: ProcessEmailResponse

### System Maintenance
- POST /api/maintenance
  - Run maintenance tasks
  - Returns: MaintenanceResponse

### Health Check
- GET /api/health
  - Check API health status
  - Returns: Health status object

## Development Setup

### Environment Configuration
1. Required Environment Variables:
   - GROQ_API_KEY
   - DEEPSEEK_API_KEY
   - Gmail OAuth credentials
   
2. Directory Structure:
```
sentient-inbox/
├── data/
│   ├── secure/          # Encrypted data storage
│   ├── cache/           # Weekly rolling history
│   └── metrics/         # Performance metrics
├── docs/                # Documentation
├── src/
│   ├── email_processing/
│   │   ├── analyzers/   # Llama and Deepseek analyzers
│   │   ├── handlers/    # Content and date processing
│   │   └── classification/
│   ├── integrations/    # API integrations
│   └── utils/          # Shared utilities
├── logs/               # System logs
└── memory-bank/        # System memory
```

3. File Organization:
   - main.py: Entry point
   - content.py: Content preprocessing and date handling
   - llama_analyzer.py: Initial classification and final decision
   - deepseek_analyzer.py: Detailed content analysis
   - email_writer.py: Response generation
   - secure_storage.py: Encrypted data management

## Technical Constraints

### API Limitations
- Groq API rate limits
- Deepseek API rate limits
- Gmail API quotas
- Response time requirements
- Token limits for model inputs
- HTML parsing complexity

### Performance Requirements
- FastAPI async request handling
- Efficient HTML content cleaning
- Accurate date pattern recognition
- Smart content chunking and preservation
- Token limit optimization
- Batch processing of 100 emails per cycle
- Efficient three-stage analysis pipeline
- Quick response generation for standard responses
- Reliable error recovery with single retry and 3-second delay

### Security Requirements
- CORS configuration for API endpoints
- OAuth2 authentication for Gmail integration
- Secure API key storage for Groq and Deepseek
- HTML content sanitization
- Pattern validation security
- Error message safety
- Processing metadata privacy
- Encrypted storage for processed emails

## Development Practices

### Code Standards
- Type hints with dataclasses
- PEP 8 compliance
- Async/await patterns
- Comprehensive error handling
- Pattern preservation practices
- Token management strategies

### Logging System
- DEBUG level logging for all operations
- File-based logging with rotation
- Structured log format for easy parsing
- Comprehensive error and exception logging
- Processing statistics tracking
- Pattern preservation monitoring

### Error Handling
- Custom ContentProcessingError
- Single retry attempt with 3-second delay
- Graceful degradation for parsing failures
- Pattern validation errors
- Token limit violations
- Date parsing fallbacks
- HTML cleaning recovery

### Testing Requirements
- API endpoint testing
- Unit tests for content processing
- Date parsing validation tests
- Pattern preservation verification
- Token management testing
- HTML cleaning validation
- Integration testing for full pipeline
- Error scenario coverage
- Performance benchmarking

## Monitoring & Metrics

### Performance Tracking
- API endpoint response times
- Request success rates
- HTML cleaning efficiency
- Date extraction accuracy
- Pattern preservation success
- Token estimation accuracy
- Content chunking effectiveness
- Processing statistics analysis
- Error frequency by type
- API usage monitoring

### Data Management
- Structured content processing results
- Enhanced date pattern storage
- Pattern preservation tracking
- Processing metadata collection
- Token usage statistics
- Error pattern analysis
- Performance metrics collection

This technical context ensures robust and efficient email processing through sophisticated content handling, comprehensive error management, and detailed performance tracking.

================
File: pre_startup.py
================
"""
Pre-Startup Initialization Script

Performs comprehensive application initialization before main application startup,
ensuring proper directory structure, dependency validation, and environment setup.

This script implements proper error handling and logging following system specifications,
creating a robust startup sequence that aligns with error handling protocols.

Usage:
    python pre_startup.py

Returns:
    0 if initialization successful, 1 otherwise
"""

import os
import sys
import logging
import subprocess
import platform
from pathlib import Path
import json
from datetime import datetime


def setup_logging():
    """Configure basic logging for initialization."""
    # Ensure logs directory exists for handler
    os.makedirs('logs', exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/initialization.log'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger("pre_startup")


def run_directory_setup():
    """
    Run directory setup script with proper error handling.
    
    Returns:
        bool: True if setup successful, False otherwise
    """
    logger = logging.getLogger("pre_startup.directories")
    logger.info("Setting up required directories")
    
    try:
        # Run the directory setup script
        result = subprocess.run(
            [sys.executable, "setup_directories.py"],
            check=True,
            capture_output=True,
            text=True
        )
        logger.info("Directory setup completed successfully")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"Directory setup failed: {e.stderr}")
        return False
    except Exception as e:
        logger.error(f"Error running directory setup: {str(e)}")
        return False


def validate_dependencies():
    """
    Validate critical dependencies with version checks.
    
    Implements comprehensive dependency validation, checking
    for known problematic versions and compatibility issues.
    
    Returns:
        bool: True if all dependencies valid, False otherwise
    """
    logger = logging.getLogger("pre_startup.dependencies")
    logger.info("Validating critical dependencies")
    
    # Critical dependencies to validate
    critical_deps = ["bcrypt", "cryptography", "fastapi", "pydantic"]
    all_valid = True
    
    try:
        # Create a subprocess to check installed packages
        result = subprocess.run(
            [sys.executable, "-m", "pip", "list", "--format=json"],
            check=True,
            capture_output=True,
            text=True
        )
        
        # Parse installed packages
        installed_packages = json.loads(result.stdout)
        installed_dict = {pkg["name"].lower(): pkg["version"] for pkg in installed_packages}
        
        # Check critical dependencies
        for dep in critical_deps:
            if dep.lower() in installed_dict:
                logger.info(f"Found {dep} version {installed_dict[dep.lower()]}")
                
                # Specific version checks
                if dep.lower() == "bcrypt":
                    version = installed_dict[dep.lower()]
                    if version.startswith("4.0."):
                        logger.info(f"✓ {dep} version {version} is compatible")
                    else:
                        logger.warning(f"⚠ {dep} version {version} may have compatibility issues")
                        logger.warning(f"  Recommended version: 4.0.1")
                        all_valid = False
            else:
                logger.error(f"❌ Required dependency {dep} not found")
                all_valid = False
                
        if all_valid:
            logger.info("✓ All critical dependencies validated successfully")
        
        return all_valid
        
    except subprocess.CalledProcessError as e:
        logger.error(f"Dependency validation failed: {e.stderr}")
        return False
    except Exception as e:
        logger.error(f"Error validating dependencies: {str(e)}")
        return False


def create_default_configs():
    """
    Create default configuration files if they don't exist.
    
    Implements configuration initialization following system 
    specifications, ensuring required configuration is available.
    
    Returns:
        bool: True if configuration setup successful, False otherwise
    """
    logger = logging.getLogger("pre_startup.config")
    logger.info("Setting up default configurations")
    
    # Ensure config directory exists
    config_dir = Path("data/config")
    config_dir.mkdir(parents=True, exist_ok=True)
    
    # Default configurations to create
    default_configs = {
        "email_settings.json": {
            "batch_size": 50,
            "auto_respond_enabled": True,
            "confidence_threshold": 0.7,
            "processing_interval_minutes": 15,
            "max_tokens_per_analysis": 4000,
            "models": {
                "classification": "llama-3.3-70b-versatile",
                "analysis": "deepseek-reasoner",
                "response": "llama-3.3-70b-versatile"
            }
        }
    }
    
    success = True
    for filename, config in default_configs.items():
        file_path = config_dir / filename
        
        if not file_path.exists():
            try:
                with open(file_path, 'w') as f:
                    json.dump(config, f, indent=2)
                logger.info(f"Created default configuration: {filename}")
            except Exception as e:
                logger.error(f"Error creating default configuration {filename}: {str(e)}")
                success = False
        else:
            logger.info(f"Configuration file already exists: {filename}")
            
    return success


def initialize_secure_storage():
    """
    Initialize secure storage with proper error handling.
    
    Implements secure storage initialization following error handling
    protocols, ensuring critical storage systems are ready.
    
    Returns:
        bool: True if initialization successful, False otherwise
    """
    logger = logging.getLogger("pre_startup.storage")
    logger.info("Initializing secure storage")
    
    try:
        # Ensure secure storage directory exists
        storage_path = Path("data/secure")
        storage_path.mkdir(parents=True, exist_ok=True)
        
        # Placeholder for actual secure storage initialization
        # This would integrate with the SecureStorage class in a real implementation
        
        # For now, just touch required files to prevent startup errors
        required_files = [
            storage_path / ".initialized",
            storage_path / "backups" / ".initialized"
        ]
        
        for file_path in required_files:
            file_path.parent.mkdir(parents=True, exist_ok=True)
            if not file_path.exists():
                file_path.touch()
                logger.info(f"Created initialization marker: {file_path}")
        
        return True
    except Exception as e:
        logger.error(f"Error initializing secure storage: {str(e)}")
        return False


def initialize_metrics():
    """
    Initialize metrics storage with proper error handling.
    
    Implements metrics storage initialization following system
    specifications, ensuring metrics systems are available.
    
    Returns:
        bool: True if initialization successful, False otherwise
    """
    logger = logging.getLogger("pre_startup.metrics")
    logger.info("Initializing metrics storage")
    
    try:
        # Ensure metrics directory exists
        metrics_path = Path("data/metrics")
        metrics_path.mkdir(parents=True, exist_ok=True)
        
        # Initialize groq_metrics.json if it doesn't exist
        groq_metrics_file = metrics_path / "groq_metrics.json"
        if not groq_metrics_file.exists():
            default_metrics = {
                "requests": [],
                "errors": [],
                "performance": {
                    "avg_response_time": 0,
                    "total_requests": 0,
                    "success_rate": 100
                },
                "initialized_at": datetime.now().isoformat()
            }
            
            with open(groq_metrics_file, 'w') as f:
                json.dump(default_metrics, f, indent=2)
            logger.info(f"Created default metrics file: {groq_metrics_file}")
        
        # Initialize email_stats.json if it doesn't exist
        email_stats_file = metrics_path / "email_stats.json"
        if not email_stats_file.exists():
            default_stats = {
                "total_emails_processed": 0,
                "emails_by_category": {
                    "meeting": 0,
                    "needs_review": 0,
                    "not_actionable": 0,
                    "not_meeting": 0
                },
                "average_processing_time_ms": 0,
                "success_rate": 0,
                "stats_period_days": 30,
                "last_updated": datetime.now().isoformat()
            }
            
            with open(email_stats_file, 'w') as f:
                json.dump(default_stats, f, indent=2)
            logger.info(f"Created default email stats file: {email_stats_file}")
            
        return True
    except Exception as e:
        logger.error(f"Error initializing metrics storage: {str(e)}")
        return False


def validate_environment():
    """
    Validate environment settings and dependencies.
    
    Implements environment validation following system specifications,
    ensuring required environment variables and settings are available.
    
    Returns:
        bool: True if environment valid, False otherwise
    """
    logger = logging.getLogger("pre_startup.environment")
    logger.info("Validating environment")
    
    # System requirements
    min_python_version = (3, 9)
    
    # Check Python version
    python_version = tuple(map(int, platform.python_version_tuple()[:2]))
    python_valid = python_version >= min_python_version
    
    if python_valid:
        logger.info(f"✓ Python version: {platform.python_version()}")
    else:
        logger.error(f"❌ Python version {platform.python_version()} is below minimum {'.'.join(map(str, min_python_version))}")
    
    # Check for .env file
    env_file = Path(".env")
    if env_file.exists():
        logger.info("✓ Found .env file")
        
        # Check for critical environment variables
        critical_env_vars = ["GROQ_API_KEY"]
        missing_vars = []
        
        for var in critical_env_vars:
            if not os.getenv(var):
                missing_vars.append(var)
                
        if missing_vars:
            logger.warning(f"⚠ Missing critical environment variables: {', '.join(missing_vars)}")
            logger.warning("  Application may fail at runtime if these are required")
    else:
        logger.warning("⚠ No .env file found - environment variables must be set manually")
    
    return python_valid


def main():
    """
    Main initialization sequence for application pre-startup.
    
    Implements comprehensive initialization following system specifications
    and error handling protocols, ensuring proper application startup.
    
    Returns:
        int: 0 for success, 1 for failure
    """
    # Setup basic logging
    logger = setup_logging()
    logger.info("=== Starting pre-startup initialization ===")
    
    # Track initialization steps
    steps = [
        {"name": "Run directory setup", "func": run_directory_setup},
        {"name": "Validate dependencies", "func": validate_dependencies},
        {"name": "Create default configurations", "func": create_default_configs},
        {"name": "Initialize secure storage", "func": initialize_secure_storage},
        {"name": "Initialize metrics", "func": initialize_metrics},
        {"name": "Validate environment", "func": validate_environment}
    ]
    
    # Run all initialization steps
    success = True
    for step in steps:
        logger.info(f"Running step: {step['name']}")
        step_success = step["func"]()
        
        if step_success:
            logger.info(f"✓ Step completed successfully: {step['name']}")
        else:
            logger.error(f"❌ Step failed: {step['name']}")
            success = False
    
    # Finalize
    if success:
        logger.info("=== Pre-startup initialization completed successfully ===")
        return 0
    else:
        logger.error("=== Pre-startup initialization completed with errors ===")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

================
File: processors/__init__.py
================
"""
Processors module initialization.

This module provides core processing components for the email analysis system,
implementing content preprocessing, analysis pipeline integration, and
extensible processing capabilities.

Architecture:
- Modular processor implementations
- Extensible base classes
- Standardized interfaces
- Comprehensive configuration
"""

from . import ContentPreprocessor
from src.email_processing.base import BaseProcessor
__all__ = [
    'ContentPreprocessor',
    'BaseProcessor'
]

================
File: processors/content_processor.py
================
# content_processor.py

from typing import Dict, Optional, List, Tuple, Set
from bs4 import BeautifulSoup
import re
import logging
from dataclasses import dataclass
from datetime import datetime
import email.utils
from zoneinfo import ZoneInfo

logger = logging.getLogger(__name__)

@dataclass
class ProcessedContent:
    """
    Structured container for processed content results.
    """
    content: str
    metadata: Dict[str, any]
    token_estimate: int
    processing_stats: Dict[str, any]
    extracted_dates: Set[str] = None

class EmailDateService:
    """
    Enhanced date handling service with proper RFC 2822 and ISO 8601 support
    """
    
    @staticmethod
    def parse_email_date(date_str: str) -> Tuple[datetime, bool]:
        """
        Robust email date parsing with fallback strategies
        """
        try:
            # Try RFC 2822 parsing first
            email_tuple = email.utils.parsedate_tz(date_str)
            if email_tuple:
                timestamp = email.utils.mktime_tz(email_tuple)
                dt = datetime.fromtimestamp(timestamp, ZoneInfo("UTC"))
                return dt, True

            # Fallback to ISO 8601 parsing
            try:
                return datetime.fromisoformat(date_str.replace('Z', '+00:00')), True
            except ValueError:
                pass

            # Additional fallback formats
            for fmt in ('%a, %d %b %Y %H:%M:%S %z',
                        '%d %b %Y %H:%M:%S %z',
                        '%Y-%m-%d %H:%M:%S%z'):
                try:
                    return datetime.strptime(date_str, fmt), True
                except ValueError:
                    continue

            return None, False
        except Exception as e:
            logger.warning(f"Date parsing failed for '{date_str}': {e}")
            return None, False

    @staticmethod
    def format_iso(dt: datetime) -> str:
        """Format datetime to ISO 8601 with timezone"""
        if not dt.tzinfo:
            dt = dt.replace(tzinfo=ZoneInfo("UTC"))
        return dt.isoformat()

class DateProcessor:
    """
    Enhanced date pattern recognition and validation
    """
    
    DATE_PATTERNS = [
        r'\w{3},\s+\d{1,2}\s+\w{3}\s+\d{4}\s+\d{2}:\d{2}:\d{2}\s+(?:[+-]\d{4}|[A-Z]{3})',
        r'\d{4}-\d{2}-\d{2}(?:T\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:?\d{2})?)?',
        r'\d{1,2}/\d{1,2}/\d{4}\s+\d{1,2}:\d{2}(?::\d{2})?(?:\s*[AaPp][Mm])?',
        r'\d{1,2}-\d{1,2}-\d{4}\s+\d{1,2}:\d{2}(?::\d{2})?(?:\s*[AaPp][Mm])?',
        r'\d{1,2}:\d{2}(?::\d{2})?\s*(?:[AaPp][Mm])?',
        r'(?:today|tomorrow|next\s+(?:monday|tuesday|wednesday|thursday|friday|saturday|sunday))',
    ]

    @classmethod
    def extract_dates(cls, content: str) -> Set[str]:
        dates = set()
        for pattern in cls.DATE_PATTERNS:
            matches = re.finditer(pattern, content, re.IGNORECASE)
            for match in matches:
                date_str = match.group()
                dt, success = EmailDateService.parse_email_date(date_str)
                if success:
                    dates.add(EmailDateService.format_iso(dt))
                else:
                    dates.add(date_str)
        return dates

class ContentPreprocessor:
    """
    Main content preprocessing implementation with enhanced date handling
    """
    
    def __init__(self, max_tokens: int = 4000, preserve_patterns: Optional[List[str]] = None, config: Optional[Dict] = None):
        self.max_tokens = max_tokens
        self.preserve_patterns = preserve_patterns or [
            r'meeting\s+at\s+\d{1,2}(?::\d{2})?\s*(?:am|pm)?',
            r'schedule.*meeting',
            r'discuss.*at\s+\d{1,2}(?::\d{2})?',
            r'conference.*\d{1,2}(?::\d{2})?',
            r'appointment.*\d{1,2}(?::\d{2})?'
        ]
        self.config = config or {}
    
    def preprocess_content(self, content: str) -> ProcessedContent:
        processing_stats = {"original_length": len(content)}
        
        cleaned_content = self._clean_html(content)
        processing_stats["cleaned_length"] = len(cleaned_content)
        
        extracted_dates = DateProcessor.extract_dates(cleaned_content)
        processing_stats["dates_found"] = len(extracted_dates)
        
        extracted_info = self._extract_key_information(cleaned_content)
        processing_stats["extraction_success"] = bool(extracted_info)
        
        final_content = self._enforce_token_limit(extracted_info)
        processing_stats.update({
            "final_length": len(final_content),
            "estimated_tokens": len(final_content.split())
        })
        
        return ProcessedContent(
            content=final_content,
            metadata={
                "preserved_patterns": self._find_preserved_patterns(final_content),
                "date_patterns": list(extracted_dates)
            },
            token_estimate=processing_stats["estimated_tokens"],
            processing_stats=processing_stats,
            extracted_dates=extracted_dates
        )

    # Rest of the ContentPreprocessor methods remain unchanged
    # (_clean_html, _extract_key_information, _enforce_token_limit, _find_preserved_patterns)

class ContentProcessingError(Exception):
    pass

================
File: requirements.txt
================
# Core Dependencies
google-auth-oauthlib>=0.8.0    # Gmail OAuth integration
google-api-python-client>=2.0.0 # Gmail API client
openai>=1.0.0                  # OpenAI integration
python-dotenv>=1.0.0           # Environment management
protobuf>=4.0.0                # Protocol buffers support
groq>=0.3.0                    # Groq AI integration
cryptography>=41.0.0           # Secure storage encryption

# Authentication Dependencies
passlib>=1.7.4                 # Password hashing
bcrypt==4.0.1                  # Secure password hashing backend for passlib (fixed version for compatibility)

# Content Processing
beautifulsoup4>=4.12.0         # HTML content processing
lxml>=4.9.0                    # XML/HTML parser

# Networking and Async
aiohttp>=3.9.0                 # Async HTTP client
urllib3>=2.0.0                 # HTTP client

# Date/Time Handling
zoneinfo; python_version < '3.9'  # Timezone support for Python <3.9

# Development Dependencies
pytest>=7.0.0                  # Testing framework
pytest-asyncio>=0.21.0         # Async testing support
pytest-cov>=4.1.0              # Test coverage
black>=23.0.0                  # Code formatting
isort>=5.12.0                  # Import sorting
mypy>=1.0.0                    # Type checking

# API Framework
fastapi>=0.104.1
uvicorn>=0.24.0
pydantic>=2.4.2
email-validator>=2.1.0
python-multipart>=0.0.6
python-jose>=3.3.0
passlib>=1.7.4
starlette>=0.27.0
pydantic-settings>=2.0.0

================
File: run_api.py
================
"""
API Server Runner

Provides a convenient entry point for running the FastAPI server
with proper configuration and environment setup.
"""

import argparse
import logging
import os
import sys
import uvicorn

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger("api_runner")

def parse_arguments():
    """Parse command line arguments for the API server."""
    parser = argparse.ArgumentParser(description="Run the Email Management API server")
    
    parser.add_argument(
        "--host", 
        type=str, 
        default="127.0.0.1", 
        help="Host to bind the server to (default: 127.0.0.1)"
    )
    
    parser.add_argument(
        "--port", 
        type=int, 
        default=8000, 
        help="Port to bind the server to (default: 8000)"
    )
    
    parser.add_argument(
        "--reload", 
        action="store_true", 
        help="Enable auto-reload for development"
    )
    
    parser.add_argument(
        "--env", 
        type=str, 
        choices=["development", "testing", "production"],
        default="development",
        help="Environment to run in (default: development)"
    )
    
    return parser.parse_args()

def setup_environment(env):
    """Set up environment variables for the API server."""
    os.environ["ENVIRONMENT"] = env
    
    # Set debug mode for development and testing
    if env in ["development", "testing"]:
        os.environ["DEBUG"] = "true"
    else:
        os.environ["DEBUG"] = "false"
    
    # Ensure required directories exist
    required_dirs = [
        "data/config",
        "data/metrics",
        "data/secure",
        "logs"
    ]
    
    for directory in required_dirs:
        os.makedirs(directory, exist_ok=True)
        logger.info(f"Ensured directory exists: {directory}")

def main():
    """Run the API server with the specified configuration."""
    args = parse_arguments()
    
    # Setup environment
    setup_environment(args.env)
    
    logger.info(f"Starting API server in {args.env} mode")
    logger.info(f"Server will be available at http://{args.host}:{args.port}")
    
    if args.env == "development":
        logger.info(f"API documentation will be available at http://{args.host}:{args.port}/docs")
    
    # Run the server
    uvicorn.run(
        "api.main:app",
        host=args.host,
        port=args.port,
        reload=args.reload,
        log_level="info" if args.env == "production" else "debug"
    )

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("Server stopped by user")
        sys.exit(0)
    except Exception as e:
        logger.error(f"Error running server: {str(e)}")
        sys.exit(1)

================
File: schedule.json
================
{
  "meetings": []
}

================
File: setup_directories.py
================
"""
Directory Structure Setup Utility

This script ensures proper directory structure initialization before application startup.
It creates all required directories based on application configuration, implementing
proper error handling and logging following system specifications.

Usage:
    python setup_directories.py

Returns:
    0 for success, 1 for failure
"""

import os
import sys
import logging
from pathlib import Path


def setup_logging():
    """Configure basic logging for directory setup."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler()]
    )
    return logging.getLogger("setup")


def create_required_directories():
    """
    Create all required application directories following system specifications.
    
    Implements comprehensive directory structure creation with proper error
    handling and validation, following requirements from system documentation.
    
    Returns:
        bool: True if all directories created successfully, False otherwise
    """
    logger = setup_logging()
    
    # Define required directories based on system documentation
    required_directories = [
        # Core system directories
        "logs",                    # Logging directory for all components
        "data",                    # Base data directory
        "data/config",             # Configuration storage
        "data/secure",             # Encrypted data storage
        "data/secure/backups",     # Backup storage for secure data
        "data/metrics",            # Performance metrics storage
        "data/cache",              # Temporary cache storage
    ]
    
    success = True
    created_dirs = []
    
    # Create each directory with proper error handling
    for directory in required_directories:
        try:
            dir_path = Path(directory)
            if not dir_path.exists():
                dir_path.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory: {directory}")
                created_dirs.append(directory)
            else:
                logger.info(f"Directory already exists: {directory}")
        except Exception as e:
            logger.error(f"Error creating directory {directory}: {str(e)}")
            success = False
    
    # Summary output
    if created_dirs:
        logger.info(f"Created {len(created_dirs)} directories: {', '.join(created_dirs)}")
    else:
        logger.info("No new directories needed to be created")
        
    return success


def check_file_permissions():
    """
    Verify write permissions on critical directories.
    
    Implements permission validation for key application directories
    to ensure proper operation throughout the application lifecycle.
    
    Returns:
        bool: True if all permissions are correct, False otherwise
    """
    logger = setup_logging()
    critical_dirs = ["logs", "data/secure", "data/config"]
    success = True
    
    for directory in critical_dirs:
        test_file = Path(directory) / ".permission_test"
        try:
            # Attempt to create and remove a test file
            with open(test_file, 'w') as f:
                f.write("test")
            test_file.unlink()
            logger.info(f"Confirmed write permissions on {directory}")
        except Exception as e:
            logger.error(f"Permission error on {directory}: {str(e)}")
            success = False
            
    return success


if __name__ == "__main__":
    logger = setup_logging()
    logger.info("Starting directory structure initialization")
    
    # Create required directories
    directory_success = create_required_directories()
    
    # Check file permissions
    permission_success = check_file_permissions()
    
    if directory_success and permission_success:
        logger.info("Directory structure initialization completed successfully")
        sys.exit(0)
    else:
        logger.error("Directory structure initialization failed")
        sys.exit(1)

================
File: setup.ps1
================
# Print colored output for better readability
function Write-ColorOutput($ForegroundColor) {
    $fc = $host.UI.RawUI.ForegroundColor
    $host.UI.RawUI.ForegroundColor = $ForegroundColor
    if ($args) {
        Write-Output $args
    }
    else {
        $input | Write-Output
    }
    $host.UI.RawUI.ForegroundColor = $fc
}

Write-ColorOutput Green "Starting Sentient Inbox setup..."

# Ensure Python 3.x is available
try {
    $pythonVersion = python --version
    if (-not $pythonVersion.Contains("Python 3")) {
        Write-ColorOutput Red "Python 3 is not installed. Please install Python 3.8 or newer."
        exit 1
    }
} catch {
    Write-ColorOutput Red "Python 3 is not installed or not in PATH. Please install Python 3.8 or newer."
    exit 1
}

# Create and activate virtual environment
Write-ColorOutput Yellow "Creating Python virtual environment..."
if (Test-Path -Path "venv") {
    Write-Output "Virtual environment already exists, skipping creation"
} else {
    python -m venv venv
}

# Activate virtual environment
Write-ColorOutput Yellow "Activating virtual environment..."
& .\venv\Scripts\Activate.ps1

# Install dependencies
Write-ColorOutput Yellow "Installing dependencies from requirements.txt..."
python -m pip install --upgrade pip
python -m pip install -r requirements.txt

# Create required directories
Write-ColorOutput Yellow "Creating necessary directories..."
New-Item -ItemType Directory -Force -Path data\secure\backups
New-Item -ItemType Directory -Force -Path data\metrics
New-Item -ItemType Directory -Force -Path data\config
New-Item -ItemType Directory -Force -Path data\cache
New-Item -ItemType Directory -Force -Path logs

# Create environment file template if it doesn't exist
if (-not (Test-Path -Path ".env")) {
    Write-ColorOutput Yellow "Creating .env template file..."
    @"
# API Keys
GROQ_API_KEY=your_groq_api_key_here
DEEPSEEK_API_KEY=your_deepseek_api_key_here
OPENAI_API_KEY=your_openai_api_key_here

# OAuth settings for Gmail
# Note: You'll need to create OAuth credentials in Google Cloud Console
# and download client_secret.json file to the project root

# Application settings
ENVIRONMENT=development
DEBUG=true
LOG_LEVEL=DEBUG
"@ | Out-File -FilePath ".env" -Encoding utf8
    Write-ColorOutput Green ".env template created. Please edit it with your actual API keys."
} else {
    Write-Output ".env file already exists, skipping creation"
}

# Check for client_secret.json for Gmail OAuth
if (-not (Test-Path -Path "client_secret.json")) {
    Write-ColorOutput Yellow "NOTICE: client_secret.json file for Gmail OAuth is missing."
    Write-Output "You'll need to create OAuth credentials in Google Cloud Console and"
    Write-Output "download the client_secret.json file to the project root before running the application."
}

Write-ColorOutput Green "Setup complete!"
Write-ColorOutput Green "To activate the virtual environment in the future, run:"
Write-Output "  .\venv\Scripts\Activate.ps1"
Write-ColorOutput Green "To run the API server:"
Write-Output "  python run_api.py --env development --reload"
Write-ColorOutput Green "To process emails as a batch job:"
Write-Output "  python main.py"

================
File: src/__init__.py
================
"""
Source package initialization.
"""

from . import email_processing
from . import integrations
from . import storage
from . import utils

__all__ = [
    'email_processing',
    'integrations',
    'storage',
    'utils'
]

================
File: src/auth/google_oauth.py
================
"""
Google OAuth Provider Implementation

Implements the Google OAuth provider service with comprehensive token
management, user profile handling, and error recovery.

Design Considerations:
- Robust error handling with retry mechanism
- Comprehensive token validation and refresh
- Detailed logging for troubleshooting
- Full Google API scope support
- Secure storage of client credentials
"""

import os
import json
import logging
import time
from typing import Dict, List, Optional, Tuple, Any
import aiohttp
from datetime import datetime, timedelta

from src.auth.oauth_base import OAuthProvider

logger = logging.getLogger(__name__)

class GoogleOAuthProvider(OAuthProvider):
    """
    Google OAuth2 provider implementation.
    
    Implements the OAuth provider interface for Google authentication
    with comprehensive error handling, token management, and user
    profile handling.
    """
    
    # Google OAuth endpoints
    AUTH_URL = "https://accounts.google.com/o/oauth2/v2/auth"
    TOKEN_URL = "https://oauth2.googleapis.com/token"
    USERINFO_URL = "https://www.googleapis.com/oauth2/v3/userinfo"
    TOKENINFO_URL = "https://oauth2.googleapis.com/tokeninfo"
    REVOKE_URL = "https://oauth2.googleapis.com/revoke"
    
    def __init__(self):
        """
        Initialize the Google OAuth provider with client credentials.
        
        Loads client credentials from environment variables and configures
        default scopes for Gmail access.
        
        Raises:
            ValueError: If required environment variables are missing
        """
        self.client_id = os.getenv("GOOGLE_CLIENT_ID")
        self.client_secret = os.getenv("GOOGLE_CLIENT_SECRET")
        
        if not self.client_id or not self.client_secret:
            raise ValueError(
                "Google OAuth configuration incomplete. "
                "Please set GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET environment variables."
            )
            
        # Default scopes for Gmail access
        self.default_scopes = [
            "https://www.googleapis.com/auth/userinfo.email",
            "https://www.googleapis.com/auth/userinfo.profile",
            "https://www.googleapis.com/auth/gmail.readonly",
            "https://www.googleapis.com/auth/gmail.modify"
        ]
        
        logger.info("Google OAuth provider initialized successfully")
    
    @property
    def provider_name(self) -> str:
        """Return the provider name."""
        return "google"
    
    async def get_authorization_url(self, redirect_uri: str, state: Optional[str] = None) -> Tuple[str, str]:
        """
        Generate Google OAuth authorization URL.
        
        Args:
            redirect_uri: URI to redirect after authorization
            state: Optional state parameter for security
            
        Returns:
            Tuple of (authorization_url, state)
        """
        # Generate random state if not provided
        if not state:
            import uuid
            state = str(uuid.uuid4())
        
        # Construct authorization URL
        scopes_str = "%20".join(self.default_scopes)
        params = {
            "client_id": self.client_id,
            "redirect_uri": redirect_uri,
            "response_type": "code",
            "scope": scopes_str,
            "access_type": "offline",
            "state": state,
            "prompt": "consent"  # Always show consent screen for refresh token
        }
        
        # Build query string
        query_string = "&".join(f"{k}={v}" for k, v in params.items())
        auth_url = f"{self.AUTH_URL}?{query_string}"
        
        logger.debug(f"Generated Google authorization URL with state: {state}")
        return auth_url, state
    
    async def exchange_code_for_tokens(self, code: str, redirect_uri: str) -> Dict[str, Any]:
        """
        Exchange authorization code for tokens.
        
        Args:
            code: Authorization code from Google
            redirect_uri: Redirect URI used in the authorization request
            
        Returns:
            Dictionary containing token information
            
        Raises:
            ValueError: If code exchange fails
        """
        try:
            # Prepare token request
            payload = {
                "client_id": self.client_id,
                "client_secret": self.client_secret,
                "code": code,
                "redirect_uri": redirect_uri,
                "grant_type": "authorization_code"
            }
            
            # Make request to token endpoint
            async with aiohttp.ClientSession() as session:
                async with session.post(self.TOKEN_URL, data=payload) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"Failed to exchange code: {error_text}")
                        raise ValueError(f"Failed to exchange code: {error_text}")
                        
                    token_data = await response.json()
                    
            # Get user information
            user_info = await self.get_user_info(token_data["access_token"])
            
            # Combine token data with user info
            result = {
                "access_token": token_data["access_token"],
                "refresh_token": token_data.get("refresh_token"),  # May not be present
                "expires_in": token_data["expires_in"],
                "token_type": token_data["token_type"],
                "id_token": token_data.get("id_token"),
                "provider_user_id": user_info["sub"],
                "provider_email": user_info["email"],
                "scopes": token_data.get("scope", "").split(" "),
                "user_info": user_info
            }
            
            logger.info(f"Successfully exchanged code for tokens for user: {user_info['email']}")
            return result
            
        except Exception as e:
            logger.error(f"Error exchanging code for tokens: {str(e)}")
            raise ValueError(f"Failed to exchange authorization code: {str(e)}")
    
    async def refresh_access_token(self, refresh_token: str) -> Dict[str, Any]:
        """
        Refresh expired access token.
        
        Args:
            refresh_token: Refresh token to use
            
        Returns:
            Dictionary containing new token information
            
        Raises:
            ValueError: If token refresh fails
        """
        try:
            # Prepare refresh request
            payload = {
                "client_id": self.client_id,
                "client_secret": self.client_secret,
                "refresh_token": refresh_token,
                "grant_type": "refresh_token"
            }
            
            # Make request to token endpoint
            async with aiohttp.ClientSession() as session:
                async with session.post(self.TOKEN_URL, data=payload) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"Failed to refresh token: {error_text}")
                        raise ValueError(f"Failed to refresh token: {error_text}")
                        
                    token_data = await response.json()
            
            # Prepare result (note: refresh token is not replaced)
            result = {
                "access_token": token_data["access_token"],
                "expires_in": token_data["expires_in"],
                "token_type": token_data["token_type"],
                "scope": token_data.get("scope", "").split(" ")
            }
            
            logger.info("Successfully refreshed Google access token")
            return result
            
        except Exception as e:
            logger.error(f"Error refreshing access token: {str(e)}")
            raise ValueError(f"Failed to refresh access token: {str(e)}")
    
    async def get_user_info(self, access_token: str) -> Dict[str, Any]:
        """
        Get user information from Google.
        
        Args:
            access_token: Access token to use
            
        Returns:
            Dictionary containing user profile information
            
        Raises:
            ValueError: If user info request fails
        """
        try:
            # Prepare headers
            headers = {"Authorization": f"Bearer {access_token}"}
            
            # Make request to userinfo endpoint
            async with aiohttp.ClientSession() as session:
                async with session.get(self.USERINFO_URL, headers=headers) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"Failed to get user info: {error_text}")
                        raise ValueError(f"Failed to get user info: {error_text}")
                        
                    user_info = await response.json()
            
            logger.debug(f"Successfully retrieved user info for: {user_info.get('email')}")
            return user_info
            
        except Exception as e:
            logger.error(f"Error getting user info: {str(e)}")
            raise ValueError(f"Failed to get user info: {str(e)}")
    
    async def validate_token(self, access_token: str) -> bool:
        """
        Validate if access token is still valid.
        
        Args:
            access_token: Access token to validate
            
        Returns:
            Boolean indicating if token is valid
        """
        try:
            # Make request to tokeninfo endpoint
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.TOKENINFO_URL}?access_token={access_token}") as response:
                    if response.status != 200:
                        logger.debug(f"Token validation failed with status: {response.status}")
                        return False
                        
                    # Check expiration
                    token_info = await response.json()
                    if "error" in token_info:
                        logger.debug(f"Token validation failed: {token_info['error']}")
                        return False
                        
                    # Token is valid
                    return True
                    
        except Exception as e:
            logger.error(f"Error validating token: {str(e)}")
            return False
    
    async def revoke_token(self, token: str, token_type_hint: str = "access_token") -> bool:
        """
        Revoke an access or refresh token.
        
        Args:
            token: Token to revoke
            token_type_hint: Type of token ('access_token' or 'refresh_token')
            
        Returns:
            Boolean indicating if revocation was successful
        """
        try:
            # Prepare revocation request
            payload = {
                "token": token,
                "token_type_hint": token_type_hint
            }
            
            # Make request to revocation endpoint
            async with aiohttp.ClientSession() as session:
                async with session.post(self.REVOKE_URL, data=payload) as response:
                    # HTTP 200 means token was revoked or was already invalid
                    success = response.status == 200
                    
                    if not success:
                        error_text = await response.text()
                        logger.error(f"Failed to revoke token: {error_text}")
                    else:
                        logger.info(f"Successfully revoked {token_type_hint}")
                        
                    return success
                    
        except Exception as e:
            logger.error(f"Error revoking token: {str(e)}")
            return False

================
File: src/auth/microsoft_oauth.py
================
"""
Microsoft OAuth Provider Implementation

Implements the Microsoft OAuth provider service with comprehensive token
management, user profile handling, and error recovery.

Design Considerations:
- Robust error handling with retry mechanism
- Comprehensive token validation and refresh
- Detailed logging for troubleshooting
- Full Microsoft Graph API scope support
- Secure storage of client credentials
"""

import os
import json
import logging
import time
from typing import Dict, List, Optional, Tuple, Any
import aiohttp
from datetime import datetime, timedelta

from src.auth.oauth_base import OAuthProvider

logger = logging.getLogger(__name__)

class MicrosoftOAuthProvider(OAuthProvider):
    """
    Microsoft OAuth2 provider implementation.
    
    Implements the OAuth provider interface for Microsoft authentication
    with comprehensive error handling, token management, and user
    profile handling.
    """
    
    # Microsoft OAuth endpoints
    AUTH_URL = "https://login.microsoftonline.com/common/oauth2/v2.0/authorize"
    TOKEN_URL = "https://login.microsoftonline.com/common/oauth2/v2.0/token"
    USERINFO_URL = "https://graph.microsoft.com/v1.0/me"
    OUTLOOK_MAIL_URL = "https://graph.microsoft.com/v1.0/me/messages"
    REVOKE_URL = "https://login.microsoftonline.com/common/oauth2/v2.0/logout"
    
    def __init__(self):
        """
        Initialize the Microsoft OAuth provider with client credentials.
        
        Loads client credentials from environment variables and configures
        default scopes for Outlook access.
        
        Raises:
            ValueError: If required environment variables are missing
        """
        self.client_id = os.getenv("MICROSOFT_CLIENT_ID")
        self.client_secret = os.getenv("MICROSOFT_CLIENT_SECRET")
        
        if not self.client_id or not self.client_secret:
            raise ValueError(
                "Microsoft OAuth configuration incomplete. "
                "Please set MICROSOFT_CLIENT_ID and MICROSOFT_CLIENT_SECRET environment variables."
            )
            
        # Default scopes for Outlook access
        self.default_scopes = [
            "User.Read",
            "Mail.Read",
            "Mail.ReadWrite",
            "offline_access"  # For refresh tokens
        ]
        
        logger.info("Microsoft OAuth provider initialized successfully")
    
    @property
    def provider_name(self) -> str:
        """Return the provider name."""
        return "microsoft"
    
    async def get_authorization_url(self, redirect_uri: str, state: Optional[str] = None) -> Tuple[str, str]:
        """
        Generate Microsoft OAuth authorization URL.
        
        Args:
            redirect_uri: URI to redirect after authorization
            state: Optional state parameter for security
            
        Returns:
            Tuple of (authorization_url, state)
        """
        # Generate random state if not provided
        if not state:
            import uuid
            state = str(uuid.uuid4())
        
        # Construct authorization URL
        scopes_str = "%20".join(self.default_scopes)
        params = {
            "client_id": self.client_id,
            "redirect_uri": redirect_uri,
            "response_type": "code",
            "scope": scopes_str,
            "state": state,
            "response_mode": "query"
        }
        
        # Build query string
        query_string = "&".join(f"{k}={v}" for k, v in params.items())
        auth_url = f"{self.AUTH_URL}?{query_string}"
        
        logger.debug(f"Generated Microsoft authorization URL with state: {state}")
        return auth_url, state
    
    async def exchange_code_for_tokens(self, code: str, redirect_uri: str) -> Dict[str, Any]:
        """
        Exchange authorization code for tokens.
        
        Args:
            code: Authorization code from Microsoft
            redirect_uri: Redirect URI used in the authorization request
            
        Returns:
            Dictionary containing token information
            
        Raises:
            ValueError: If code exchange fails
        """
        try:
            # Prepare token request
            payload = {
                "client_id": self.client_id,
                "client_secret": self.client_secret,
                "code": code,
                "redirect_uri": redirect_uri,
                "grant_type": "authorization_code",
                "scope": " ".join(self.default_scopes)
            }
            
            # Make request to token endpoint
            async with aiohttp.ClientSession() as session:
                async with session.post(self.TOKEN_URL, data=payload) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"Failed to exchange code: {error_text}")
                        raise ValueError(f"Failed to exchange code: {error_text}")
                        
                    token_data = await response.json()
                    
            # Get user information
            user_info = await self.get_user_info(token_data["access_token"])
            
            # Combine token data with user info
            result = {
                "access_token": token_data["access_token"],
                "refresh_token": token_data.get("refresh_token"),  # May not be present
                "expires_in": token_data["expires_in"],
                "token_type": token_data["token_type"],
                "provider_user_id": user_info["id"],
                "provider_email": user_info["userPrincipalName"],
                "scopes": token_data.get("scope", "").split(" "),
                "user_info": user_info
            }
            
            logger.info(f"Successfully exchanged code for tokens for user: {user_info['userPrincipalName']}")
            return result
            
        except Exception as e:
            logger.error(f"Error exchanging code for tokens: {str(e)}")
            raise ValueError(f"Failed to exchange authorization code: {str(e)}")
    
    async def refresh_access_token(self, refresh_token: str) -> Dict[str, Any]:
        """
        Refresh expired access token.
        
        Args:
            refresh_token: Refresh token to use
            
        Returns:
            Dictionary containing new token information
            
        Raises:
            ValueError: If token refresh fails
        """
        try:
            # Prepare refresh request
            payload = {
                "client_id": self.client_id,
                "client_secret": self.client_secret,
                "refresh_token": refresh_token,
                "grant_type": "refresh_token",
                "scope": " ".join(self.default_scopes)
            }
            
            # Make request to token endpoint
            async with aiohttp.ClientSession() as session:
                async with session.post(self.TOKEN_URL, data=payload) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"Failed to refresh token: {error_text}")
                        raise ValueError(f"Failed to refresh token: {error_text}")
                        
                    token_data = await response.json()
            
            # Prepare result (note: Microsoft may return a new refresh token)
            result = {
                "access_token": token_data["access_token"],
                "refresh_token": token_data.get("refresh_token"),  # Microsoft may return a new one
                "expires_in": token_data["expires_in"],
                "token_type": token_data["token_type"],
                "scope": token_data.get("scope", "").split(" ")
            }
            
            logger.info("Successfully refreshed Microsoft access token")
            return result
            
        except Exception as e:
            logger.error(f"Error refreshing access token: {str(e)}")
            raise ValueError(f"Failed to refresh access token: {str(e)}")
    
    async def get_user_info(self, access_token: str) -> Dict[str, Any]:
        """
        Get user information from Microsoft Graph API.
        
        Args:
            access_token: Access token to use
            
        Returns:
            Dictionary containing user profile information
            
        Raises:
            ValueError: If user info request fails
        """
        try:
            # Prepare headers
            headers = {"Authorization": f"Bearer {access_token}"}
            
            # Make request to Microsoft Graph API
            async with aiohttp.ClientSession() as session:
                async with session.get(self.USERINFO_URL, headers=headers) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"Failed to get user info: {error_text}")
                        raise ValueError(f"Failed to get user info: {error_text}")
                        
                    user_info = await response.json()
            
            logger.debug(f"Successfully retrieved user info for: {user_info.get('userPrincipalName')}")
            return user_info
            
        except Exception as e:
            logger.error(f"Error getting user info: {str(e)}")
            raise ValueError(f"Failed to get user info: {str(e)}")
    
    async def validate_token(self, access_token: str) -> bool:
        """
        Validate if access token is still valid.
        
        Args:
            access_token: Access token to validate
            
        Returns:
            Boolean indicating if token is valid
        """
        try:
            # Microsoft doesn't have a direct validation endpoint, so we'll try to use the token
            headers = {"Authorization": f"Bearer {access_token}"}
            
            # Make a simple request to the user profile endpoint
            async with aiohttp.ClientSession() as session:
                async with session.get(self.USERINFO_URL, headers=headers) as response:
                    if response.status != 200:
                        logger.debug(f"Token validation failed with status: {response.status}")
                        return False
                    
                    # Token is valid
                    return True
                    
        except Exception as e:
            logger.error(f"Error validating token: {str(e)}")
            return False
    
    async def revoke_token(self, token: str, token_type_hint: str = "access_token") -> bool:
        """
        Revoke an access or refresh token.
        
        Note: Microsoft OAuth 2.0 doesn't have a dedicated revocation endpoint.
        This method will always return True but log a warning.
        
        Args:
            token: Token to revoke
            token_type_hint: Type of token ('access_token' or 'refresh_token')
            
        Returns:
            Boolean indicating if revocation was successful
        """
        # Microsoft doesn't have a proper revocation endpoint in their OAuth implementation
        # The best practice is to clear the token on the client side
        logger.warning(
            "Microsoft OAuth doesn't support token revocation. "
            "The token will remain valid until it expires."
        )
        return True

================
File: src/auth/oauth_base.py
================
"""
Base OAuth Provider Service

Defines the abstract base class for OAuth provider implementations with
standardized interface and comprehensive error handling.

Design Considerations:
- Provider-agnostic abstract interface
- Comprehensive error handling
- Standardized token management
- Common OAuth flow patterns
- Detailed logging for troubleshooting
"""

import logging
from abc import ABC, abstractmethod
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any

logger = logging.getLogger(__name__)

class OAuthProvider(ABC):
    """
    Abstract base class for OAuth provider implementations.
    
    Defines the standard interface that all OAuth provider implementations
    must follow, ensuring consistent behavior across different providers.
    """
    
    @property
    @abstractmethod
    def provider_name(self) -> str:
        """Return the name of this OAuth provider."""
        pass
    
    @abstractmethod
    async def get_authorization_url(self, redirect_uri: str, state: Optional[str] = None) -> Tuple[str, str]:
        """
        Generate the authorization URL for the OAuth flow.
        
        Args:
            redirect_uri: URL to redirect to after authorization
            state: Optional state parameter for security
            
        Returns:
            Tuple containing (authorization_url, state)
        """
        pass
    
    @abstractmethod
    async def exchange_code_for_tokens(self, code: str, redirect_uri: str) -> Dict[str, Any]:
        """
        Exchange authorization code for access and refresh tokens.
        
        Args:
            code: Authorization code from OAuth provider
            redirect_uri: Redirect URI used in authorization request
            
        Returns:
            Dictionary containing token information
        """
        pass
    
    @abstractmethod
    async def refresh_access_token(self, refresh_token: str) -> Dict[str, Any]:
        """
        Refresh an expired access token.
        
        Args:
            refresh_token: Refresh token to use
            
        Returns:
            Dictionary containing new token information
        """
        pass
    
    @abstractmethod
    async def get_user_info(self, access_token: str) -> Dict[str, Any]:
        """
        Retrieve user information using the access token.
        
        Args:
            access_token: OAuth access token
            
        Returns:
            Dictionary containing user profile information
        """
        pass
    
    @abstractmethod
    async def validate_token(self, access_token: str) -> bool:
        """
        Validate if an access token is still valid.
        
        Args:
            access_token: OAuth access token to validate
            
        Returns:
            Boolean indicating if token is valid
        """
        pass
    
    @abstractmethod
    async def revoke_token(self, token: str, token_type_hint: str = "access_token") -> bool:
        """
        Revoke an access or refresh token.
        
        Args:
            token: Token to revoke
            token_type_hint: Type of token ('access_token' or 'refresh_token')
            
        Returns:
            Boolean indicating if revocation was successful
        """
        pass

================
File: src/auth/oauth_factory.py
================
"""
OAuth Provider Factory

Provides a centralized factory for creating OAuth provider instances
with proper provider registration and comprehensive error handling.

Design Considerations:
- Dynamic provider registration
- Lazy provider instantiation
- Comprehensive error handling
- Clean separation of provider implementations
"""

import logging
from typing import Dict, Type, Optional

from src.auth.oauth_base import OAuthProvider
from src.auth.google_oauth import GoogleOAuthProvider
from src.auth.microsoft_oauth import MicrosoftOAuthProvider

logger = logging.getLogger(__name__)

class OAuthProviderFactory:
    """
    Factory for creating OAuth provider instances.
    
    Implements a registry-based factory pattern for OAuth provider
    instantiation with proper error handling and lazy loading.
    """
    
    # Provider registry mapping provider names to classes
    _registry: Dict[str, Type[OAuthProvider]] = {
        "google": GoogleOAuthProvider,
        "microsoft": MicrosoftOAuthProvider
    }
    
    # Provider cache for singleton instances
    _instances: Dict[str, OAuthProvider] = {}
    
    @classmethod
    def register_provider(cls, name: str, provider_class: Type[OAuthProvider]) -> None:
        """
        Register a new provider class.
        
        Args:
            name: Provider name
            provider_class: Provider class
            
        Raises:
            ValueError: If provider name is already registered
        """
        if name in cls._registry:
            raise ValueError(f"Provider {name} is already registered")
            
        cls._registry[name] = provider_class
        logger.info(f"Registered OAuth provider: {name}")
    
    @classmethod
    def get_provider(cls, name: str) -> OAuthProvider:
        """
        Get provider instance by name.
        
        Args:
            name: Provider name
            
        Returns:
            Provider instance
            
        Raises:
            ValueError: If provider is not registered
        """
        if name not in cls._registry:
            logger.error(f"No provider registered with name: {name}")
            raise ValueError(f"No provider registered with name: {name}")
            
        # Create instance if not in cache
        if name not in cls._instances:
            try:
                cls._instances[name] = cls._registry[name]()
                logger.debug(f"Created new provider instance: {name}")
            except Exception as e:
                logger.error(f"Failed to create provider {name}: {str(e)}")
                raise ValueError(f"Failed to create provider {name}: {str(e)}")
                
        return cls._instances[name]
    
    @classmethod
    def get_available_providers(cls) -> Dict[str, str]:
        """
        Get list of available provider names.
        
        Returns:
            Dictionary mapping provider names to display names
        """
        display_names = {
            "google": "Google",
            "microsoft": "Microsoft / Outlook"
        }
        
        return {
            name: display_names.get(name, name.capitalize())
            for name in cls._registry.keys()
        }

================
File: src/auth/service.py
================
"""
Authentication Service Implementation with OAuth Support

Provides comprehensive authentication services including token generation,
validation, user credential verification, and OAuth authentication.

Design Considerations:
- Secure token handling with proper cryptographic practices
- OAuth integration with multiple providers
- Comprehensive error handling and logging
- Clear separation of concerns
- Future extensibility for different authentication methods
"""

import os
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Union, Any, Tuple

from jose import jwt
from passlib.context import CryptContext
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
import aiohttp

from api.config import get_settings
from api.models.auth import TokenData, UserCredentials, Token
from src.storage.user_repository import UserRepository
from src.auth.oauth_factory import OAuthProviderFactory

# Configure logging
logger = logging.getLogger(__name__)

# Initialize security components
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")


class AuthenticationService:
    """
    Comprehensive authentication service with secure token management and OAuth integration.
    
    Implements secure user authentication, token generation and validation,
    comprehensive permission management, and OAuth provider integration for
    multiple identity providers.
    """
    
    def __init__(self):
        """
        Initialize authentication service with configuration settings.
        
        Loads security configuration and sets up authentication context
        with proper error handling and validation.
        """
        self.settings = get_settings()
        self.secret_key = self.settings.JWT_SECRET_KEY.get_secret_value()
        self.algorithm = self.settings.JWT_ALGORITHM
        self.access_token_expire_minutes = self.settings.JWT_TOKEN_EXPIRE_MINUTES
        
        # OAuth provider factory
        self.oauth_factory = OAuthProviderFactory
        
        # Load legacy users for backward compatibility
        # These will be migrated to the database on first access
        self.legacy_users_db = {
            "admin": {
                "username": "admin",
                "hashed_password": pwd_context.hash("securepassword"),
                "email": "admin@example.com",
                "permissions": ["admin", "process", "view"]
            },
            "viewer": {
                "username": "viewer",
                "hashed_password": pwd_context.hash("viewerpass"),
                "email": "viewer@example.com",
                "permissions": ["view"]
            }
        }
        
        logger.info("Authentication service initialized with OAuth provider integration")
    
    def verify_password(self, plain_password: str, hashed_password: str) -> bool:
        """
        Verify password against stored hash with proper cryptographic verification.
        
        Args:
            plain_password: Plain text password to verify
            hashed_password: Stored password hash
            
        Returns:
            Boolean indicating if password matches
        """
        return pwd_context.verify(plain_password, hashed_password)
    
    def get_password_hash(self, password: str) -> str:
        """
        Generate secure password hash with proper cryptographic practices.
        
        Args:
            password: Plain text password to hash
            
        Returns:
            Secure password hash
        """
        return pwd_context.hash(password)
    
    async def authenticate_user(self, username: str, password: str) -> Optional[Dict[str, Any]]:
        """
        Authenticate user against stored credentials with secure verification.
        
        Implements comprehensive authentication with proper error handling
        and secure password verification using cryptographic best practices.
        
        Args:
            username: Username to authenticate
            password: Password to verify
            
        Returns:
            User data if authenticated, None otherwise
        """
        # First try to get user from database
        user = await UserRepository.get_user_by_username(username)
        
        if not user:
            # Check legacy users
            legacy_user = self.legacy_users_db.get(username)
            if not legacy_user:
                logger.warning(f"Authentication attempt for unknown user: {username}")
                return None
                
            # Verify password for legacy user
            if not self.verify_password(password, legacy_user["hashed_password"]):
                logger.warning(f"Failed password verification for legacy user: {username}")
                return None
                
            # Create user in database
            try:
                user = await UserRepository.create_user(
                    email=legacy_user["email"],
                    username=legacy_user["username"],
                    display_name=legacy_user.get("display_name"),
                    permissions=legacy_user["permissions"],
                    profile_picture=legacy_user.get("profile_picture")
                )
                logger.info(f"Migrated legacy user to database: {username}")
            except Exception as e:
                logger.error(f"Failed to migrate legacy user {username}: {str(e)}")
                # Return legacy user data
                return legacy_user
        else:
            # We don't support password authentication for database users
            # They should use OAuth
            logger.warning(f"Password authentication attempted for OAuth user: {username}")
            return None
        
        logger.info(f"User authenticated successfully: {username}")
        return user.to_dict()
    
    async def get_authorization_url(self, provider: str, redirect_uri: str) -> Tuple[str, str]:
        """
        Get authorization URL for OAuth provider.
        
        Args:
            provider: OAuth provider name
            redirect_uri: Redirect URI for callback
            
        Returns:
            Tuple of (authorization_url, state)
            
        Raises:
            ValueError: If provider is not supported
        """
        try:
            oauth_provider = self.oauth_factory.get_provider(provider)
            return await oauth_provider.get_authorization_url(redirect_uri)
        except Exception as e:
            logger.error(f"Error getting authorization URL for provider {provider}: {str(e)}")
            raise ValueError(f"Failed to get authorization URL: {str(e)}")
    
    async def process_oauth_callback(self, provider: str, code: str, redirect_uri: str) -> Dict[str, Any]:
        """
        Process OAuth callback and create or update user.
        
        Args:
            provider: OAuth provider name
            code: Authorization code
            redirect_uri: Redirect URI used in authorization request
            
        Returns:
            Dictionary containing user information and access token
            
        Raises:
            ValueError: If OAuth callback processing fails
        """
        try:
            # Get OAuth provider
            oauth_provider = self.oauth_factory.get_provider(provider)
            
            # Exchange code for tokens
            tokens = await oauth_provider.exchange_code_for_tokens(code, redirect_uri)
            
            # Get or create user
            user = await self._get_or_create_oauth_user(provider, tokens)
            
            # Save OAuth tokens
            await UserRepository.save_oauth_token(
                user_id=user["id"],
                provider=provider,
                provider_user_id=tokens["provider_user_id"],
                provider_email=tokens["provider_email"],
                access_token=tokens["access_token"],
                refresh_token=tokens.get("refresh_token"),
                expires_in=tokens["expires_in"],
                scopes=tokens["scopes"]
            )
            
            # Update last login timestamp
            await UserRepository.update_user_last_login(user["id"])
            
            # Generate JWT token
            access_token = self.create_access_token(
                data={
                    "username": user["username"],
                    "permissions": user["permissions"]
                }
            )
            
            return {
                "user": user,
                "access_token": access_token,
                "token_type": "bearer",
                "expires_in": self.access_token_expire_minutes * 60
            }
            
        except Exception as e:
            logger.error(f"Error processing OAuth callback: {str(e)}")
            raise ValueError(f"Failed to process OAuth callback: {str(e)}")
    
    async def _get_or_create_oauth_user(self, provider: str, tokens: Dict[str, Any]) -> Dict[str, Any]:
        """
        Get existing user or create a new one based on OAuth profile.
        
        Args:
            provider: OAuth provider name
            tokens: Token data including user profile
            
        Returns:
            User data dictionary
        """
        provider_user_id = tokens["provider_user_id"]
        provider_email = tokens["provider_email"]
        user_info = tokens["user_info"]
        
        # Try to find user by OAuth provider and ID
        user = await UserRepository.get_user_by_oauth(provider, provider_user_id)
        
        if user:
            # User exists, return data
            logger.info(f"Found existing user for {provider} ID {provider_user_id}")
            return user.to_dict()
            
        # Try to find user by email
        user = await UserRepository.get_user_by_email(provider_email)
        
        if user:
            # User exists with this email, link OAuth account
            logger.info(f"Linking {provider} account to existing user: {user.username}")
            # User will be linked when we save the OAuth token
            return user.to_dict()
            
        # Create new user
        # Generate username from email
        email_username = provider_email.split('@')[0]
        base_username = email_username.lower()
        username = base_username
        
        # Check if username exists
        attempts = 0
        while await UserRepository.get_user_by_username(username):
            attempts += 1
            username = f"{base_username}{attempts}"
            
        # Get display name from provider-specific fields
        display_name = None
        if provider == "google":
            display_name = user_info.get("name")
        elif provider == "microsoft":
            display_name = user_info.get("displayName")
            
        # Create new user with view permission
        try:
            user = await UserRepository.create_user(
                email=provider_email,
                username=username,
                display_name=display_name,
                permissions=["view"],
                profile_picture=user_info.get("picture")
            )
            logger.info(f"Created new user from {provider} authentication: {username}")
            return user.to_dict()
        except Exception as e:
            logger.error(f"Failed to create user from OAuth profile: {str(e)}")
            raise ValueError(f"Failed to create user: {str(e)}")
    
    def create_access_token(self, data: Dict[str, Any], expires_delta: Optional[timedelta] = None) -> str:
        """
        Create JWT access token with proper security practices.
        
        Generates secure JWT token with appropriate claims and expiration
        using cryptographic best practices.
        
        Args:
            data: Token payload data
            expires_delta: Optional custom expiration time
            
        Returns:
            Encoded JWT token string
        """
        to_encode = data.copy()
        
        # Set token expiration
        if expires_delta:
            expire = datetime.utcnow() + expires_delta
        else:
            expire = datetime.utcnow() + timedelta(minutes=self.access_token_expire_minutes)
        
        to_encode.update({"exp": expire})
        
        # Generate token with proper security
        try:
            encoded_jwt = jwt.encode(
                to_encode, 
                self.secret_key, 
                algorithm=self.algorithm
            )
            return encoded_jwt
        except Exception as e:
            logger.error(f"Token generation error: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Could not generate authentication token"
            )
    
    async def get_current_user(self, token: str = Depends(oauth2_scheme)) -> Dict[str, Any]:
        """
        Validate token and extract current user with proper security validation.
        
        Implements comprehensive token validation with proper error handling
        and security verification of token claims.
        
        Args:
            token: JWT token to validate
            
        Returns:
            Validated user data
            
        Raises:
            HTTPException: If token is invalid or expired
        """
        credentials_exception = HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication credentials",
            headers={"WWW-Authenticate": "Bearer"},
        )
        
        try:
            # Decode and validate token
            payload = jwt.decode(
                token, 
                self.secret_key, 
                algorithms=[self.algorithm]
            )
            
            # Extract user information
            username = payload.get("username")
            if username is None:
                logger.warning("Token missing username claim")
                raise credentials_exception
            
            permissions = payload.get("permissions", [])
            token_data = TokenData(username=username, permissions=permissions, exp=payload["exp"])
            
        except jwt.JWTError as e:
            logger.warning(f"Token validation error: {str(e)}")
            raise credentials_exception
        
        # Verify user exists in database
        user = await UserRepository.get_user_by_username(token_data.username)
        
        if user:
            return user.to_dict()
            
        # Check legacy users (for backward compatibility)
        legacy_user = self.legacy_users_db.get(token_data.username)
        if legacy_user:
            # Create user in database
            try:
                user = await UserRepository.create_user(
                    email=legacy_user["email"],
                    username=legacy_user["username"],
                    display_name=legacy_user.get("display_name"),
                    permissions=legacy_user["permissions"],
                    profile_picture=legacy_user.get("profile_picture")
                )
                logger.info(f"Migrated legacy user to database during token validation: {token_data.username}")
                return user.to_dict()
            except Exception as e:
                logger.error(f"Failed to migrate legacy user {token_data.username}: {str(e)}")
                # Return legacy user data
                return legacy_user
                
        # User not found in database or legacy storage
        logger.warning(f"Token contains unknown user: {token_data.username}")
        raise credentials_exception
    
    async def get_current_user_permissions(self, user: Dict[str, Any] = Depends(get_current_user)) -> List[str]:
        """
        Extract permissions from authenticated user.
        
        Args:
            user: Authenticated user data
            
        Returns:
            List of user permissions
        """
        return user.get("permissions", [])
    
    async def check_permission(self, required_permission: str, user: Dict[str, Any] = Depends(get_current_user)) -> bool:
        """
        Check if user has required permission with proper authorization verification.
        
        Implements comprehensive permission checking with proper error handling
        and security validation.
        
        Args:
            required_permission: Permission to check
            user: Authenticated user data
            
        Returns:
            Boolean indicating if user has permission
            
        Raises:
            HTTPException: If user lacks required permission
        """
        user_permissions = user.get("permissions", [])
        
        if required_permission not in user_permissions:
            logger.warning(
                f"Permission denied: {user['username']} lacks {required_permission} permission"
            )
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Not authorized for {required_permission}",
            )
        
        return True
    

# Singleton instance for dependency injection
auth_service = AuthenticationService()

# Common dependencies for route handlers
def get_auth_service() -> AuthenticationService:
    """Provide authentication service instance for dependency injection."""
    return auth_service

async def require_admin(
    auth_service: AuthenticationService = Depends(get_auth_service),
    user: Dict[str, Any] = Depends(auth_service.get_current_user)
) -> bool:
    """Require admin permission for route access."""
    return await auth_service.check_permission("admin", user)

async def require_process(
    auth_service: AuthenticationService = Depends(get_auth_service),
    user: Dict[str, Any] = Depends(auth_service.get_current_user)
) -> bool:
    """Require process permission for route access."""
    return await auth_service.check_permission("process", user)

async def require_view(
    auth_service: AuthenticationService = Depends(get_auth_service),
    user: Dict[str, Any] = Depends(auth_service.get_current_user)
) -> bool:
    """Require view permission for route access."""
    return await auth_service.check_permission("view", user)

================
File: src/config/analyzer_config.py
================
# config/analyzer_config.py

ANALYZER_CONFIG = {
    "default_analyzer": {
        "model": {
            "name": "llama-3.3-70b-versatile",
            "temperature": 0.3,
            "max_tokens": 2000,
            "max_input_tokens": 4000,
            "retry_count": 3,
            "retry_delay": 2
        },
        "content_processing": {
            "preserve_patterns": [
                r'meeting\s+at\s+\d{1,2}(?::\d{2})?\s*(?:am|pm)?',
                r'schedule.*meeting',
                r'discuss.*at\s+\d{1,2}(?::\d{2})?'
            ],
            "max_paragraphs": 3,
            "token_buffer": 500
        }
    },
    "meeting_analyzer": {
        "model": {
            "name": "llama-3.3-70b-versatile",
            "temperature": 0.3,
            "max_tokens": 2000
        },
        "logging": {
            "base_dir": "logs/meeting_analyzer",
            "archive_retention_days": 30,
            "max_log_size_mb": 10,
            "backup_count": 5
        },
        "analysis": {
            "confidence_threshold": 0.7,
            "review_threshold": 0.5
        }
    },
    "deepseek_analyzer": {
        "model": {
            "name": "deepseek-reasoner",
            "temperature": 0.7,
            "max_tokens": 5000,
            "api_endpoint": "https://api.deepseek.com/v1",
            "api_key": "${DEEPSEEK_API_KEY}"
        },
        "logging": {
            "base_dir": "logs/deepseek_analyzer",
            "archive_retention_days": 30,
            "max_log_size_mb": 10,
            "backup_count": 5
        },
        "analysis": {
            "confidence_threshold": 0.7,
            "review_threshold": 0.5,
            "system_prompt": "You are an AI specialized in analyzing meeting-related communications. Extract key information systematically and provide structured analysis."
        },
        # Simple numeric timeout in seconds - MUST BE A SINGLE INTEGER
        "timeout": 180,  # 3 minutes total timeout
        
        # Retry configuration
        "retry_count": 1,     # Number of retry attempts (1 retry = 2 total attempts)
        "retry_delay": 3,     # Delay between retry attempts in seconds
        
        # Development fallback configuration
        "use_fallback": False,           # Set to True to use mock responses during development
        "use_fallback_on_error": True    # Use fallback analysis when API errors occur
    }
}

================
File: src/email_processing/__init__.py
================
"""
Email processing package initialization.
"""

from .models import EmailMetadata, EmailTopic
from .classification.classifier import EmailClassifier, EmailRouter
from .handlers.writer import EmailAgent
from .analyzers.llama import LlamaAnalyzer
from .analyzers.deepseek import DeepseekAnalyzer
from .analyzers.response_categorizer import ResponseCategorizer
from .processor import EmailProcessor

__all__ = [
    'EmailMetadata',
    'EmailTopic',
    'EmailClassifier',
    'EmailRouter',
    'EmailAgent',
    'LlamaAnalyzer',
    'DeepseekAnalyzer',
    'ResponseCategorizer',
    'EmailProcessor'
]

================
File: src/email_processing/analyzers/deepseek.py
================
"""
DeepseekAnalyzer: Detailed Email Content Analysis Service

Implements comprehensive email analysis using the DeepSeek Reasoner model,
following the architecture defined in analysis-pipeline.md. This component
serves as the second stage of the three-stage email analysis pipeline.

Design Considerations:
- Comprehensive content analysis with proper parameter extraction
- Dynamic response generation with enhanced formality control
- Robust error handling with configurable retry mechanisms
- Clear logging for system monitoring and debugging
"""

import logging
import os
import json
import asyncio
import time
import re
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any

from src.config.analyzer_config import ANALYZER_CONFIG

logger = logging.getLogger(__name__)

class DeepseekAnalyzer:
    """
    Detailed content analyzer using DeepSeek Reasoner model.
    
    Implements the second stage of the email analysis pipeline as specified
    in analysis-pipeline.md, providing comprehensive content analysis and
    response generation with enhanced formality control.
    
    Key capabilities:
    - Complete meeting parameter extraction (date, time, location, agenda)
    - Formality detection and adjustment (two levels more formal than sender)
    - Response generation with appropriate business language
    - Robust error handling with configurable retry
    """
    
    def __init__(self):
        """
        Initialize the analyzer with configuration from central settings.
        
        Loads configuration parameters from ANALYZER_CONFIG, establishes
        API connection details, and configures operational behaviors like
        timeouts and retry logic.
        """
        # Load configuration from centralized analyzer configuration
        self.config = ANALYZER_CONFIG.get("deepseek_analyzer", {})
        self.model_name = self.config.get("model", {}).get("name", "deepseek-reasoner")
        self.api_endpoint = self.config.get("model", {}).get("api_endpoint", "https://api.deepseek.com/v1")
        self.api_key = os.getenv("DEEPSEEK_API_KEY", "")
        self.temperature = self.config.get("model", {}).get("temperature", 0.7)
        self.timeout = self.config.get("timeout", 180)
        self.retry_count = self.config.get("retry_count", 1)
        self.retry_delay = self.config.get("retry_delay", 3)
        
        # Define formality levels for reference
        self.formality_levels = {
            1: "Very casual",
            2: "Casual",
            3: "Neutral",
            4: "Formal", 
            5: "Very formal"
        }
        
        # Validate API key existence
        if not self.api_key:
            logger.warning("DEEPSEEK_API_KEY not found in environment variables")
        
        logger.debug(f"DeepseekAnalyzer initialized with configuration: "
                   f"model={self.model_name}, endpoint={self.api_endpoint}, "
                   f"temperature={self.temperature}, timeout={self.timeout}s")

    async def analyze_email(self, email_content: str) -> Tuple[Dict, str, str, Optional[str]]:
        """
        Analyze email content with comprehensive parameter extraction.
        
        Implements detailed content analysis with proper formality detection,
        tone adjustment, and response generation according to the rules in
        analysis-pipeline.md and response-management.md.
        
        Args:
            email_content: Raw email content to analyze
            
        Returns:
            Tuple containing:
            - analysis_data: Structured analysis results
            - response_text: Generated response with appropriate formality
            - recommendation: Processing recommendation (standard_response, needs_review, ignore)
            - error: Error message if analysis failed, None otherwise
        """
        # Generate unique request ID for tracking and logging
        request_id = f"deepseek-{datetime.now().strftime('%Y%m%d%H%M%S')}-{os.urandom(3).hex()}"
        
        try:
            # Handle empty or unavailable content
            if not email_content or email_content.strip() == "No content available":
                email_content = "No content available"
                
            # Log analysis start
            logger.info(f"[{request_id}] Starting detailed email content analysis")
            logger.debug(f"[{request_id}] Analyzing content of length: {len(email_content)} characters")
            logger.debug(f"[{request_id}] Content preview: {email_content[:100]}..." 
                        if len(email_content) > 100 else email_content)

            # Generate analysis prompt with formality instructions
            prompt = self._create_analysis_prompt(email_content, request_id)
            logger.debug(f"[{request_id}] Analysis prompt generated with length: {len(prompt)}")
            
            # Call API with retry logic
            self._start_time = time.time()
            analysis = await self._call_deepseek_api(prompt, request_id)
            logger.debug(f"[{request_id}] Raw analysis result:\n{analysis}")
            
            # Extract components from the unstructured response
            analysis_data, response_text, recommendation = self._process_analysis_result(analysis, request_id)
            
            # Log completion and details
            logger.info(f"[{request_id}] Successfully completed detailed analysis in "
                       f"{time.time() - self._start_time:.3f} seconds")
            logger.debug(f"[{request_id}] Analysis results:\n"
                        f"Analysis data: {json.dumps(analysis_data)}\n"
                        f"Response text length: {len(response_text)}\n"
                        f"Recommendation: {recommendation}")
            
            return analysis_data, response_text, recommendation, None
            
        except Exception as e:
            # Log error and return error information
            logger.error(f"[{request_id}] Analysis failed: {str(e)}")
            return {}, "", "needs_review", f"Analysis failed: {str(e)}"

    def _create_analysis_prompt(self, email_content: str, request_id: str) -> str:
        """
        Create comprehensive analysis prompt with formality guidance.
        
        Implements a structured prompt designed to extract meeting parameters,
        detect tone, adjust formality, and generate appropriate responses
        following the pipeline specifications.
        
        Args:
            email_content: Email content to analyze
            request_id: Unique identifier for this analysis request
            
        Returns:
            Formatted prompt string with detailed instructions
        """
        # Create system prompt with comprehensive instructions
        system_prompt = f"""You are an expert email analyzer specialized in meeting-related communications.

TASK:
Analyze this email to determine if it contains meeting information and extract key parameters.

ANALYSIS REQUIREMENTS:
1. Check if all REQUIRED elements are present:
   - Specific time/date
   - Location (physical or virtual meeting link)
   - Agenda/purpose
   - List of attendees

2. Assess any risk factors that might require human review:
   - Financial commitments
   - Legal implications
   - Complex multi-party coordination
   - Sensitive content
   - Technical complexity

3. Detect sender's tone and formality level on this 5-point scale:
   1. Very casual (emojis, slang, extremely informal language)
   2. Casual (conversational, friendly, informal)
   3. Neutral (balanced, standard business communication)
   4. Formal (professional, structured, traditional business style)
   5. Very formal (highly structured, ceremonial, extremely professional)

FORMALITY ADJUSTMENT RULES:
- ALWAYS make responses ONE LEVELS MORE FORMAL than the detected sender's tone
- If sender is casual (2), make response neutral (3)
- If sender is neutral (3), make response formal (4)
- If sender is formal (4), make response very formal (5)
- Minimum formality level is Neutral (3)
- For formal/very formal responses:
  - Remove emojis and exclamation points
  - Use complete sentences and proper business language
  - Address recipient with appropriate titles (Mr./Ms./Dr. if name known)
  - Include proper greeting and closing

RESPONSE REQUIREMENTS:
- Respond appropriately to the email content
- For complete meeting details, confirm the meeting
- For missing elements, request the specific missing information
- For high-risk content, indicate human review is needed
- Match formality level to the rules above
- Do not include placeholders like [NAME] - make reasonable assumptions

OUTPUT FORMAT:
Respond in free-form text that includes the following clearly marked sections:

ANALYSIS: 
Include completeness (e.g., "3/4 elements"), missing elements, risk factors, and detected tone.

RESPONSE:
Include your complete, formality-adjusted email response text.

RECOMMENDATION:
End with one of these keywords: standard_response, needs_review, or ignore

EMAIL TO ANALYZE:
{email_content}
"""

        return system_prompt

    async def _call_deepseek_api(self, prompt: str, request_id: str) -> str:
        """
        Call DeepSeek API with comprehensive error handling.
        
        Implements robust API calling with timeout protection,
        retry logic, and proper error handling following the
        protocols in error-handling.md.
        
        Args:
            prompt: Analysis prompt to send
            request_id: Request identifier for logging
            
        Returns:
            Raw analysis result text
            
        Raises:
            RuntimeError: If API call fails after all retries
        """
        import requests
        
        # Configure API request with timeout
        logger.debug(f"[{request_id}] Sending API request with configuration:\n"
                   f"Model: {self.model_name}\n"
                   f"Temperature: {self.temperature}\n"
                   f"Message length: {len(prompt)}")
        logger.debug(f"[{request_id}] Configured API request with timeout: {self.timeout}s")
        
        # Try API call with retries
        for attempt in range(self.retry_count + 1):
            try:
                logger.debug(f"[{request_id}] API request attempt {attempt + 1}/{self.retry_count + 1}")
                
                # Make API request
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                
                data = {
                    "model": self.model_name,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": self.temperature
                }
                
                # Send request with timeout
                start_time = time.time()
                response = requests.post(
                    f"{self.api_endpoint}/chat/completions",
                    headers=headers,
                    json=data,
                    timeout=self.timeout
                )
                connection_time = time.time() - start_time
                
                logger.debug(f"[{request_id}] API response status: {response.status_code} "
                           f"(connection time: {connection_time:.3f}s)")
                
                if response.status_code != 200:
                    raise RuntimeError(f"API returned status code {response.status_code}: {response.text}")
                
                # Process response
                response_data = response.json()
                if "choices" not in response_data or not response_data["choices"]:
                    raise ValueError("Invalid API response format")
                    
                result = response_data["choices"][0]["message"]["content"]
                
                # Calculate and log timing
                total_time = time.time() - start_time
                logger.debug(f"[{request_id}] API request successful: received {len(result)} bytes "
                           f"in {total_time:.3f}s")
                
                return result
                
            except Exception as e:
                # Log error and retry if attempts remain
                if attempt < self.retry_count:
                    logger.warning(f"[{request_id}] API request failed (attempt {attempt + 1}): {str(e)}")
                    logger.info(f"[{request_id}] Retrying in {self.retry_delay} seconds...")
                    await asyncio.sleep(self.retry_delay)
                else:
                    # Final failure
                    logger.error(f"[{request_id}] API request failed after {self.retry_count + 1} attempts: {str(e)}")
                    raise RuntimeError("API request timed out after all retry attempts")
        
        # This point should never be reached due to the raise in the loop
        raise RuntimeError("Unexpected error in API call retry logic")

    def _process_analysis_result(self, analysis: str, request_id: str) -> Tuple[Dict, str, str]:
        """
        Process unstructured analysis result from DeepSeek API.
        
        Extracts key information from the unstructured response, including
        analysis data, response text, and recommendation.
        
        Args:
            analysis: Raw analysis output from DeepSeek API
            request_id: Request identifier for logging
            
        Returns:
            Tuple containing:
            - analysis_data: Extracted analysis information
            - response_text: Generated response text
            - recommendation: Processing recommendation
        """
        logger.debug(f"[{request_id}] Processing unstructured analysis output of length: {len(analysis)}")
        
        # Initialize default values
        analysis_data = {}
        response_text = ""
        recommendation = "needs_review"  # Default to needs_review for safety
        
        # Extract analysis data
        analysis_match = re.search(r'ANALYSIS:(.*?)(?=RESPONSE:|$)', analysis, re.DOTALL | re.IGNORECASE)
        if analysis_match:
            analysis_text = analysis_match.group(1).strip()
            
            # Extract completeness
            completeness_match = re.search(r'(\d+)/4 elements', analysis_text)
            if completeness_match:
                analysis_data["completeness"] = f"{completeness_match.group(1)}/4 elements"
                
            # Extract missing elements
            missing_match = re.search(r'missing elements?:?\s*(.*?)(?=\.|$)', analysis_text, re.IGNORECASE)
            if missing_match:
                missing_elements = missing_match.group(1).strip()
                if missing_elements.lower() != "none":
                    analysis_data["missing elements"] = missing_elements
                    
            # Extract risk factors
            risk_match = re.search(r'risk factors?:?\s*(.*?)(?=\.|$)', analysis_text, re.IGNORECASE)
            if risk_match:
                risk_factors = risk_match.group(1).strip()
                analysis_data["risk factors"] = risk_factors
                
            # Extract tone
            tone_match = re.search(r'tone:?\s*(.*?)(?=\.|$)', analysis_text, re.IGNORECASE)
            if tone_match:
                detected_tone = tone_match.group(1).strip()
                analysis_data["detected tone"] = detected_tone
        
        # Extract response text
        response_match = re.search(r'RESPONSE:(.*?)(?=RECOMMENDATION:|$)', analysis, re.DOTALL | re.IGNORECASE)
        if response_match:
            response_text = response_match.group(1).strip()
            
            # Add tone information for the ResponseCategorizer
            response_tone = "friendly" 
            for level, tone_name in self.formality_levels.items():
                if tone_name.lower() in response_text.lower()[:100]:
                    response_tone = tone_name.lower()
                    break
            analysis_data["tone"] = response_tone
        
        # Extract recommendation
        recommendation_match = re.search(r'RECOMMENDATION:?\s*(.*?)(?=\.|$)', analysis, re.DOTALL | re.IGNORECASE)
        if recommendation_match:
            rec_text = recommendation_match.group(1).strip().lower()
            if "standard_response" in rec_text:
                recommendation = "standard_response"
            elif "needs_review" in rec_text:
                recommendation = "needs_review"
            elif "ignore" in rec_text:
                recommendation = "ignore"
        
        # As a final fallback, try to detect the recommendation from the entire response
        if recommendation == "needs_review":
            if "standard_response" in analysis.lower():
                recommendation = "standard_response"
            elif "ignore" in analysis.lower():
                recommendation = "ignore"
                
        return analysis_data, response_text, recommendation

    def decide_action(self, analysis_result: Any) -> str:
        """
        Determine appropriate action based on analysis results.
        
        Implements decision logic based on the recommendation
        provided by DeepSeek and additional safety checks for
        high-risk or incomplete content.
        
        Args:
            analysis_result: Analysis results from DeepSeek API
            
        Returns:
            Action string (respond, flag_for_review, ignore)
        """
        # Implementation depends on the analysis_result structure
        if hasattr(analysis_result, 'recommendation'):
            recommendation = analysis_result.recommendation
        elif isinstance(analysis_result, dict) and 'recommendation' in analysis_result:
            recommendation = analysis_result['recommendation']
        else:
            return "flag_for_review"  # Default to review if structure is unclear
        
        # Map recommendations to actions
        action_mapping = {
            "standard_response": "respond",
            "needs_review": "flag_for_review",
            "ignore": "ignore"
        }
        
        return action_mapping.get(recommendation, "flag_for_review")

================
File: src/email_processing/analyzers/llama.py
================
"""
LlamaAnalyzer: Initial Meeting Classification Service

Implements focused binary classification of emails to determine if they are
meeting-related. Acts as the first stage in the three-stage email analysis
pipeline, providing initial filtering before detailed content analysis.

Design Considerations:
- Comprehensive DEBUG level logging of all operations
- Complete input/output logging for model interactions
- Detailed error state documentation
- Processing decision tracking
"""

import logging
import json
from typing import Tuple, Dict, Optional, Any
from datetime import datetime
import traceback

from src.integrations.groq.client_wrapper import EnhancedGroqClient
from src.config.analyzer_config import ANALYZER_CONFIG

# Configure logger with proper naming
logger = logging.getLogger(__name__)

class LlamaAnalyzer:
    """
    Initial stage analyzer using Llama model for binary meeting classification.
    
    Focuses solely on determining whether an email contains meeting-related
    content, acting as the gateway for further detailed analysis in the 
    three-stage email analysis pipeline.
    
    Implements comprehensive DEBUG level logging for:
    - Complete model input/output tracking
    - Detailed error state documentation
    - Processing flow monitoring
    - Decision point logging
    """
    
    def __init__(self):
        """
        Initialize analyzer with required Groq client and configuration.
        
        Sets up the EnhancedGroqClient connection and loads model configuration
        parameters from the centralized analyzer configuration.
        """
        self.client = EnhancedGroqClient()
        self.model_config = ANALYZER_CONFIG["default_analyzer"]["model"]
        logger.debug(
            f"LlamaAnalyzer initialized with model configuration: "
            f"{json.dumps(self.model_config, indent=2)}"
        )
        
    async def classify_email(
        self,
        message_id: str,
        subject: str,
        content: str,
        sender: str
    ) -> Tuple[bool, Optional[str]]:
        """
        Determine if an email is meeting-related through binary classification.
        
        Implements the first stage of the analysis pipeline by performing
        binary classification on email content to identify meeting-related
        information. This serves as a gateway filter before more detailed
        analysis in subsequent pipeline stages.
        
        Args:
            message_id: Unique identifier for the email
            subject: Email subject line
            content: Email body content
            sender: Email sender address
            
        Returns:
            Tuple of (is_meeting: bool, error: Optional[str])
        """
        try:
            logger.info(f"Starting initial classification for email {message_id}")
            
            # Log input data at debug level with proper information masking
            logger.debug(
                f"Classification input for {message_id}:\n"
                f"Subject: {subject}\n"
                f"Sender: {self._mask_email(sender)}\n"
                f"Content length: {len(content)} characters\n"
                f"Content preview: {content[:100]}..." if len(content) > 100 else content
            )
            
            # Construct focused classification prompt
            prompt = self._construct_classification_prompt(subject, content)
            
            # Prepare messages for the API
            messages = [
                {"role": "system", "content": "You are a binary email classifier. Analyze the email and respond with EXACTLY 'meeting' or 'not_meeting'."},
                {"role": "user", "content": prompt}
            ]
            
            # Log the API request details
            logger.debug(
                f"Sending API request for {message_id} with configuration:\n"
                f"Model: {self.model_config['name']}\n"
                f"Temperature: {0.3}\n"
                f"Max tokens: {10}\n"
                f"Messages: {json.dumps(messages, indent=2)}"
            )
            
            # Process with Groq API
            start_time = datetime.now()
            response = await self.client.process_with_retry(
                messages=messages,
                model=self.model_config["name"],
                temperature=0.3,  # Low temperature for consistent binary classification
                max_completion_tokens=10  # Minimal tokens needed for binary response
            )
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # Log the complete API response
            logger.debug(
                f"API response for {message_id} (processing time: {processing_time:.3f}s):\n"
                f"{json.dumps(self._extract_response_for_logging(response), indent=2)}"
            )
            
            # Extract and normalize response
            classification = response.choices[0].message.content.strip().lower()
            is_meeting = classification == "meeting"
            
            logger.info(
                f"Completed classification for {message_id}: meeting={is_meeting} "
                f"(processing time: {processing_time:.3f}s)"
            )
            
            # Log the decision point
            logger.debug(f"Classification decision for {message_id}: {classification} → is_meeting={is_meeting}")
            
            return is_meeting, None
            
        except Exception as e:
            # Capture full error context
            error_msg = f"Classification failed: {str(e)}"
            stack_trace = traceback.format_exc()
            
            # Log comprehensive error information
            logger.error(
                f"Error classifying email {message_id}: {error_msg}\n"
                f"Stack trace: {stack_trace}"
            )
            
            # Return error state following error handling protocol
            return False, error_msg

    def _construct_classification_prompt(self, subject: str, content: str) -> str:
        """
        Construct focused prompt for binary meeting classification.
        
        Creates a prompt that emphasizes clear binary classification without
        requesting additional analysis or details. The prompt is designed
        for optimal performance with the Llama model focused on the binary
        classification task.
        
        Args:
            subject: Email subject line
            content: Email body content
            
        Returns:
            Formatted prompt string optimized for binary classification
        """
        prompt = f"""
        Determine if this email is related to a meeting, gathering, or appointment.
        
        Subject: {subject}
        
        Content:
        {content}
        
        Respond with ONLY:
        'meeting' - if the email is about scheduling, discussing, or coordinating any type of meeting
        'not_meeting' - for all other email content
        """
        
        logger.debug(f"Constructed classification prompt of length {len(prompt)}")
        return prompt
    
    def _extract_response_for_logging(self, response: Any) -> Dict[str, Any]:
        """
        Extract relevant information from the API response for logging.
        
        Creates a structured representation of the API response suitable
        for logging while handling potential serialization issues.
        
        Args:
            response: Raw API response object
            
        Returns:
            Dictionary containing structured response data
        """
        try:
            # Extract only the necessary fields for logging
            return {
                "choices": [
                    {
                        "message": {
                            "content": choice.message.content,
                            "role": choice.message.role
                        },
                        "index": choice.index,
                        "finish_reason": choice.finish_reason
                    }
                    for choice in response.choices
                ],
                "created": response.created,
                "model": response.model,
                # Add any other relevant fields that should be logged
            }
        except Exception as e:
            logger.warning(f"Error extracting response for logging: {e}")
            return {"error": "Unable to extract response data for logging"}
    
    def _mask_email(self, email: str) -> str:
        """
        Mask email addresses for privacy in logs.
        
        Implements privacy protection by masking parts of email addresses
        while preserving enough information for debugging purposes.
        
        Args:
            email: Email address to mask
            
        Returns:
            Masked email address
        """
        if not email or '@' not in email:
            return email
            
        try:
            username, domain = email.split('@', 1)
            if len(username) <= 2:
                masked_username = '*' * len(username)
            else:
                masked_username = username[0] + '*' * (len(username) - 2) + username[-1]
                
            domain_parts = domain.split('.')
            masked_domain = domain_parts[0][0] + '*' * (len(domain_parts[0]) - 1)
            
            return f"{masked_username}@{masked_domain}.{'.'.join(domain_parts[1:])}"
        except Exception:
            # If masking fails, return a generic masked value
            return "***@***.***"

================
File: src/email_processing/analyzers/meeting.py
================
import logging
from typing import Tuple, Dict, Optional
from email.message import Message
from src.email_processing.analyzers.deepseek import DeepseekAnalyzer

logger = logging.getLogger(__name__)

class MeetingEmailAnalyzer:
    def __init__(self, groq_client):
        logger.info("Initializing MeetingEmailAnalyzer")
        self.deepseek_analyzer = DeepseekAnalyzer(groq_client)

    async def analyze_meeting_email(self, message_id: str, subject: str, content: str, sender: str) -> Tuple[str, Dict]:
        """
        Analyze an email using DeepSeek R1 model to determine appropriate action.

        Args:
            message_id (str): Unique identifier for the email
            subject (str): Email subject
            content (str): Email content
            sender (str): Email sender

        Returns:
            Tuple[str, Dict]: A tuple containing the recommendation and analysis results
        """
        logger.info(f"Analyzing email {message_id} with DeepSeek R1 model")
        
        analysis_result = await self.deepseek_analyzer.analyze_email(
            email_content=content,
            subject=subject,
            sender=sender
        )

        action = self.deepseek_analyzer.decide_action(analysis_result)

        if action == "respond":
            recommendation = "needs_standard_response"
            analysis = {
                "is_meeting": True,
                "action": action,
                "response_text": analysis_result.response_text,
                "confidence": analysis_result.confidence
            }
        elif action == "flag_for_review":
            recommendation = "needs_review"
            analysis = {
                "is_meeting": True,
                "action": action,
                "confidence": analysis_result.confidence
            }
        else:
            recommendation = "needs_standard_response"
            analysis = {
                "is_meeting": False,
                "action": action,
                "confidence": analysis_result.confidence
            }

        return recommendation, analysis

================
File: src/email_processing/analyzers/response_categorizer.py
================
"""
ResponseCategorizer: Final Email Categorization Service

Implements the final stage of the email analysis pipeline, determining
appropriate handling categories based on Deepseek's detailed analysis
and generating appropriate response templates.

Design Considerations:
- Integration with enhanced structured DeepseekAnalyzer output
- Prioritization of pre-generated responses from structured analysis
- Fallback to AI-generated responses when needed
- Comprehensive parameter extraction from structured data
- Robust error handling with detailed logging
- Backward compatibility with previous pipeline versions
"""

import logging
import re
import traceback
from typing import Dict, Tuple, List, Optional, Any
from datetime import datetime
import json

from src.integrations.groq.client_wrapper import EnhancedGroqClient
from src.config.analyzer_config import ANALYZER_CONFIG

logger = logging.getLogger(__name__)

class ResponseCategorizer:
    """
    Final stage analyzer for determining email handling categories and responses.
    
    Implements the third stage of the three-stage email analysis pipeline, making
    final determinations about email handling and response generation. This component
    processes structured analysis data from DeepseekAnalyzer, extracts pre-generated
    responses, and finalizes categorization decisions.
    
    Key responsibilities:
    - Process structured analysis data from DeepseekAnalyzer
    - Prioritize pre-generated responses from structured output
    - Generate responses when needed for missing parameters
    - Make final categorization decisions for email handling
    - Implement comprehensive logging and error handling
    
    The categorizer maintains backward compatibility with previous pipeline versions
    while leveraging enhanced structured output from the updated DeepseekAnalyzer.
    """
    
    def __init__(self):
        """
        Initialize categorizer with required components.
        
        Sets up the GroqClient for fallback response generation and loads configuration
        parameters from the centralized analyzer configuration.
        """
        self.client = EnhancedGroqClient()
        self.model_config = ANALYZER_CONFIG["default_analyzer"]["model"]
        logger.debug(f"ResponseCategorizer initialized with model configuration: {self.model_config['name']}")
    
    async def categorize_email(
        self,
        analysis_data: Dict[str, Any],
        response_text: str,
        deepseek_recommendation: str,
        deepseek_summary: Optional[str] = None
    ) -> Tuple[str, Optional[str]]:
        """
        Determine final handling category and process or generate response.
        
        Processes structured analysis data from DeepseekAnalyzer to make final
        categorization decisions and handle response generation. Prioritizes
        pre-generated responses when available and falls back to generating
        responses when needed.
        
        Args:
            analysis_data: Structured analysis data from DeepseekAnalyzer
            response_text: Pre-generated response text from DeepseekAnalyzer
            deepseek_recommendation: Recommended handling category from DeepseekAnalyzer
            deepseek_summary: Optional legacy summary text for backward compatibility
            
        Returns:
            Tuple of (category: str, response_template: Optional[str])
        """
        request_id = f"respond-{datetime.now().strftime('%Y%m%d%H%M%S')}"
        try:
            logger.info(f"[{request_id}] Processing categorization with recommendation: {deepseek_recommendation}")
            logger.debug(f"[{request_id}] Analysis data: {json.dumps(analysis_data)}")
            logger.debug(f"[{request_id}] Pre-generated response text length: {len(response_text)}")
            
            # Extract missing parameters from structured analysis when available
            missing_params = self._extract_missing_parameters_structured(analysis_data)
            
            # Fall back to extracting from summary if needed
            if not missing_params and deepseek_summary:
                missing_params = self._extract_missing_parameters(deepseek_summary)
                
            logger.debug(f"[{request_id}] Extracted missing parameters: {missing_params}")
            
            # Priority logic for categorization
            if deepseek_recommendation == "ignore":
                logger.info(f"[{request_id}] Categorizing email for ignoring based on DeepseekAnalyzer recommendation")
                return "ignore", None
                
            # For meeting emails requiring review
            if deepseek_recommendation == "needs_review":
                logger.info(f"[{request_id}] Categorizing email for manual review based on DeepseekAnalyzer recommendation")
                return "needs_review", None
                
            # For standard responses
            if deepseek_recommendation == "standard_response":
                # Use pre-generated response if available
                if response_text:
                    logger.info(f"[{request_id}] Using pre-generated response from DeepseekAnalyzer")
                    return "standard_response", response_text
                
                # Handle missing parameters even if recommendation is standard_response
                if missing_params:
                    logger.info(f"[{request_id}] Found missing parameters: {missing_params}, generating parameter request")
                    response_template = await self._generate_parameter_request(
                        analysis_data, 
                        missing_params, 
                        deepseek_summary
                    )
                    return "standard_response", response_template
                
                # Fallback to generating response template
                logger.info(f"[{request_id}] Generating standard response template")
                response_template = await self._generate_response_template(analysis_data, deepseek_summary)
                return "standard_response", response_template
                
            # Default handling - treat as needs_review for safety
            logger.warning(f"[{request_id}] Unrecognized recommendation: {deepseek_recommendation}, treating as needs_review")
            return "needs_review", None
                
        except Exception as e:
            # Comprehensive error logging with stack trace
            logger.error(f"[{request_id}] Categorization failed: {str(e)}")
            logger.error(f"[{request_id}] Stack trace: {traceback.format_exc()}")
            return "needs_review", None
    
    def _extract_missing_parameters_structured(self, analysis_data: Dict[str, Any]) -> List[str]:
        """
        Extract missing parameters from structured analysis data.
        
        Processes structured analysis data to identify missing parameters that 
        can be requested from the sender. Handles various formats of structured
        data to ensure consistent parameter extraction.
        
        Args:
            analysis_data: Structured analysis data from DeepseekAnalyzer
            
        Returns:
            List of missing parameter names
        """
        missing_params = []
        
        # Check for direct missing_elements field in structured data
        if "missing_elements" in analysis_data:
            missing_elements = analysis_data["missing_elements"]
            
            # Convert to lowercase for case-insensitive matching
            missing_elements_lower = missing_elements.lower()
            
            # Map missing elements to parameter names
            param_patterns = {
                "date": ["date", "day", "when"],
                "time": ["time", "hour", "when"],
                "location": ["location", "place", "where", "venue", "meeting link", "zoom"],
                "agenda": ["agenda", "purpose", "topic", "objective"]
            }
            
            # Extract missing parameters based on patterns
            for param, patterns in param_patterns.items():
                if any(pattern in missing_elements_lower for pattern in patterns):
                    missing_params.append(param)
                    
        # Check for completeness score to infer missing elements
        elif "completeness" in analysis_data:
            try:
                # Extract completeness value and convert to int
                completeness_str = analysis_data["completeness"]
                if "/" in completeness_str:
                    completeness = int(completeness_str.split("/")[0])
                else:
                    completeness = int(completeness_str)
                    
                # If completeness is less than total, infer missing elements
                if completeness < 4:  # We expect 4 total parameters
                    # Default to requesting the most critical params if we can't determine specifics
                    missing_params = ["date", "time", "location"]
            except (ValueError, TypeError):
                logger.warning(f"Invalid completeness value in analysis_data: {analysis_data.get('completeness')}")
        
        return missing_params
            
    def _extract_missing_parameters(self, summary: str) -> List[str]:
        """
        Extract missing parameters from text summary (legacy method).
        
        Analyzes the summary text to identify parameters that can be requested
        from the sender, such as date, time, location, or agenda. Used as a
        fallback when structured data is unavailable.
        
        Args:
            summary: Detailed summary from DeepseekAnalyzer
            
        Returns:
            List of missing parameter names
        """
        if not summary:
            return []
            
        missing_params = []
        
        # Common patterns for missing parameter detection
        missing_patterns = {
            "date": ["missing date", "date is missing", "no date", "without date", "date absent", "date: absent"],
            "time": ["missing time", "time is missing", "am/pm unclear", "am/pm unspecified", "unclear time", "time: absent"],
            "location": ["missing location", "location is missing", "vague location", "unclear location", "location: absent"],
            "agenda": ["missing agenda", "purpose unclear", "no purpose", "unclear purpose", "agenda: absent"]
        }
        
        summary_lower = summary.lower()
        
        for param, patterns in missing_patterns.items():
            if any(pattern in summary_lower for pattern in patterns):
                missing_params.append(param)
                
        return missing_params

    async def _generate_response_template(self, analysis_data: Dict[str, Any], summary: Optional[str] = None) -> str:
        """
        Generate appropriate response template based on analysis data and summary.
        
        Creates either an information request for missing details or a meeting
        confirmation template based on available analysis data. Uses either
        structured data or text summary depending on availability.
        
        Args:
            analysis_data: Structured analysis data from DeepseekAnalyzer
            summary: Optional text summary for backward compatibility
            
        Returns:
            Formatted response template
        """
        try:
            # Extract sender name for personalization
            sender_name = analysis_data.get("sender_name", self._extract_sender_name(summary))
            
            # Determine tone for response
            tone = analysis_data.get("tone", "formal").lower()
            
            # Check if missing elements are specified
            missing_elements = analysis_data.get("missing_elements")
            
            # Generate appropriate greeting based on tone
            greeting = self._generate_greeting(sender_name, tone)
            
            # Generate response body based on available data
            if missing_elements:
                # Generate request for missing information
                body = (
                    f"Thank you for your meeting request. To help me properly schedule our meeting, "
                    f"could you please provide the following information: {missing_elements}?"
                )
            else:
                # Generate confirmation response
                body = (
                    "Thank you for your meeting request. I am reviewing the details "
                    "and will confirm our meeting arrangements shortly."
                )
            
            # Generate appropriate closing based on tone
            closing = "Thanks!" if tone == "friendly" else "Best regards,"
            signature = "Ivaylo's AI Assistant"
            
            # Assemble complete response
            response = f"{greeting}\n\n{body}\n\n{closing}\n{signature}"
            
            logger.debug(f"Generated response template of length: {len(response)}")
            return response
            
        except Exception as e:
            logger.error(f"Response template generation failed: {str(e)}")
            logger.error(f"Stack trace: {traceback.format_exc()}")
            return self._get_default_response_template()

    async def _generate_parameter_request(
        self, 
        analysis_data: Dict[str, Any], 
        missing_params: List[str],
        summary: Optional[str] = None
    ) -> str:
        """
        Generate a response requesting missing parameters.
        
        Creates a polite, structured response requesting specific
        missing information needed to process the meeting.
        
        Args:
            analysis_data: Structured analysis data from DeepseekAnalyzer
            missing_params: List of parameters to request
            summary: Optional text summary for backward compatibility
            
        Returns:
            Formatted response template requesting information
        """
        # Parameter descriptions for user-friendly requests
        param_descriptions = {
            "date": "the meeting date",
            "time": "the specific time (including AM/PM)",
            "location": "the exact meeting location or virtual meeting link",
            "agenda": "the meeting purpose or agenda"
        }
        
        # Format parameters for natural language inclusion
        formatted_params = [param_descriptions[param] for param in missing_params if param in param_descriptions]
        
        if len(formatted_params) == 1:
            param_text = formatted_params[0]
        elif len(formatted_params) == 2:
            param_text = f"{formatted_params[0]} and {formatted_params[1]}"
        else:
            param_text = ", ".join(formatted_params[:-1]) + f", and {formatted_params[-1]}"
        
        # Extract sender name from analysis data or summary
        sender_name = analysis_data.get("sender_name", self._extract_sender_name(summary))
        
        # Determine tone for response
        tone = analysis_data.get("tone", "formal").lower()
        
        # Generate greeting based on tone and sender information
        greeting = self._generate_greeting(sender_name, tone)
        
        # Generate appropriate closing based on tone
        closing = "Thanks!" if tone == "friendly" else "Best regards,"
        
        # Create complete response
        response_template = f"""{greeting}

Thank you for your meeting request. To help me properly schedule our meeting, could you please provide {param_text}?

{closing}
Ivaylo's AI Assistant"""

        logger.debug(f"Generated parameter request for: {missing_params}")
        return response_template

    def _generate_greeting(self, sender_name: Optional[str], tone: str) -> str:
        """
        Generate appropriate greeting based on sender name and tone.
        
        Creates a personalized greeting that matches the specified tone
        and includes the sender's name when available.
        
        Args:
            sender_name: Sender's name for personalization, if available
            tone: Communication tone (friendly or formal)
            
        Returns:
            Formatted greeting
        """
        sender_name = sender_name or "[Sender]"
        
        if tone == "friendly":
            return f"Hi {sender_name},"
        else:
            return f"Dear {sender_name},"

    def _extract_sender_name(self, summary: Optional[str]) -> Optional[str]:
        """
        Extract sender name from summary text.
        
        Attempts to identify sender information from text summary
        for personalization in responses.
        
        Args:
            summary: Text summary that might contain sender information
            
        Returns:
            Sender name if found, None otherwise
        """
        if not summary:
            return None
            
        # Look for common sender information patterns
        patterns = [
            r"sender(?:'s)?\s*(?:name|is)?:\s*([^,\n]+)",
            r"from\s*:\s*([^,\n]+)",
            r"email from\s+([^,\n.]+)"
        ]
        
        for pattern in patterns:
            match = re.search(pattern, summary, re.IGNORECASE)
            if match:
                return match.group(1).strip()
                
        return None

    def _get_default_response_template(self) -> str:
        """
        Provide a safe default response template for error cases.
        
        Returns a generalized response that can safely be used when
        specific template generation fails.
        
        Returns:
            Default response template
        """
        return """Dear Sender,

Thank you for your meeting request. To help me properly schedule our meeting, could you please provide additional details about the proposed meeting?

Best regards,
Ivaylo's AI Assistant"""

    # Legacy method signature for backward compatibility
    async def categorize_email_legacy(
        self,
        deepseek_summary: str,
        deepseek_recommendation: str
    ) -> Tuple[str, Optional[str]]:
        """
        Legacy method signature for backward compatibility.
        
        Implements the previous interface for categorizing emails
        to maintain compatibility with older code.
        
        Args:
            deepseek_summary: Detailed analysis from DeepseekAnalyzer
            deepseek_recommendation: Recommended handling category
            
        Returns:
            Tuple of (category: str, response_template: Optional[str])
        """
        # Extract minimal analysis data from summary
        analysis_data = {}
        
        # Call the new implementation with empty structured data
        return await self.categorize_email(
            analysis_data=analysis_data,
            response_text="",
            deepseek_recommendation=deepseek_recommendation,
            deepseek_summary=deepseek_summary
        )

================
File: src/email_processing/base.py
================
from typing import Dict, Tuple, Optional, Any
from dataclasses import dataclass

@dataclass
class EmailAnalysisResult:
    """
    Structured result container for email analysis operations.
    
    Provides a standardized format for analysis results while maintaining
    flexibility for different analyzer implementations. Includes core fields
    for basic analysis as well as extensible fields for detailed results.

    Attributes:
        is_meeting (bool): Indicates if the email is meeting-related
        action_required (bool): Indicates if user action is needed
        relevant_data (Dict[str, str]): Extracted key information from email
        raw_content (str): Original email content for reference
        confidence (float): Confidence score of the analysis (0.0 to 1.0)
        analysis_details (Optional[Dict]): Additional analyzer-specific results
    """
    is_meeting: bool
    action_required: bool
    relevant_data: Dict[str, str]
    raw_content: str
    confidence: float = 0.0
    analysis_details: Optional[Dict[str, Any]] = None

class BaseEmailAnalyzer:
    """
    Base analyzer class defining the contract for email analysis implementations.
    
    Provides a standardized interface for email analysis operations while allowing
    specialized implementations to maintain their unique analysis capabilities.
    Includes utility methods for content validation and result formatting.
    """
    
    async def analyze_email(self, email_content: str) -> Tuple[str, Dict[str, Any]]:
        """
        Analyze email content and determine appropriate actions.
        
        This method defines the core contract that all analyzer implementations
        must follow. Implementations should process the email content and return
        a decision along with detailed analysis results.

        Args:
            email_content: Raw content of the email to analyze
            
        Returns:
            Tuple containing:
                - str: Decision string (e.g., "standard_response", "flag_for_action")
                - Dict[str, Any]: Analysis details including explanation and metadata
        
        Raises:
            NotImplementedError: Must be implemented by concrete analyzer classes
        """
        raise NotImplementedError("Must implement analyze_email")

    def _validate_email_content(self, content: str) -> bool:
        """
        Validate email content before analysis.
        
        Performs basic validation to ensure the content is suitable for analysis.
        Implementations may extend this with additional validation logic.

        Args:
            content: Email content to validate
            
        Returns:
            bool: True if content is valid for analysis, False otherwise
        """
        if not content:
            return False
        return bool(content.strip())

    def _format_analysis_result(
        self, 
        decision: str, 
        details: Dict[str, Any], 
        confidence: float = 0.0
    ) -> Tuple[str, Dict[str, Any]]:
        """
        Format analysis results in a consistent structure.
        
        Standardizes the output format across different analyzer implementations
        while preserving all relevant analysis details and metadata.

        Args:
            decision: Analysis decision string
            details: Raw analysis details and metadata
            confidence: Confidence score for the analysis (0.0 to 1.0)
            
        Returns:
            Tuple containing formatted decision and analysis details with
            standardized structure for consistent handling downstream
        """
        return decision, {
            "explanation": details.get("explanation", ""),
            "confidence": confidence,
            "metadata": details.get("metadata", {}),
            "analysis_details": details
        }

================
File: src/email_processing/classification/classifier.py
================
import asyncio
import json
import logging
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple

from src.integrations.groq.client_wrapper import EnhancedGroqClient
from src.integrations.groq.model_manager import ModelManager
from src.email_processing.models import EmailMetadata, EmailTopic

logger = logging.getLogger(__name__)

class EmailClassifier:
    """
    Classifies emails by topic and determines if they require a response.
    Uses AI-first approach with pattern matching fallback.
    """
    
    def __init__(self):
        """Initialize classifier with model manager and pattern fallbacks."""
        self.model_manager = ModelManager()
        self.groq_client = EnhancedGroqClient()
        
        # Fallback patterns for when AI is unavailable
        self.topic_patterns: Dict[EmailTopic, List[str]] = {
            EmailTopic.MEETING: [
                "schedule meeting", "meeting request", "let's meet",
                "calendar invite", "schedule time", "schedule a call",
                "meeting availability", "when are you free",
                "zoom", "teams", "google meet"
            ]
        }
        
        self.response_required_patterns = [
            "please respond", "let me know", "confirm", "rsvp",
            "your thoughts", "what do you think", "get back to me",
            "need your input", "your feedback", "please reply",
            "can you", "would you", "?"
        ]

    def _normalize_text(self, text: Optional[str]) -> str:
        """Normalize text for pattern matching."""
        if text is None:
            return ""
        return text.lower().strip()

    def _contains_pattern(self, text: str, patterns: List[str]) -> bool:
        """Check if text contains any of the patterns."""
        normalized_text = self._normalize_text(text)
        return any(pattern in normalized_text for pattern in patterns)

    async def _determine_topic_llm(self, subject: str, content: str) -> EmailTopic:
        """
        Use LLM to determine email topic.
        Implements retry logic and fallback to pattern matching.
        """
        try:
            model_config = self.model_manager.get_model_config('email_classification')
            logger.info(f"Using model {model_config['name']} for topic classification")
            
            prompt = [
                {"role": "system", "content": "You are an email classifier. Analyze the email and determine if it's about scheduling a meeting. Respond with ONLY 'meeting' or 'unknown'."},
                {"role": "user", "content": f"Subject: {subject}\n\nContent: {content}"}
            ]
            logger.info(f"Sending prompt to model: {json.dumps(prompt, indent=2)}")
            
            start_time = time.time()
            response = await self.groq_client.process_with_retry(
                messages=prompt,
                model=model_config['name'],
                temperature=0.1,
                max_completion_tokens=10
            )
            duration = time.time() - start_time
            
            result = response.choices[0].message.content.strip().lower()
            logger.info(f"Model response received in {duration:.2f}s: {result}")
            print(f"Topic classification model response: {result}")  # Add this line
            
            return EmailTopic.MEETING if result == 'meeting' else EmailTopic.UNKNOWN
            
        except Exception as e:
            logger.error(f"LLM classification failed: {e}, falling back to pattern matching")
            return self._determine_topic_patterns(subject, content)
    
    def _determine_topic_patterns(self, subject: str, content: str) -> EmailTopic:
        """Fallback pattern-based topic determination."""
        logger.info("Falling back to pattern matching for topic determination")
        normalized_subject = self._normalize_text(subject)
        normalized_content = self._normalize_text(content)
        
        logger.info(f"Checking patterns for subject: {normalized_subject}")
        logger.info(f"Checking patterns for content: {normalized_content[:100]}...")
        
        for topic, patterns in self.topic_patterns.items():
            if self._contains_pattern(normalized_subject, patterns):
                logger.info(f"Found matching pattern in subject for topic: {topic.value}")
                return topic
            if self._contains_pattern(normalized_content, patterns):
                logger.info(f"Found matching pattern in content for topic: {topic.value}")
                return topic
        
        logger.info("No matching patterns found, returning UNKNOWN topic")
        return EmailTopic.UNKNOWN

    async def _requires_response_llm(self, subject: str, content: str) -> bool:
        """
        Use LLM to determine if email requires response.
        Implements retry logic and fallback to pattern matching.
        """
        try:
            model_config = self.model_manager.get_model_config('email_classification')
            logger.info(f"Using model {model_config['name']} for response requirement check")
            
            prompt = [
                {"role": "system", "content": "You are an email analyzer. Determine if this email requires a response. Respond with ONLY 'yes' or 'no'."},
                {"role": "user", "content": f"Subject: {subject}\n\nContent: {content}"}
            ]
            logger.info(f"Sending prompt to model: {json.dumps(prompt, indent=2)}")
            
            start_time = time.time()
            response = await self.groq_client.process_with_retry(
                messages=prompt,
                model=model_config['name'],
                temperature=0.1,
                max_completion_tokens=10
            )
            duration = time.time() - start_time
            
            result = response.choices[0].message.content.strip().lower()
            logger.info(f"Model response received in {duration:.2f}s: {result}")
            print(f"Response requirement model response: {result}")  # Add this line
            
            return result == 'yes'
            
        except Exception as e:
            logger.error(f"LLM response check failed: {e}, falling back to pattern matching")
            return self._requires_response_patterns(subject, content)
    
    def _requires_response_patterns(self, subject: str, content: str) -> bool:
        """Fallback pattern-based response check."""
        logger.info("Falling back to pattern matching for response requirement check")
        normalized_text = f"{self._normalize_text(subject)} {self._normalize_text(content)}"
        logger.info(f"Checking response patterns in combined text: {normalized_text[:100]}...")
        result = self._contains_pattern(normalized_text, self.response_required_patterns)
        logger.info(f"Response requirement determination: {result}")
        return result

    async def classify_email(self, 
                           message_id: str,
                           subject: str,
                           sender: str,
                           content: str,
                           received_at: datetime) -> EmailMetadata:
        """
        Classify an email and determine if it requires a response.
        Uses AI-first approach with pattern matching fallback.
        
        Args:
            message_id: Unique identifier for the email
            subject: Email subject line
            sender: Email sender address
            content: Email body content
            received_at: When the email was received
            
        Returns:
            EmailMetadata containing classification results
        """
        try:
            logger.info(f"Starting classification for email {message_id}")
            logger.info(f"Email details - Subject: {subject}, Sender: {sender}")
            
            start_time = time.time()
            
            # Run topic and response requirement checks in parallel
            topic, requires_response = await asyncio.gather(
                self._determine_topic_llm(subject, content),
                self._requires_response_llm(subject, content)
            )
            
            metadata = EmailMetadata(
                message_id=message_id,
                subject=subject,
                sender=sender,
                received_at=received_at,
                topic=topic,
                requires_response=requires_response,
                raw_content=content
            )
            
            duration = time.time() - start_time
            logger.info(f"Classification completed in {duration:.2f}s")
            logger.info(f"Classification result - Topic: {topic.value}, Requires Response: {requires_response}")
            
            return metadata
            
        except Exception as e:
            logger.error(f"Error classifying email {message_id}: {e}")
            # Return as unknown topic that doesn't require response
            return EmailMetadata(
                message_id=message_id,
                subject=subject,
                sender=sender,
                received_at=received_at,
                topic=EmailTopic.UNKNOWN,
                requires_response=False,
                raw_content=content
            )

class EmailRouter:
    """
    Routes classified emails to appropriate agents.
    Handles asynchronous processing and agent coordination.
    """
    
    def __init__(self):
        """Initialize router with classifier and agent registry."""
        self.classifier = EmailClassifier()
        self.agents: Dict[EmailTopic, object] = {}
        
    def register_agent(self, topic: EmailTopic, agent: object):
        """Register an agent to handle a specific email topic."""
        if not hasattr(agent, 'process_email'):
            raise ValueError(f"Agent for topic {topic.value} must implement process_email method")
            
        self.agents[topic] = agent
        logger.info(f"Registered agent for topic: {topic.value}")
        
    async def process_email(self,
                          message_id: str,
                          subject: str,
                          sender: str,
                          content: str,
                          received_at: datetime) -> Tuple[bool, Optional[str]]:
        """
        Process an email by classifying it and routing to appropriate agent.
        
        Args:
            message_id: Unique identifier for the email
            subject: Email subject line
            sender: Email sender address
            content: Email body content
            received_at: When the email was received
            
        Returns:
            Tuple of (should_mark_read: bool, error_message: Optional[str])
        """
        try:
            logger.info(f"Starting email processing for {message_id}")
            start_time = time.time()
            
            # Classify the email
            metadata = await self.classifier.classify_email(
                message_id=message_id,
                subject=subject,
                sender=sender,
                content=content,
                received_at=received_at
            )
            
            # If no response required, keep unread
            if not metadata.requires_response:
                logger.info(f"Email {message_id} does not require response, keeping unread")
                return False, None
                
            # Get the appropriate agent
            agent = self.agents.get(metadata.topic)
            if not agent:
                logger.warning(f"No agent registered for topic: {metadata.topic.value}")
                return False, f"No agent available for topic: {metadata.topic.value}"
                
            # Process with agent
            try:
                if asyncio.iscoroutinefunction(agent.process_email):
                    # Handle async agent
                    success = await agent.process_email(metadata)
                else:
                    # Handle sync agent
                    success = await asyncio.to_thread(agent.process_email, metadata)
                
                if success:
                    duration = time.time() - start_time
                    logger.info(f"Successfully processed email {message_id} with {metadata.topic.value} agent in {duration:.2f}s")
                    return True, None
                return False, f"Agent processing failed for {message_id}"
                
            except Exception as e:
                error_msg = f"Agent error processing email {message_id}: {e}"
                logger.error(error_msg)
                return False, error_msg
                
        except Exception as e:
            error_msg = f"Error routing email {message_id}: {e}"
            logger.error(error_msg)
            return False, error_msg

================
File: src/email_processing/handlers/content.py
================
from typing import Dict, Optional, List, Tuple, Set
from bs4 import BeautifulSoup
import re
import logging
from dataclasses import dataclass
from datetime import datetime
import email.utils
from zoneinfo import ZoneInfo
from src.email_processing.base import BaseEmailAnalyzer as BaseProcessor

logger = logging.getLogger(__name__)

@dataclass
class ProcessedContent:
    """
    Structured container for processed content results.
    """
    content: str
    metadata: Dict[str, any]
    token_estimate: int
    processing_stats: Dict[str, any]
    extracted_dates: Set[str] = None

class EmailDateService:
    """
    Enhanced date handling service with proper RFC 2822 and ISO 8601 support
    """
    
    @staticmethod
    def parse_email_date(date_str: str) -> Tuple[datetime, bool]:
        """
        Robust email date parsing with fallback strategies
        """
        try:
            # Try RFC 2822 parsing first
            email_tuple = email.utils.parsedate_tz(date_str)
            if email_tuple:
                timestamp = email.utils.mktime_tz(email_tuple)
                dt = datetime.fromtimestamp(timestamp, ZoneInfo("UTC"))
                return dt, True

            # Fallback to ISO 8601 parsing
            try:
                return datetime.fromisoformat(date_str.replace('Z', '+00:00')), True
            except ValueError:
                pass

            # Additional fallback formats
            for fmt in ('%a, %d %b %Y %H:%M:%S %z',
                        '%d %b %Y %H:%M:%S %z',
                        '%Y-%m-%d %H:%M:%S%z'):
                try:
                    return datetime.strptime(date_str, fmt), True
                except ValueError:
                    continue

            return None, False
        except Exception as e:
            logger.warning(f"Date parsing failed for '{date_str}': {e}")
            return None, False

    @staticmethod
    def format_iso(dt: datetime) -> str:
        """Format datetime to ISO 8601 with timezone"""
        if not dt.tzinfo:
            dt = dt.replace(tzinfo=ZoneInfo("UTC"))
        return dt.isoformat()

class DateProcessor:
    """
    Enhanced date pattern recognition and validation
    """
    
    DATE_PATTERNS = [
        r'\w{3},\s+\d{1,2}\s+\w{3}\s+\d{4}\s+\d{2}:\d{2}:\d{2}\s+(?:[+-]\d{4}|[A-Z]{3})',
        r'\d{4}-\d{2}-\d{2}(?:T\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:?\d{2})?)?',
        r'\d{1,2}/\d{1,2}/\d{4}\s+\d{1,2}:\d{2}(?::\d{2})?(?:\s*[AaPp][Mm])?',
        r'\d{1,2}-\d{1,2}-\d{4}\s+\d{1,2}:\d{2}(?::\d{2})?(?:\s*[AaPp][Mm])?',
        r'\d{1,2}:\d{2}(?::\d{2})?\s*(?:[AaPp][Mm])?',
        r'(?:today|tomorrow|next\s+(?:monday|tuesday|wednesday|thursday|friday|saturday|sunday))',
    ]

    @classmethod
    def extract_dates(cls, content: str) -> Set[str]:
        """Extract dates from content using defined patterns"""
        dates = set()
        for pattern in cls.DATE_PATTERNS:
            matches = re.finditer(pattern, content, re.IGNORECASE)
            for match in matches:
                date_str = match.group()
                dt, success = EmailDateService.parse_email_date(date_str)
                if success:
                    dates.add(EmailDateService.format_iso(dt))
                else:
                    dates.add(date_str)
        return dates

class ContentPreprocessor:
    """
    Main content preprocessing implementation with enhanced date handling
    """
    
    def __init__(self, max_tokens: int = 4000, preserve_patterns: Optional[List[str]] = None, config: Optional[Dict] = None):
        self.max_tokens = max_tokens
        self.preserve_patterns = preserve_patterns or [
            r'meeting\s+at\s+\d{1,2}(?::\d{2})?\s*(?:am|pm)?',
            r'schedule.*meeting',
            r'discuss.*at\s+\d{1,2}(?::\d{2})?',
            r'conference.*\d{1,2}(?::\d{2})?',
            r'appointment.*\d{1,2}(?::\d{2})?'
        ]
        self.config = config or {}
    
    def preprocess_content(self, content: str) -> ProcessedContent:
        """Process and structure email content"""
        processing_stats = {"original_length": len(content)}
        
        cleaned_content = self._clean_html(content)
        processing_stats["cleaned_length"] = len(cleaned_content)
        
        extracted_dates = DateProcessor.extract_dates(cleaned_content)
        processing_stats["dates_found"] = len(extracted_dates)
        
        extracted_info = self._extract_key_information(cleaned_content)
        processing_stats["extraction_success"] = bool(extracted_info)
        
        final_content = self._enforce_token_limit(extracted_info)
        processing_stats.update({
            "final_length": len(final_content),
            "estimated_tokens": len(final_content.split())
        })
        
        return ProcessedContent(
            content=final_content,
            metadata={
                "preserved_patterns": self._find_preserved_patterns(final_content),
                "date_patterns": list(extracted_dates)
            },
            token_estimate=processing_stats["estimated_tokens"],
            processing_stats=processing_stats,
            extracted_dates=extracted_dates
        )

    def _clean_html(self, content: str) -> str:
        """Clean HTML content from email body"""
        try:
            soup = BeautifulSoup(content, 'html.parser')
            
            # Remove script and style elements
            for script in soup(["script", "style"]):
                script.decompose()
                
            # Get text content
            text = soup.get_text()
            
            # Break into lines and remove leading/trailing space
            lines = (line.strip() for line in text.splitlines())
            
            # Break multi-headlines into a line each
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            
            # Drop blank lines
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            return text
            
        except Exception as e:
            logger.warning(f"HTML cleaning failed: {e}, returning original content")
            return content.strip()

    def _extract_key_information(self, content: str) -> str:
        """Extract key information from email content"""
        try:
            # Keep content manageable - limit to relevant paragraphs
            paragraphs = content.split('\n\n')
            if len(paragraphs) > self.config.get('max_paragraphs', 3):
                # Keep first paragraph, any paragraphs with preserved patterns, and last paragraph
                selected_paragraphs = [paragraphs[0]]
                
                # Check middle paragraphs for important patterns
                for paragraph in paragraphs[1:-1]:
                    if any(re.search(pattern, paragraph, re.IGNORECASE) 
                          for pattern in self.preserve_patterns):
                        selected_paragraphs.append(paragraph)
                        
                # Add last paragraph if we haven't exceeded max
                if len(selected_paragraphs) < self.config.get('max_paragraphs', 3):
                    selected_paragraphs.append(paragraphs[-1])
                    
                content = '\n\n'.join(selected_paragraphs)
            
            # Extract dates from content
            dates = DateProcessor.extract_dates(content)
            
            # Find and preserve important patterns
            preserved_content = content
            for pattern in self.preserve_patterns:
                matches = re.finditer(pattern, content, re.IGNORECASE)
                for match in matches:
                    preserved_content = preserved_content.replace(
                        match.group(), f"__PRESERVED__{match.group()}__PRESERVED__"
                    )
            
            # Clean up extra whitespace while preserving intentional line breaks
            lines = [line.strip() for line in preserved_content.splitlines()]
            cleaned_content = '\n'.join(line for line in lines if line)
            
            # Restore preserved patterns
            final_content = cleaned_content.replace('__PRESERVED__', '')
            
            # Add extracted dates to content if not already present
            if dates:
                date_section = "\n\nExtracted Dates:\n" + "\n".join(sorted(dates))
                final_content = final_content + date_section
                
            return final_content
            
        except Exception as e:
            logger.error(f"Error extracting key information: {e}")
            return content

    def _enforce_token_limit(self, content: str) -> str:
        """Enforce token limit while preserving key information"""
        if not content:
            return ""
            
        # Rough token estimation (words as proxy)
        words = content.split()
        if len(words) <= self.max_tokens:
            return content
            
        # Keep introduction and preserved patterns
        preserved_indices = set()
        for pattern in self.preserve_patterns:
            for match in re.finditer(pattern, content, re.IGNORECASE):
                start_word = len(content[:match.start()].split())
                end_word = len(content[:match.end()].split())
                preserved_indices.update(range(start_word, end_word))
                
        # Build limited content
        limited_words = []
        for i, word in enumerate(words):
            # Keep start, end, and preserved patterns
            if (i < self.max_tokens // 3 or  
                i >= len(words) - (self.max_tokens // 3) or 
                i in preserved_indices):
                limited_words.append(word)
                
        return ' '.join(limited_words)

    def _find_preserved_patterns(self, content: str) -> List[str]:
        """Find all preserved patterns in content"""
        preserved = []
        for pattern in self.preserve_patterns:
            matches = re.finditer(pattern, content, re.IGNORECASE)
            for match in matches:
                preserved.append(match.group())
        return preserved

class ContentProcessingError(Exception):
    """Custom exception for content processing errors"""
    pass

================
File: src/email_processing/handlers/date_service.py
================
"""
Comprehensive date handling service for email processing system.

This module provides robust date parsing and management capabilities,
specifically handling email date formats per RFC 2822 and other common
formats encountered in email processing.

Design Considerations:
- Robust parsing of multiple date formats
- Timezone-aware processing
- Comprehensive error handling
- Detailed logging for debugging
"""

import email.utils
import logging
from datetime import datetime, timezone
from typing import Tuple, Optional
from zoneinfo import ZoneInfo
import re
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class ParsedDate:
    """Container for parsed date results with metadata."""
    datetime: datetime
    original: str
    confidence: float
    source_format: str

class EmailDateService:
    """
    Manages date parsing and validation for email processing.
    
    Implements comprehensive date handling with support for:
    - RFC 2822 email dates
    - ISO format dates
    - Common date variations
    - Timezone handling
    """
    
    # Common date format patterns for validation
    DATE_FORMATS = [
        "%a, %d %b %Y %H:%M:%S %z",  # RFC 2822
        "%Y-%m-%dT%H:%M:%S%z",       # ISO format
        "%Y-%m-%d %H:%M:%S%z",       # Common variant
        "%Y-%m-%d %H:%M:%S"          # Simple format
    ]
    
    @classmethod
    def parse_email_date(cls, date_str: str, default_timezone: str = "UTC") -> Tuple[datetime, bool]:
        """
        Parse email date string with comprehensive format handling.
        
        Implements multi-stage parsing with fallbacks:
        1. RFC 2822 parsing
        2. ISO format attempt
        3. Common format patterns
        4. Timezone normalization
        
        Args:
            date_str: Date string to parse
            default_timezone: Timezone to use if none specified
            
        Returns:
            Tuple of (parsed datetime, success flag)
        """
        if not date_str:
            logger.warning("Empty date string provided")
            return datetime.now(ZoneInfo(default_timezone)), False
            
        try:
            # First attempt: RFC 2822 parsing
            email_tuple = email.utils.parsedate_tz(date_str)
            if email_tuple:
                timestamp = email.utils.mktime_tz(email_tuple)
                return datetime.fromtimestamp(timestamp, timezone.utc), True
                
            # Second attempt: Direct ISO parsing
            try:
                dt = datetime.fromisoformat(date_str)
                if not dt.tzinfo:
                    dt = dt.replace(tzinfo=ZoneInfo(default_timezone))
                return dt, True
            except ValueError:
                pass
                
            # Third attempt: Common formats
            for fmt in cls.DATE_FORMATS:
                try:
                    dt = datetime.strptime(date_str, fmt)
                    if not dt.tzinfo:
                        dt = dt.replace(tzinfo=ZoneInfo(default_timezone))
                    return dt, True
                except ValueError:
                    continue
                    
            # Final attempt: Extract components
            return cls._extract_date_components(date_str, default_timezone)
            
        except Exception as e:
            logger.error(f"Date parsing failed for '{date_str}': {str(e)}")
            return datetime.now(ZoneInfo(default_timezone)), False
            
    @classmethod
    def _extract_date_components(cls, date_str: str, default_timezone: str) -> Tuple[datetime, bool]:
        """
        Extract date components from non-standard format strings.
        
        Implements pattern-based extraction for dates that don't match
        standard formats but contain recognizable components.
        
        Args:
            date_str: Date string to parse
            default_timezone: Default timezone to apply
            
        Returns:
            Tuple of (datetime, success flag)
        """
        try:
            # Extract basic date components using regex
            date_pattern = r"(\d{1,2})\s*(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s*(\d{4})"
            time_pattern = r"(\d{1,2}):(\d{2})(?::(\d{2}))?"
            zone_pattern = r"([+-]\d{4}|[A-Z]{3})"
            
            date_match = re.search(date_pattern, date_str, re.IGNORECASE)
            time_match = re.search(time_pattern, date_str)
            zone_match = re.search(zone_pattern, date_str)
            
            if not date_match:
                return datetime.now(ZoneInfo(default_timezone)), False
                
            # Parse date components
            day = int(date_match.group(1))
            month = cls._month_to_number(date_match.group(2))
            year = int(date_match.group(3))
            
            # Parse time components
            hour = minute = second = 0
            if time_match:
                hour = int(time_match.group(1))
                minute = int(time_match.group(2))
                second = int(time_match.group(3)) if time_match.group(3) else 0
                
            # Create datetime with timezone
            dt = datetime(year, month, day, hour, minute, second)
            if zone_match:
                try:
                    if len(zone_match.group(1)) == 5:  # +HHMM format
                        offset = int(zone_match.group(1)[:3]) * 3600 + int(zone_match.group(1)[3:]) * 60
                        dt = dt.replace(tzinfo=timezone(timedelta(seconds=offset)))
                    else:  # Timezone abbreviation
                        dt = dt.replace(tzinfo=ZoneInfo(default_timezone))
                except Exception:
                    dt = dt.replace(tzinfo=ZoneInfo(default_timezone))
            else:
                dt = dt.replace(tzinfo=ZoneInfo(default_timezone))
                
            return dt, True
            
        except Exception as e:
            logger.error(f"Component extraction failed for '{date_str}': {str(e)}")
            return datetime.now(ZoneInfo(default_timezone)), False
            
    @staticmethod
    def _month_to_number(month: str) -> int:
        """Convert month name to number."""
        months = {
            'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,
            'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,
            'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12
        }
        return months[month.lower()[:3]]
        
    @classmethod
    def format_iso(cls, dt: datetime) -> str:
        """
        Format datetime as ISO string with timezone.
        
        Ensures consistent ISO format output while maintaining
        timezone information for accurate timestamp representation.
        
        Args:
            dt: Datetime object to format
            
        Returns:
            ISO formatted datetime string
        """
        if not dt.tzinfo:
            dt = dt.replace(tzinfo=ZoneInfo("UTC"))
        return dt.isoformat()
        
    @classmethod
    def is_valid_date(cls, date_str: str) -> bool:
        """
        Validate date string format.
        
        Implements comprehensive validation for supported date formats.
        
        Args:
            date_str: Date string to validate
            
        Returns:
            Boolean indicating if date string is valid
        """
        try:
            parsed_date, success = cls.parse_email_date(date_str)
            return success
        except Exception:
            return False

================
File: src/email_processing/handlers/sorter.py
================
import os
from datetime import datetime, timedelta
import email.utils
import json
import logging
from pathlib import Path
from typing import Dict, Optional

from dotenv import load_dotenv
from integrations.groq.client_wrapper import EnhancedGroqClient

# Set up logging to track what's happening in our application
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/mail_sorter.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class MeetingSorter:
    """A class that processes emails to identify and extract meeting-related information."""

    def __init__(self):
        """Initialize the MeetingSorter with necessary configurations and clients."""
        # Load environment variables for configuration
        load_dotenv(override=True)

        # Initialize our enhanced Groq client for AI processing
        self.groq_client = EnhancedGroqClient()

        # Set up file paths and ensure directories exist
        self.json_file = "data/cache/meeting_mails.json"
        Path(os.path.dirname(self.json_file)).mkdir(parents=True, exist_ok=True)

    def parse_email_content(self, raw_content: str) -> dict:
        """Parse raw email content into a structured format we can work with."""
        email_data = {"headers": {}, "body": ""}

        # Skip metadata lines and process the actual content
        content_lines = raw_content.split('\n')[2:]
        current_section = "headers"

        for line in content_lines:
            if line.startswith('Body:'):
                current_section = "body"
                continue
            elif line.strip() == '' or line.startswith('----'):
                continue

            if current_section == "headers":
                if ': ' in line:
                    key, value = line.split(': ', 1)
                    email_data["headers"][key] = value.strip()
            else:
                email_data["body"] += line + "\n"

        return email_data

    async def extract_meeting_details(self, emails_content: str) -> str:
        """Extract meeting information from email content using AI processing."""
        try:
            # First parse the email into a structured format
            email_data = self.parse_email_content(emails_content)

            # Extract and format sender information
            from_header = email_data['headers'].get('From', '')
            sender_name = from_header.split(' <')[0] if ' <' in from_header else from_header
            sender_email = from_header.split('<')[-1].rstrip('>') if '<' in from_header else from_header

            # Format the content for our AI model to process
            formatted_content = f"""
From: {from_header}
Subject: {email_data['headers'].get('Subject', '')}
Date: {email_data['headers'].get('Date', '')}
Content: {email_data['body']}
"""

            # Define our AI system prompt for meeting detection
            system_prompt = """You are an expert email analyzer specialized in meeting request identification.
            Analyze the email and extract meeting information in the following JSON format:
            {
                "found_meetings": boolean,
                "meetings": [{
                    "date": "YYYY-MM-DD",
                    "time": "HH:MM",
                    "topic": "string",
                    "location": "string or null",
                    "sender_name": "string",
                    "sender_email": "string"
                }]
            }

            Follow these rules:
            - Convert all relative dates to YYYY-MM-DD format
            - Use 24-hour time format (HH:MM)
            - Include both physical and virtual meeting locations
            - Set found_meetings to false if time information is ambiguous"""

            # Process with our enhanced Groq client
            response = await self.groq_client.process_with_retry(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": formatted_content}
                ],
                temperature=0.7,
                response_format={"type": "json_object"}
            )

            # Extract and process the response
            content = response.choices[0].message.content.strip()
            logger.debug(f"Groq API Response: {content}")

            # Parse and validate the JSON response
            parsed_json = json.loads(content)

            # Process any relative dates in the response
            if parsed_json.get("found_meetings", False):
                email_date = email.utils.parsedate_to_datetime(
                    email_data['headers'].get('Date', '')
                )
                parsed_json = self._process_relative_dates(parsed_json, email_date)

            return json.dumps(parsed_json, indent=2)

        except Exception as e:
            logger.error(f"Error extracting meeting details: {str(e)}", exc_info=True)
            return json.dumps({
                "found_meetings": False,
                "meetings": [],
                "error": str(e)
            }, indent=2)

    def _process_relative_dates(self, data: Dict, reference_date: datetime) -> Dict:
        """Convert relative dates to absolute dates based on the email's date."""
        date_keywords = {
            "tomorrow": timedelta(days=1),
            "next week": timedelta(days=7),
            "day after tomorrow": timedelta(days=2),
            "next month": timedelta(days=30),
        }

        for meeting in data.get("meetings", []):
            if meeting.get("date"):
                date_str = meeting["date"].lower()

                # Handle relative date expressions
                for keyword, delta in date_keywords.items():
                    if keyword in date_str:
                        new_date = reference_date + delta
                        meeting["date"] = new_date.strftime("%Y-%m-%d")
                        break

        return data

    async def process_emails(self, email_file_path: str) -> str:
        """Main method to process emails and extract meeting information."""
        try:
            # Try to read the email file with UTF-8 encoding first
            emails_content = self._read_email_file(email_file_path)

            # Extract and process meeting details
            json_response = await self.extract_meeting_details(emails_content)

            # Save results and return formatted output
            self.save_to_json(json_response)
            return self.format_results(json_response)

        except Exception as e:
            logger.error(f"Error processing emails: {str(e)}", exc_info=True)
            return f"Error processing emails: {str(e)}"

    def _read_email_file(self, file_path: str) -> str:
        """Read email file with encoding fallback."""
        encodings = ['utf-8', 'latin-1', 'ascii']

        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as file:
                    return file.read()
            except UnicodeDecodeError:
                continue

        raise ValueError(f"Could not read file with any of the encodings: {encodings}")

    def format_results(self, json_response: str) -> str:
        """Format the JSON response into a human-readable string."""
        try:
            data = json.loads(json_response)
        except json.JSONDecodeError:
            return "Error: Could not parse meeting data"

        if not data.get("found_meetings", False):
            return "No meeting emails found."

        output = "Meeting-related emails found:\n\n"
        for meeting in data.get("meetings", []):
            output += f"Date: {meeting.get('date', 'Not specified')}\n"
            output += f"Time: {meeting.get('time', 'Not specified')}\n"
            output += f"Topic: {meeting.get('topic', 'Not specified')}\n"
            output += f"From: {meeting.get('sender_name', 'Unknown')} "
            output += f"<{meeting.get('sender_email', 'unknown')}>\n"
            if meeting.get('location'):
                output += f"Location: {meeting['location']}\n"
            output += "-" * 50 + "\n"

        return output

    def save_to_json(self, json_response: str) -> None:
        """Save the extracted meeting information to a JSON file with deduplication."""
        try:
            data = json.loads(json_response)
            current_time = datetime.now().isoformat()

            # Load or initialize stored data
            try:
                with open(self.json_file, 'r') as f:
                    stored_data = json.load(f)
            except (FileNotFoundError, json.JSONDecodeError):
                stored_data = {
                    "last_updated": current_time,
                    "meetings": []
                }

            # Add new meetings only if they don't exist
            if data.get("found_meetings", False):
                for new_meeting in data.get("meetings", []):
                    if not self._is_duplicate_meeting(stored_data, new_meeting):
                        stored_data["meetings"].append({
                            "date": new_meeting.get("date"),
                            "time": new_meeting.get("time"),
                            "topic": new_meeting.get("topic"),
                            "sender": {
                                "name": new_meeting.get("sender_name"),
                                "email": new_meeting.get("sender_email")
                            },
                            "location": new_meeting.get("location"),
                            "added_on": current_time
                        })

            stored_data["last_updated"] = current_time

            # Save updated data
            with open(self.json_file, 'w') as f:
                json.dump(stored_data, f, indent=2, sort_keys=True, ensure_ascii=False)

        except Exception as e:
            logger.error(f"Error saving to JSON: {str(e)}", exc_info=True)

    def _is_duplicate_meeting(self, stored_data: Dict, new_meeting: Dict) -> bool:
        """Check if a meeting already exists in stored data."""
        return any(
            existing_meeting.get("date") == new_meeting.get("date") and
            existing_meeting.get("time") == new_meeting.get("time") and
            existing_meeting.get("topic", "").lower() == new_meeting.get("topic", "").lower() and
            existing_meeting.get("sender", {}).get("email") == new_meeting.get("sender_email")
            for existing_meeting in stored_data.get("meetings", [])
        )

================
File: src/email_processing/handlers/writer.py
================
import base64
import json
import logging
import os
import re
from datetime import datetime
from email.mime.text import MIMEText
from typing import Dict, List, Optional, Tuple

from dotenv import load_dotenv
from openai import OpenAI

from src.email_processing.models import EmailMetadata
from src.integrations.gmail.client import GmailClient
from src.integrations.groq.client_wrapper import EnhancedGroqClient

logger = logging.getLogger(__name__)
load_dotenv(override=True)

class EmailAgent:
    """Agent for processing and responding to emails with AI-first approach."""
    
    def __init__(self):
        """Initialize the email agent with necessary clients and configurations."""
        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.gmail = GmailClient()
        self.groq_client = EnhancedGroqClient()
        self.load_response_log()
        
    def load_response_log(self):
        """Load or create the response log that tracks all email responses."""
        try:
            with open('data/email_responses.json', 'r') as f:
                content = f.read().strip()
                if content:
                    self.response_log = json.load(f)
                else:
                    self.response_log = {"responses": []}
        except (FileNotFoundError, json.JSONDecodeError):
            self.response_log = {"responses": []}
            os.makedirs('data', exist_ok=True)
            with open('data/email_responses.json', 'w') as f:
                json.dump(self.response_log, f, indent=2)

    def save_response_log(self, email_id: str, response_data: Dict):
        """Save a new response to the log with timestamp."""
        self.response_log["responses"].append({
            "email_id": email_id,
            "response_time": datetime.now().isoformat(),
            "response_data": response_data
        })
        with open('data/email_responses.json', 'w') as f:
            json.dump(self.response_log, f, indent=2)

    def has_responded(self, email_id: str) -> bool:
        """Check if we've already responded to this email."""
        return any(r["email_id"] == email_id for r in self.response_log["responses"])

    async def verify_meeting_parameters_ai(self, content: str, subject: str) -> Tuple[Dict, bool]:
        """
        Use Groq AI to verify meeting parameters and identify missing information.
        
        Args:
            content: Email content to analyze
            subject: Email subject line
            
        Returns:
            Tuple of (parameter_analysis, success_flag)
        """
        try:
            # Detailed system prompt for parameter verification
            system_prompt = """You are an expert meeting coordinator. Analyze the email content 
            and identify if all required meeting parameters are provided. Extract and verify:
            1. Meeting date (must be a specific date)
            2. Meeting time (must be a specific hour)
            3. Meeting location (physical location or virtual meeting link)
            4. Meeting agenda/purpose

            Respond in JSON format:
            {
                "parameters": {
                    "date": {"found": boolean, "value": string or null, "confidence": float},
                    "time": {"found": boolean, "value": string or null, "confidence": float},
                    "location": {"found": boolean, "value": string or null, "confidence": float},
                    "agenda": {"found": boolean, "value": string or null, "confidence": float}
                },
                "missing_parameters": [string],
                "has_all_required": boolean,
                "overall_confidence": float
            }
            
            For confidence scores:
            - 1.0: Explicitly stated and clear
            - 0.7-0.9: Strongly implied or contextually clear
            - 0.4-0.6: Somewhat unclear or ambiguous
            - <0.4: Very uncertain or likely missing"""

            # Process with Groq AI
            response = await self.groq_client.process_with_retry(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Subject: {subject}\n\nContent: {content}"}
                ],
                model="llama-3.3-70b-versatile",
                temperature=0.3,
                response_format={"type": "json_object"}
            )

            result = json.loads(response.choices[0].message.content)
            return result, True

        except Exception as e:
            logger.error(f"AI parameter verification failed: {e}")
            return self._fallback_parameter_check(content), False

    def _fallback_parameter_check(self, content: str) -> Dict:
        """
        Fallback method using pattern matching to check meeting parameters.
        Used when AI verification fails.
        """
        info = self.extract_meeting_info(content)
        
        # Default confidence for pattern matching is 0.6
        return {
            "parameters": {
                "date": {"found": False, "value": None, "confidence": 0.0},
                "time": {"found": False, "value": None, "confidence": 0.0},
                "location": {
                    "found": bool(info['location']),
                    "value": info['location'],
                    "confidence": 0.6 if info['location'] else 0.0
                },
                "agenda": {
                    "found": bool(info['agenda']),
                    "value": info['agenda'],
                    "confidence": 0.6 if info['agenda'] else 0.0
                }
            },
            "missing_parameters": [
                param for param, details in {
                    "date": True,
                    "time": True,
                    "location": not info['location'],
                    "agenda": not info['agenda']
                }.items() if details
            ],
            "has_all_required": False,
            "overall_confidence": 0.6 if (info['location'] and info['agenda']) else 0.3
        }

    def extract_meeting_info(self, content: str) -> Dict[str, Optional[str]]:
        """Extract meeting information using pattern matching (fallback method)."""
        info = {
            'location': None,
            'agenda': None
        }
        
        location_patterns = [
            r'at\s+([^\.!?\n]+)',
            r'in\s+([^\.!?\n]+)',
            r'location:\s*([^\.!?\n]+)',
            r'meet\s+(?:at|in)\s+([^\.!?\n]+)'
        ]

        agenda_patterns = [
            r'(?:about|discuss|regarding|re:|topic:|agenda:)\s+([^\.!?\n]+)',
            r'for\s+([^\.!?\n]+\s+(?:meeting|discussion|sync|catch-up))',
            r'purpose:\s*([^\.!?\n]+)'
        ]

        for pattern in location_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                info['location'] = match.group(1).strip()
                break

        for pattern in agenda_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                info['agenda'] = match.group(1).strip()
                break

        return info

    def _format_missing_parameters(self, missing_params: List[str]) -> str:
        """Format the list of missing parameters into a natural language request."""
        if not missing_params:
            return ""

        parameter_names = {
            "date": "the meeting date",
            "time": "the meeting time",
            "location": "the meeting location",
            "agenda": "the meeting agenda/purpose"
        }

        readable_params = [parameter_names.get(param, param) for param in missing_params]

        if len(readable_params) == 1:
            return readable_params[0]
        elif len(readable_params) == 2:
            return f"{readable_params[0]} and {readable_params[1]}"
        else:
            readable_params[-1] = f"and {readable_params[-1]}"
            return ", ".join(readable_params)

    async def create_response(self, metadata: EmailMetadata) -> Optional[str]:
        """
        Create an appropriate response based on email metadata and AI analysis.
        Uses AI-first approach with pattern matching as fallback.
        """
        try:
            sender_name = metadata.sender.split('<')[0].strip()
            if not sender_name:
                sender_name = metadata.sender

            # Verify parameters using AI
            ai_analysis, ai_success = await self.verify_meeting_parameters_ai(
                metadata.raw_content, 
                metadata.subject
            )

            # Log the analysis results
            logger.info(f"Parameter analysis for {metadata.message_id}: {json.dumps(ai_analysis)}")

            if not ai_analysis["has_all_required"]:
                missing_info = self._format_missing_parameters(ai_analysis["missing_parameters"])
                
                return f"""Dear {sender_name},

Thank you for your meeting request. To help me properly schedule our meeting, could you please specify {missing_info}?

Best regards,
Ivaylo's AI Assistant"""
            parameter_names = {
                "date": "the meeting date",
                "time": "the meeting time",
                "location": "the meeting location",
                "agenda": "the meeting agenda/purpose"
            }
                        # If all parameters are present, verify confidence
            params = ai_analysis["parameters"]
            if ai_analysis["overall_confidence"] < 0.7:
                # Ask for confirmation when confidence is low
                verification_text = []
                for param, details in params.items():
                    if details["confidence"] < 0.7:
                        verification_text.append(f"{parameter_names[param]} ({details['value']})")
                
                verify_items = self._format_missing_parameters(verification_text)
                return f"""Dear {sender_name},

Thank you for your meeting request. Could you please confirm {verify_items}?

Best regards,
Ivaylo's AI Assistant"""

            # Create confirmation with all parameters
            return f"""Dear {sender_name},

Thank you for your meeting request. I am pleased to confirm our meeting on {params['date']['value']} at {params['time']['value']} at {params['location']['value']} to discuss {params['agenda']['value']}.

Best regards,
Ivaylo's AI Assistant"""

        except Exception as e:
            logger.error(f"Error in AI-based response creation: {e}")
            # Fallback to pattern-based response creation
            response = self._create_fallback_response(metadata)
            if response:
                logger.info(f"Successfully created fallback response for {metadata.message_id}")
            return response

    def send_email(self, to_email: str, subject: str, message_text: str) -> bool:
        """Send an email using Gmail API."""
        message = MIMEText(message_text)
        message['to'] = to_email
        message['subject'] = subject

        raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode('utf-8')

        try:
            self.gmail.service.users().messages().send(
                userId="me",
                body={'raw': raw_message}
            ).execute()
            return True
        except Exception as e:
            logger.error(f"Error sending email: {e}")
            return False

    async def process_email(self, metadata: EmailMetadata) -> bool:
    # """
    # Process an email and send appropriate response.
    # This method is called by the EmailProcessor.
    
    # Implements comprehensive email processing with:
    # - Duplicate detection
    # - Response generation (or using provided template)
    # - Email sending
    # - Response logging
    
    # Args:
    #     metadata: Comprehensive email metadata including analysis results
        
    # Returns:
    #     bool: True if email successfully processed and responded to
    # """
        try:
            if self.has_responded(metadata.message_id):
                logger.info(f"Already responded to email {metadata.message_id}")
                return True

            # Check if a pre-generated response template is provided
            if metadata.analysis_data and 'response_template' in metadata.analysis_data:
                response_text = metadata.analysis_data['response_template']
                logger.info(f"Using provided response template for {metadata.message_id}")
            else:
                # Generate response if not provided
                response_text = await self.create_response(metadata)
                
            if not response_text:
                logger.error("Failed to create response")
                return False

            subject = metadata.subject if metadata.subject else "Meeting Request"
            subject_prefix = "Re: " if not subject.startswith("Re:") else ""
            full_subject = f"{subject_prefix}{subject}"

            success = self.send_email(
                to_email=metadata.sender,
                subject=full_subject,
                message_text=response_text
            )

            if success:
                self.save_response_log(metadata.message_id, {
                    "sender": metadata.sender,
                    "subject": metadata.subject,
                    "response": response_text
                })
                return True

            return False

        except Exception as e:
            logger.error(f"Error processing email {metadata.message_id}: {e}")
            return False

================
File: src/email_processing/models.py
================
"""
Shared data models for email processing.
"""

from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Dict, Optional

class EmailTopic(Enum):
    """Supported email topics for classification."""
    MEETING = "meeting"
    UNKNOWN = "unknown"
    # Add new topics here as more agents are introduced
    # Example: TASK = "task"
    # Example: REPORT = "report"

@dataclass
class EmailMetadata:
    """Metadata about a processed email."""
    message_id: str
    subject: str
    sender: str
    received_at: datetime
    topic: EmailTopic
    requires_response: bool
    raw_content: str
    analysis_data: Optional[Dict] = None

================
File: src/email_processing/processor.py
================
"""
Enhanced Email Processing Implementation

Implements a sophisticated three-stage email analysis pipeline with comprehensive
error handling, logging, and state management. Coordinates between specialized
analyzers while maintaining proper processing state and data integrity.
"""

import logging
import asyncio
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from src.email_processing.models import EmailMetadata, EmailTopic
from src.email_processing.classification.classifier import EmailRouter
from src.email_processing.handlers.content import ContentPreprocessor
from src.email_processing.handlers.date_service import EmailDateService
from src.email_processing.analyzers.llama import LlamaAnalyzer
from src.email_processing.analyzers.deepseek import DeepseekAnalyzer
from src.email_processing.analyzers.response_categorizer import ResponseCategorizer
from src.integrations.gmail.client import GmailClient
from src.storage.secure import SecureStorage
from src.email_processing.handlers.writer import EmailAgent

logger = logging.getLogger(__name__)

class EmailProcessor:
    """
    Sophisticated email processor implementing three-stage analysis pipeline.
    
    Coordinates between specialized analyzers for comprehensive email understanding:
    1. Initial meeting classification (LlamaAnalyzer)
    2. Detailed content analysis (DeepseekAnalyzer)
    3. Final categorization and response generation (ResponseCategorizer)
    
    Implements robust error handling, comprehensive logging, and proper state
    management throughout the processing pipeline.
    """
    
    def __init__(self, 
                 gmail_client: GmailClient,
                 llama_analyzer: LlamaAnalyzer,
                 deepseek_analyzer: DeepseekAnalyzer,
                 response_categorizer: ResponseCategorizer,
                 storage_path: str = "data/secure",
                 strict_date_parsing: bool = True):
        """
        Initialize the email processor with required components.
        
        Args:
            gmail_client: Client for Gmail API interactions
            llama_analyzer: Initial meeting classification analyzer
            deepseek_analyzer: Detailed content analysis component
            response_categorizer: Final categorization and response generator
            storage_path: Path for secure storage of processing records
            strict_date_parsing: Whether to enforce strict date validation
        """
        self.gmail = gmail_client
        self.llama_analyzer = llama_analyzer
        self.deepseek_analyzer = deepseek_analyzer
        self.response_categorizer = response_categorizer
        self.storage = SecureStorage(storage_path)
        self.content_processor = ContentPreprocessor()
        self.strict_date_parsing = strict_date_parsing
        self.agents = {}

        # Ensure storage directory exists
        Path(storage_path).mkdir(parents=True, exist_ok=True)
        logger.info("EmailProcessor initialized successfully")

    def register_agent(self, topic: EmailTopic, agent: any) -> None:
        """
        Register an agent to handle specific email topics.
        
        Implements agent registration for specialized email handling.
        Each agent is mapped to a specific EmailTopic and must implement
        the required interface for email processing.
        
        Args:
            topic: EmailTopic enum value indicating handled email type
            agent: Agent instance implementing required processing interface
            
        Raises:
            ValueError: If agent doesn't implement required methods
        """
        if not hasattr(agent, 'process_email'):
            raise ValueError(f"Agent for topic {topic.value} must implement process_email method")
            
        self.agents[topic] = agent
        logger.info(f"Registered agent for topic: {topic.value}")

    async def _process_single_email(self, email: Dict) -> Tuple[bool, Optional[str]]:
        """
        Process a single email through the three-stage analysis pipeline.
        
        Implements comprehensive email analysis through coordinated stages:
        1. Initial meeting classification using LlamaAnalyzer
        2. Detailed content analysis using DeepseekAnalyzer for meeting emails
        3. Final categorization and response generation using ResponseCategorizer
        
        Maintains proper error handling and state management throughout the
        pipeline, ensuring reliable processing and appropriate response generation.
        
        Args:
            email: Dictionary containing email data and metadata
            
        Returns:
            Tuple containing (success_flag: bool, error_message: Optional[str])
        """
        message_id = email.get("message_id")
        try:
            logger.info(f"Starting pipeline processing for email {message_id}")
            
            # Stage 1: Initial Classification with LlamaAnalyzer
            is_meeting, llama_error = await self.llama_analyzer.classify_email(
                message_id=message_id,
                subject=email.get("subject", ""),
                content=email.get("processed_content", ""),
                sender=email.get("sender", "")
            )
            
            if llama_error:
                logger.error(f"LlamaAnalyzer error: {llama_error}")
                return False, f"Initial classification failed: {llama_error}"
                
            if not is_meeting:
                logger.info(f"Email {message_id} classified as non-meeting")
                await self.storage.add_record(email)
                return True, None
                
            # Stage 2: Detailed Analysis with DeepseekAnalyzer
            analysis_data, response_text, recommendation, deepseek_error = await self.deepseek_analyzer.analyze_email(
                email_content=email.get("processed_content", "")
            )
            
            if deepseek_error:
                logger.error(f"DeepseekAnalyzer error: {deepseek_error}")
                return False, f"Detailed analysis failed: {deepseek_error}"
                
            # Stage 3: Final Categorization and Response Generation
            category, response_template = await self.response_categorizer.categorize_email(
                analysis_data=analysis_data,
                response_text=response_text,
                deepseek_recommendation=recommendation,
                deepseek_summary=analysis_data.get("summary", "")
            )
            
            # Update email metadata with processing results
            email["analysis_results"] = {
                "is_meeting": is_meeting,
                "analysis_data": analysis_data,
                "response_text": response_text,
                "deepseek_recommendation": recommendation,
                "final_category": category,
                "processed_at": datetime.now().isoformat(),
                "requires_response": response_template is not None
            }
            
            # Handle email based on categorization
            await self._handle_categorized_email(
                message_id=message_id,
                category=category,
                response_template=response_template,
                email_data=email
            )
            
            logger.info(f"Successfully processed email {message_id} as category: {category}")
            return True, None
            
        except Exception as e:
            error_msg = f"Pipeline error for {message_id}: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return False, error_msg

    async def _handle_categorized_email(
    self,
    message_id: str,
    category: str,
    response_template: Optional[str],
    email_data: Dict
        ) -> None:
        
                # Handle email based on its final categorization.
                
                # Implements appropriate actions for each category:
                # - standard_response: Generate response via EmailAgent and mark as read
                # - needs_review: Keep unread for manual review
                # - ignore: Mark as read with no further action
                
                # Args:
                #     message_id: Unique identifier for the email
                #     category: Final categorization result
                #     response_template: Generated response text if applicable
                #     email_data: Complete email data dictionary
                # """
        try:
            # Register with agent in case it isn't already
            if EmailTopic.MEETING not in self.agents:
                email_agent = EmailAgent()
                self.register_agent(EmailTopic.MEETING, email_agent)
            
            email_agent = self.agents.get(EmailTopic.MEETING)
            
            if category == "standard_response" and response_template:
                # Create email metadata for the agent
                metadata = EmailMetadata(
                    message_id=message_id,
                    subject=email_data.get('subject', ''),
                    sender=email_data.get('sender', ''),
                    received_at=datetime.now(),
                    topic=EmailTopic.MEETING,
                    requires_response=True,
                    raw_content=email_data.get('content', ''),
                    analysis_data={
                        'response_template': response_template
                    }
                )
                
                # Process with the email agent to send response
                success = await email_agent.process_email(metadata)
                
                if success:
                    self.gmail.mark_as_read(message_id)
                    logger.info(f"Sent response and marked email {message_id} as read")
                else:
                    logger.error(f"Failed to send response for email {message_id}")
                    
            elif category == "needs_review":
                # Keep email unread for manual review
                self.gmail.mark_as_unread(message_id)
                logger.info(f"Marked email {message_id} for review")
                
            else:  # ignore
                self.gmail.mark_as_read(message_id)
                logger.info(f"Marked email {message_id} as read (ignored)")
                
            # Store processing record
            await self.storage.add_record(email_data)
            
        except Exception as e:
            logger.error(f"Error handling categorized email {message_id}: {e}")
            raise

    async def process_email_batch(self, batch_size: int = 100) -> Tuple[int, int, List[str]]:
        """
        Process a batch of emails through the analysis pipeline.
        
        Implements batch processing with proper error handling and state tracking.
        Maintains processing history and prevents duplicate processing.
        
        Args:
            batch_size: Maximum number of emails to process in this batch
            
        Returns:
            Tuple containing (processed_count, error_count, error_messages)
        """
        processed_count = 0
        error_count = 0
        error_messages = []
        
        try:
            # Fetch unread emails
            unread_emails = await asyncio.to_thread(
                self.gmail.get_unread_emails,
                max_results=batch_size
            )
            logger.info(f"Found {len(unread_emails)} unread emails")
            
            # Process each email through the pipeline
            for email in unread_emails:
                message_id = email.get("message_id")
                
                try:
                    # Check processing history
                    is_processed, check_success = await self.storage.is_processed(message_id)
                    if not check_success:
                        error_msg = f"Failed to check processing status for {message_id}"
                        logger.error(error_msg)
                        error_count += 1
                        error_messages.append(error_msg)
                        continue
                        
                    if is_processed:
                        logger.info(f"Email {message_id} already processed, skipping")
                        continue
                        
                    # Process content
                    processed_content = self.content_processor.preprocess_content(
                        email.get("content", "")
                    )
                    email["processed_content"] = processed_content.content
                    
                    # Process through pipeline
                    success, error = await self._process_single_email(email)
                    if success:
                        processed_count += 1
                    else:
                        error_count += 1
                        error_messages.append(error)
                        
                except Exception as e:
                    error_msg = f"Error processing email {message_id}: {str(e)}"
                    logger.error(error_msg)
                    error_count += 1
                    error_messages.append(error_msg)
                    
            logger.info(f"Completed batch processing: {processed_count} processed, {error_count} errors")
            return processed_count, error_count, error_messages
            
        except Exception as e:
            error_msg = f"Batch processing error: {str(e)}"
            logger.error(error_msg)
            return 0, 1, [error_msg]

================
File: src/integrations/__init__.py
================
from .gmail.client import GmailClient
from .gmail.auth_manager import GmailAuthenticationManager
from .groq.client import EnhancedGroqClient
from .groq.model_manager import ModelManager

__all__ = [
    'GmailClient',
    'GmailAuthenticationManager',
    'EnhancedGroqClient',
    'ModelManager',
]

================
File: src/integrations/gmail/auth_manager.py
================
"""
Enhanced Gmail Authentication Manager with Web OAuth Flow Support

Implements robust OAuth2 authentication management for both web and installed
application flows, with comprehensive token handling, automatic refresh, and 
secure storage following system specifications.

Key Features:
- Support for both web OAuth flow and installed application flow
- Secure token storage and management
- Automatic token refresh with retry logic
- Comprehensive error handling and logging
- Integration with email processing pipeline
"""

import os
import json
import logging
import time
from pathlib import Path
from typing import Optional, Tuple, Dict, Any
from datetime import datetime, timedelta

from google.oauth2.credentials import Credentials
from google.auth.transport.requests import Request
from google.auth.exceptions import RefreshError
from google_auth_oauthlib.flow import InstalledAppFlow, Flow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

logger = logging.getLogger(__name__)

class GmailAuthenticationManager:
    """
    Manages Gmail OAuth2 authentication with comprehensive token handling
    and secure storage mechanisms, supporting both web and installed flows.
    """
    
    def __init__(self, 
                 token_path: str = 'token.json',
                 credentials_path: str = 'client_secret.json',
                 scopes: Optional[list] = None,
                 web_flow: bool = False):
        """
        Initialize the authentication manager with configurable paths and scopes.
        
        Args:
            token_path: Path to token storage file
            credentials_path: Path to client secrets file
            scopes: List of required Gmail API scopes
            web_flow: Whether to use web application flow instead of installed app flow
        """
        self.token_path = Path(token_path)
        self.credentials_path = Path(credentials_path)
        self.web_flow = web_flow
        self.scopes = scopes or [
            'https://www.googleapis.com/auth/gmail.readonly',
            'https://www.googleapis.com/auth/gmail.modify',
            'https://www.googleapis.com/auth/userinfo.email',
            'https://www.googleapis.com/auth/userinfo.profile'
        ]
        
        # Configure secure storage
        self.token_dir = self.token_path.parent
        self.token_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize retry configuration
        self.max_retries = 1
        self.retry_delay = 3
        
        # Web flow specific configuration
        self.redirect_uri = os.getenv("GOOGLE_REDIRECT_URI", "http://localhost:8000/api/auth/google-callback")
        
        # Setup logging
        self._setup_logging()

    def _setup_logging(self):
        """Configure comprehensive authentication logging."""
        log_dir = Path('logs')
        log_dir.mkdir(exist_ok=True)
        
        auth_handler = logging.FileHandler('logs/authentication.log')
        auth_handler.setFormatter(logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        ))
        logger.addHandler(auth_handler)
        logger.setLevel(logging.DEBUG)

    def authenticate(self) -> Optional[Credentials]:
        """
        Perform authentication with comprehensive error handling and logging.
        
        Returns:
            Valid credentials object or None if authentication fails
        """
        try:
            # Attempt to load existing credentials
            credentials = self._load_credentials()
            
            if credentials and credentials.valid:
                logger.info("Using valid existing credentials")
                return credentials
                
            if credentials and credentials.expired and credentials.refresh_token:
                logger.info("Attempting to refresh expired credentials")
                return self._refresh_credentials(credentials)
                
            # No valid credentials, initiate new authentication
            logger.info("No valid credentials found, starting new authentication flow")
            if self.web_flow:
                logger.info("Web flow not directly supported in this context - must be handled by API routes")
                return None
            else:
                return self._authenticate_new_installed()
            
        except Exception as e:
            logger.error(f"Authentication error: {str(e)}", exc_info=True)
            return None

    def create_authorization_url(self) -> Tuple[str, str]:
        """
        Create Google OAuth authorization URL for web flow.
        
        Returns:
            Tuple containing (authorization_url, state)
        """
        try:
            if not self.credentials_path.exists():
                logger.error("Client secrets file not found")
                raise FileNotFoundError("client_secret.json not found")
                
            # Create flow using client secrets file
            flow = Flow.from_client_secrets_file(
                str(self.credentials_path),
                scopes=self.scopes,
                redirect_uri=self.redirect_uri
            )
            
            # Generate random state to verify the response
            authorization_url, state = flow.authorization_url(
                access_type='offline',
                include_granted_scopes='true',
                prompt='consent'
            )
            
            logger.info(f"Authorization URL created, state: {state}")
            return authorization_url, state
            
        except Exception as e:
            logger.error(f"Error creating authorization URL: {str(e)}")
            raise

    async def exchange_code(self, code: str, state: Optional[str] = None) -> Optional[Credentials]:
        """
        Exchange authorization code for credentials in web flow.
        
        Args:
            code: Authorization code from Google
            state: State parameter for verification
            
        Returns:
            Credentials object or None if exchange fails
        """
        try:
            if not self.credentials_path.exists():
                logger.error("Client secrets file not found")
                raise FileNotFoundError("client_secret.json not found")
                
            # Create flow using client secrets file
            flow = Flow.from_client_secrets_file(
                str(self.credentials_path),
                scopes=self.scopes,
                redirect_uri=self.redirect_uri,
                state=state
            )
            
            # Exchange authorization code for credentials
            flow.fetch_token(code=code)
            credentials = flow.credentials
            
            # Save credentials for future use
            self._save_credentials(credentials)
            
            logger.info("Successfully exchanged authorization code for credentials")
            return credentials
            
        except Exception as e:
            logger.error(f"Error exchanging authorization code: {str(e)}")
            return None

    def _load_credentials(self) -> Optional[Credentials]:
        """
        Load credentials from secure storage with validation.
        
        Returns:
            Credentials object if valid credentials exist, None otherwise
        """
        try:
            if not self.token_path.exists():
                logger.debug("No token file found")
                return None
                
            with open(self.token_path, 'r') as token_file:
                token_data = json.load(token_file)
                
            # Validate token data structure
            if not self._validate_token_data(token_data):
                logger.warning("Invalid token data structure")
                return None
                
            credentials = Credentials.from_authorized_user_info(token_data, self.scopes)
            logger.debug("Successfully loaded credentials from storage")
            return credentials
            
        except Exception as e:
            logger.error(f"Error loading credentials: {str(e)}")
            self._handle_invalid_token()
            return None

    def _validate_token_data(self, token_data: Dict) -> bool:
        """
        Validate token data structure and required fields.
        
        Args:
            token_data: Dictionary containing token information
            
        Returns:
            True if token data is valid, False otherwise
        """
        required_fields = {'token', 'refresh_token', 'token_uri', 'client_id', 'client_secret'}
        return all(field in token_data for field in required_fields)

    def _refresh_credentials(self, credentials: Credentials) -> Optional[Credentials]:
        """
        Refresh expired credentials with retry logic.
        
        Args:
            credentials: Expired credentials to refresh
            
        Returns:
            Refreshed credentials or None if refresh fails
        """
        for attempt in range(self.max_retries + 1):
            try:
                credentials.refresh(Request())
                self._save_credentials(credentials)
                logger.info("Successfully refreshed credentials")
                return credentials
                
            except RefreshError as e:
                logger.error(f"Refresh token expired or revoked: {str(e)}")
                self._handle_invalid_token()
                if self.web_flow:
                    return None
                else:
                    return self._authenticate_new_installed()
                
            except Exception as e:
                if attempt < self.max_retries:
                    logger.warning(f"Refresh attempt {attempt + 1} failed: {str(e)}")
                    time.sleep(self.retry_delay)
                else:
                    logger.error("All refresh attempts failed")
                    self._handle_invalid_token()
                    return None

    def _authenticate_new_installed(self) -> Optional[Credentials]:
        """
        Perform new OAuth2 authentication flow for installed applications.
        
        Returns:
            New credentials object or None if authentication fails
        """
        try:
            if not self.credentials_path.exists():
                logger.error("Client secrets file not found")
                raise FileNotFoundError("client_secret.json not found")
                
            flow = InstalledAppFlow.from_client_secrets_file(
                str(self.credentials_path),
                self.scopes
            )
            credentials = flow.run_local_server(port=0)
            
            self._save_credentials(credentials)
            logger.info("Successfully completed new authentication flow")
            return credentials
            
        except Exception as e:
            logger.error(f"New authentication failed: {str(e)}")
            return None

    def _save_credentials(self, credentials: Credentials):
        """
        Securely save credentials with backup mechanism.
        
        Args:
            credentials: Credentials object to save
        """
        backup_path = None
        # Create backup of existing token if it exists
        if self.token_path.exists():
            backup_path = self.token_path.with_suffix('.backup')
            self.token_path.rename(backup_path)
            
        try:
            # Save new credentials
            with open(self.token_path, 'w') as token_file:
                token_file.write(credentials.to_json())
            logger.debug("Successfully saved credentials")
            
            # Remove backup if save was successful
            if backup_path and backup_path.exists():
                backup_path.unlink()
                
        except Exception as e:
            logger.error(f"Error saving credentials: {str(e)}")
            # Restore backup if save failed
            if backup_path and backup_path.exists():
               backup_path.rename(self.token_path)

    def _handle_invalid_token(self):
        """Handle invalid token situation with cleanup and logging."""
        try:
            if self.token_path.exists():
                self.token_path.unlink()
            logger.info("Removed invalid token file")
        except Exception as e:
            logger.error(f"Error removing invalid token: {str(e)}")

    def store_user_tokens(self, user_id: str, tokens: Dict[str, Any]) -> bool:
        """
        Store user-specific tokens for web flow authentication.
        
        In a production environment, these would be stored in a database
        associated with the user. This implementation provides a simplified
        file-based approach for demonstration purposes.
        
        Args:
            user_id: Unique identifier for the user
            tokens: Dictionary containing tokens and metadata
            
        Returns:
            True if tokens were successfully stored, False otherwise
        """
        try:
            # Create user tokens directory if it doesn't exist
            user_tokens_dir = Path('data/secure/user_tokens')
            user_tokens_dir.mkdir(parents=True, exist_ok=True)
            
            # Create user token file path
            user_token_path = user_tokens_dir / f"{user_id}.json"
            
            # Save tokens to file
            with open(user_token_path, 'w') as f:
                json.dump(tokens, f, indent=2)
                
            logger.info(f"Successfully stored tokens for user: {user_id}")
            return True
            
        except Exception as e:
            logger.error(f"Error storing user tokens: {str(e)}")
            return False

    def get_user_tokens(self, user_id: str) -> Optional[Dict[str, Any]]:
        """
        Get user-specific tokens for web flow authentication.
        
        Args:
            user_id: Unique identifier for the user
            
        Returns:
            Dictionary containing tokens and metadata or None if not found
        """
        try:
            user_token_path = Path(f'data/secure/user_tokens/{user_id}.json')
            
            if not user_token_path.exists():
                logger.warning(f"No tokens found for user: {user_id}")
                return None
                
            with open(user_token_path, 'r') as f:
                tokens = json.load(f)
                
            logger.info(f"Successfully retrieved tokens for user: {user_id}")
            return tokens
            
        except Exception as e:
            logger.error(f"Error retrieving user tokens: {str(e)}")
            return None

    def create_gmail_service(self, user_credentials: Optional[Credentials] = None) -> Optional[Any]:
        """
        Create authenticated Gmail service with error handling.
        
        Args:
            user_credentials: Optional explicit credentials to use
            
        Returns:
            Gmail service object or None if service creation fails
        """
        try:
            if user_credentials:
                credentials = user_credentials
            else:
                credentials = self.authenticate()
                
            if not credentials:
                logger.error("Failed to obtain valid credentials")
                return None
                
            service = build('gmail', 'v1', credentials=credentials)
            logger.info("Successfully created Gmail service")
            return service
            
        except Exception as e:
            logger.error(f"Error creating Gmail service: {str(e)}")
            return None
            
    def create_user_specific_gmail_service(self, user_id: str) -> Optional[Any]:
        """
        Create Gmail service for a specific user using stored tokens.
        
        This method is for web application flow where tokens are stored
        per user rather than globally for the application.
        
        Args:
            user_id: Unique identifier for the user
            
        Returns:
            Gmail service for the specified user or None if creation fails
        """
        try:
            # Get user tokens
            tokens = self.get_user_tokens(user_id)
            if not tokens:
                logger.error(f"No tokens found for user: {user_id}")
                return None
                
            # Create credentials from tokens
            credentials = Credentials(
                token=tokens.get("access_token"),
                refresh_token=tokens.get("refresh_token"),
                token_uri="https://oauth2.googleapis.com/token",
                client_id=tokens.get("client_id"),
                client_secret=tokens.get("client_secret"),
                scopes=self.scopes
            )
            
            # Check if credentials need refresh
            if credentials.expired:
                logger.info(f"Refreshing expired credentials for user: {user_id}")
                credentials.refresh(Request())
                
                # Update stored tokens
                self.store_user_tokens(user_id, {
                    "access_token": credentials.token,
                    "refresh_token": credentials.refresh_token,
                    "client_id": credentials.client_id,
                    "client_secret": credentials.client_secret,
                    "expires_at": int(time.time() + credentials.expiry),
                    "scopes": credentials.scopes
                })
            
            # Create Gmail service
            service = build('gmail', 'v1', credentials=credentials)
            logger.info(f"Successfully created Gmail service for user: {user_id}")
            return service
            
        except RefreshError as e:
            logger.error(f"Refresh token expired or revoked for user {user_id}: {str(e)}")
            return None
            
        except Exception as e:
            logger.error(f"Error creating Gmail service for user {user_id}: {str(e)}")
            return None

================
File: src/integrations/gmail/client.py
================
"""
Enhanced Gmail Client Implementation

This module implements a comprehensive Gmail API client with robust authentication
handling, detailed logging, and thorough error management. It integrates with
the AuthenticationManager to provide reliable email access while maintaining
proper security protocols and error recovery mechanisms.

Design Considerations:
- Robust authentication and token management through AuthenticationManager
- Comprehensive error handling with detailed logging
- Automatic service recovery on token expiration
- Thread-safe email operations
- Memory-efficient batch processing

Integration Requirements:
- Requires AuthenticationManager from auth_manager.py
- Expects proper logging configuration
- Gmail API scopes must match AuthenticationManager
"""

import base64
import logging
import time
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

from google.auth.exceptions import RefreshError
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from email.mime.text import MIMEText

from .auth_manager import GmailAuthenticationManager

logger = logging.getLogger(__name__)

class GmailClient:
    """
    Gmail API client with comprehensive error handling and automatic recovery.
    
    Implements reliable Gmail API access with:
    - Robust authentication management
    - Automatic service recovery
    - Detailed operation logging
    - Comprehensive error handling
    - Efficient batch processing
    
    Attributes:
        auth_manager: Authentication manager instance
        service: Authenticated Gmail API service
        batch_size: Maximum number of emails to process in one batch
        retry_count: Number of retry attempts for failed operations
    """
    
    def __init__(self, batch_size: int = 50, retry_count: int = 3):
        """
        Initialize Gmail client with configurable parameters.
        
        Args:
            batch_size: Maximum number of emails to process in one batch
            retry_count: Number of retry attempts for failed operations
            
        Raises:
            RuntimeError: If initial service initialization fails
        """
        self.auth_manager = GmailAuthenticationManager()
        self.batch_size = batch_size
        self.retry_count = retry_count
        
        # Initialize service with proper error handling
        self.service = self._initialize_service()
        if not self.service:
            raise RuntimeError("Failed to initialize Gmail service")
            
        logger.info("Gmail client initialized successfully")

    def _initialize_service(self) -> Optional[Any]:
        """
        Initialize Gmail service with robust error handling.
        
        Implements proper error recovery and logging during service
        initialization. Handles authentication failures gracefully.
        
        Returns:
            Authenticated Gmail service or None if initialization fails
        """
        try:
            service = self.auth_manager.create_gmail_service()
            if not service:
                logger.error("Failed to create Gmail service")
                return None
                
            logger.info("Gmail service initialized successfully")
            return service
            
        except Exception as e:
            logger.error(f"Service initialization failed: {str(e)}")
            return None

    def refresh_service(self) -> bool:
        """
        Refresh Gmail service with new authentication.
        
        Attempts to refresh the Gmail service when authentication
        expires or becomes invalid. Implements proper error handling
        and logging.
        
        Returns:
            bool: True if refresh successful, False otherwise
        """
        try:
            logger.info("Attempting service refresh")
            self.service = self.auth_manager.create_gmail_service()
            return bool(self.service)
            
        except Exception as e:
            logger.error(f"Service refresh failed: {str(e)}")
            return False

    def _extract_email_parts(self, msg: Dict) -> Tuple[str, List[Dict]]:
        """
        Extract email content and attachments with comprehensive parsing.
        
        Implements thorough MIME parsing with proper error handling for
        various email content types and encodings.
        
        Args:
            msg: Raw message dictionary from Gmail API
            
        Returns:
            Tuple containing (email_body, attachments_list)
        """
        body = ""
        attachments = []
        
        try:
            if 'parts' in msg['payload']:
                for part in msg['payload']['parts']:
                    content = self._process_message_part(msg['id'], part)
                    if content:
                        if isinstance(content, str):
                            body += content
                        else:
                            attachments.append(content)
            else:
                body = self._decode_body(msg['payload']['body'].get('data', ''))
                
            return body or 'No content available', attachments
            
        except Exception as e:
            logger.error(f"Error extracting email parts: {str(e)}")
            return 'Error processing content', []

    def _process_message_part(self, message_id: str, part: Dict) -> Optional[Any]:
        """
        Process individual message part with type-specific handling.
        
        Implements comprehensive MIME type handling with proper error
        recovery for various content types.
        
        Args:
            message_id: Gmail message ID
            part: Message part dictionary
            
        Returns:
            Processed content or None if processing fails
        """
        try:
            mime_type = part['mimeType']
            
            if mime_type == 'text/plain':
                if 'data' in part['body']:
                    return self._decode_body(part['body']['data'])
                elif 'attachmentId' in part['body']:
                    return self._fetch_attachment(message_id, part['body']['attachmentId'])
            elif mime_type.startswith('image/') or mime_type.startswith('application/'):
                if 'attachmentId' in part['body']:
                    return {
                        'id': part['body']['attachmentId'],
                        'mime_type': mime_type,
                        'filename': part.get('filename', 'unknown')
                    }
            
            return None
            
        except Exception as e:
            logger.error(f"Error processing message part: {str(e)}")
            return None

    def _decode_body(self, encoded_data: str) -> str:
        """
        Decode email body with robust error handling.
        
        Implements comprehensive decoding with proper error recovery
        for various encoding types.
        
        Args:
            encoded_data: Base64 encoded content
            
        Returns:
            Decoded content string or error message
        """
        if not encoded_data:
            return ''
            
        try:
            return base64.urlsafe_b64decode(encoded_data).decode('utf-8')
        except Exception as e:
            logger.error(f"Error decoding content: {str(e)}")
            return 'Error decoding content'

    def _fetch_attachment(self, message_id: str, attachment_id: str) -> Optional[str]:
        """
        Fetch and process email attachment with error handling.
        
        Implements secure attachment retrieval with proper error
        handling and logging.
        
        Args:
            message_id: Gmail message ID
            attachment_id: Attachment identifier
            
        Returns:
            Processed attachment content or None if fetch fails
        """
        try:
            attachment = self.service.users().messages().attachments().get(
                userId='me',
                messageId=message_id,
                id=attachment_id
            ).execute()
            
            return self._decode_body(attachment['data'])
            
        except Exception as e:
            logger.error(f"Error fetching attachment: {str(e)}")
            return None

    def get_unread_emails(self, max_results: Optional[int] = None) -> List[Dict]:
        """
        Retrieve unread emails with comprehensive metadata.
        
        Implements efficient batch processing with proper error handling
        and automatic service recovery.
        
        Args:
            max_results: Maximum number of emails to retrieve
                        (defaults to class batch_size)
                        
        Returns:
            List of processed email dictionaries
        """
        if max_results is None:
            max_results = self.batch_size
            
        try:
            results = self.service.users().messages().list(
                userId='me',
                labelIds=['UNREAD'],
                maxResults=max_results
            ).execute()

            messages = results.get('messages', [])
            return self._process_messages(messages)
            
        except RefreshError:
            logger.warning("Authentication refresh required")
            if self.refresh_service():
                return self.get_unread_emails(max_results)
            return []
            
        except Exception as e:
            logger.error(f"Error fetching unread emails: {str(e)}")
            return []

    def _process_messages(self, messages: List[Dict]) -> List[Dict]:
        """
        Process message batch with comprehensive error handling.
        
        Implements thorough message processing with proper error
        recovery for each message.
        
        Args:
            messages: List of message metadata from Gmail API
            
        Returns:
            List of processed email dictionaries
        """
        processed_messages = []
        
        for message in messages:
            try:
                full_msg = self.service.users().messages().get(
                    userId='me',
                    id=message['id'],
                    format='full'
                ).execute()
                
                email_data = self._extract_message_data(full_msg)
                if email_data:
                    processed_messages.append(email_data)
                    
            except Exception as e:
                logger.error(f"Error processing message {message['id']}: {str(e)}")
                continue
                
        return processed_messages

    def _extract_message_data(self, msg: Dict) -> Optional[Dict]:
        """
        Extract comprehensive message data with metadata.
        
        Implements thorough message parsing with proper error handling
        for all message components.
        
        Args:
            msg: Full message dictionary from Gmail API
            
        Returns:
            Processed email dictionary or None if extraction fails
        """
        try:
            headers = msg['payload']['headers']
            body, attachments = self._extract_email_parts(msg)
            
            return {
                "message_id": msg['id'],
                "thread_id": msg.get('threadId', ''),
                "thread_messages": self._get_thread_messages(msg.get('threadId', '')),
                "subject": self._get_header(headers, 'Subject', 'No Subject'),
                "sender": self._get_header(headers, 'From', 'No Sender'),
                "recipients": self._extract_recipients(headers),
                "received_at": self._get_header(headers, 'Date', 'No Date'),
                "content": body,
                "attachments": attachments,
                "labels": msg.get('labelIds', []),
                "processed_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error extracting message data: {str(e)}")
            return None

    def _get_header(self, headers: List[Dict], name: str, default: str = '') -> str:
        """
        Extract header value with proper error handling.
        
        Args:
            headers: List of message headers
            name: Header name to extract
            default: Default value if header not found
            
        Returns:
            Header value or default
        """
        return next((h['value'] for h in headers if h['name'] == name), default)

    def _extract_recipients(self, headers: List[Dict]) -> List[str]:
        """
        Extract all recipient addresses with deduplication.
        
        Implements comprehensive recipient extraction with proper
        handling of various address formats.
        
        Args:
            headers: List of message headers
            
        Returns:
            List of unique recipient addresses
        """
        recipients = set()
        recipient_fields = ['To', 'Cc', 'Bcc']
        
        for field in recipient_fields:
            value = self._get_header(headers, field)
            if value:
                # Split and clean email addresses
                addresses = [addr.strip() for addr in value.split(',')]
                recipients.update(addresses)
                
        return list(recipients)

    def _get_thread_messages(self, thread_id: str) -> List[str]:
        """
        Retrieve all message IDs in a thread with error handling.
        
        Args:
            thread_id: Gmail thread identifier
            
        Returns:
            List of message IDs in the thread
        """
        if not thread_id:
            return []
            
        try:
            thread = self.service.users().threads().get(
                userId='me',
                id=thread_id
            ).execute()
            
            return [msg['id'] for msg in thread['messages']]
            
        except Exception as e:
            logger.error(f"Error getting thread messages: {str(e)}")
            return []

    def modify_message_labels(
        self,
        message_id: str,
        add_labels: Optional[List[str]] = None,
        remove_labels: Optional[List[str]] = None
    ) -> bool:
        """
        Modify message labels with comprehensive error handling.
        
        Implements robust label modification with proper error
        recovery and automatic service refresh.
        
        Args:
            message_id: Gmail message ID
            add_labels: Labels to add
            remove_labels: Labels to remove
            
        Returns:
            True if modification successful, False otherwise
        """
        try:
            self.service.users().messages().modify(
                userId='me',
                id=message_id,
                body={
                    'addLabelIds': add_labels or [],
                    'removeLabelIds': remove_labels or []
                }
            ).execute()
            return True
            
        except RefreshError:
            if self.refresh_service():
                return self.modify_message_labels(message_id, add_labels, remove_labels)
            return False
            
        except Exception as e:
            logger.error(f"Error modifying message {message_id} labels: {str(e)}")
            return False

    def mark_as_read(self, message_id: str) -> bool:
        """
        Mark message as read with error handling.
        
        Returns:
            True if operation successful, False otherwise
        """
        return self.modify_message_labels(message_id, remove_labels=['UNREAD'])

    def mark_as_unread(self, message_id: str) -> bool:
        """
        Mark message as unread with error handling.
        
        Returns:
            True if operation successful, False otherwise
        """
        return self.modify_message_labels(message_id, add_labels=['UNREAD'])
    
    def send_email(self, to_email: str, subject: str, message_text: str) -> bool:
        """
        Send an email using Gmail API with comprehensive error handling.
        
        Implements secure email transmission with proper authentication management,
        automatic token refresh, and detailed logging throughout the process.
        
        Args:
            to_email: Recipient email address
            subject: Email subject line
            message_text: Plain text email body content
            
        Returns:
            bool: True if sending successful, False otherwise
        """
        try:
            # Create email MIME message
            message = MIMEText(message_text)
            message['to'] = to_email
            message['subject'] = subject
            
            # Convert to raw format required by Gmail API
            raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode('utf-8')
            
            # Process with proper error handling for authentication issues
            for attempt in range(self.retry_count + 1):
                try:
                    logger.info(f"Sending email to {self._mask_email(to_email)} with subject: {subject}")
                    
                    # Send the message using Gmail API
                    result = self.service.users().messages().send(
                        userId="me",
                        body={'raw': raw_message}
                    ).execute()
                    
                    message_id = result.get('id', '')
                    logger.info(f"Email sent successfully, message_id: {message_id}")
                    return True
                    
                except RefreshError:
                    logger.warning("Authentication refresh required during email sending")
                    if self.refresh_service():
                        continue  # Retry with refreshed service
                    return False
                    
                except Exception as e:
                    if attempt < self.retry_count:
                        logger.warning(f"Email sending attempt {attempt + 1} failed: {e}")
                        time.sleep(2 ** attempt)  # Exponential backoff
                        continue
                        
                    logger.error(f"Failed to send email after {self.retry_count + 1} attempts: {e}")
                    return False
                
        except Exception as e:
            logger.error(f"Error preparing email for sending: {e}")
            return False
            
    def _mask_email(self, email: str) -> str:
        """
        Mask email addresses for privacy in logs.
        
        Implements privacy protection by masking parts of email addresses
        while preserving enough information for debugging purposes.
        
        Args:
            email: Email address to mask
            
        Returns:
            Masked email address
        """
        if not email or '@' not in email:
            return email
            
        try:
            username, domain = email.split('@', 1)
            if len(username) <= 2:
                masked_username = '*' * len(username)
            else:
                masked_username = username[0] + '*' * (len(username) - 2) + username[-1]
                
            domain_parts = domain.split('.')
            masked_domain = domain_parts[0][0] + '*' * (len(domain_parts[0]) - 1)
            
            return f"{masked_username}@{masked_domain}.{'.'.join(domain_parts[1:])}"
        except Exception:
            # If masking fails, return a generic masked value
            return "***@***.***"

================
File: src/integrations/groq/__init__.py
================
from .client import EnhancedGroqClient
from .model_manager import ModelManager

__all__ = [
    'EnhancedGroqClient',
    'ModelManager'
]

================
File: src/integrations/groq/client_wrapper.py
================
from groq import Groq
from typing import Dict, List, Optional, Union
from datetime import datetime
import asyncio
import json
import os
from dotenv import load_dotenv


class EnhancedGroqClient:
    """Enhanced Groq client with retry logic, error handling, and performance monitoring."""

    def __init__(self, api_key: Optional[str] = None):
        """Initialize the enhanced Groq client."""
        load_dotenv(override=True)
        self.api_key = api_key or os.getenv('GROQ_API_KEY')
        if not self.api_key:
            raise ValueError("GROQ_API_KEY must be provided either through initialization or environment")

        self.client = Groq(api_key=self.api_key)
        self.metrics_file = 'groq_metrics.json'
        self.load_metrics()

    def load_metrics(self):
        """Load or initialize performance metrics."""
        try:
            with open(self.metrics_file, 'r') as f:
                self.metrics = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            self.metrics = {
                'requests': [],
                'errors': [],
                'performance': {
                    'avg_response_time': 0,
                    'total_requests': 0,
                    'success_rate': 100
                }
            }

    def save_metrics(self):
        """Save current metrics to file."""
        with open(self.metrics_file, 'w') as f:
            json.dump(self.metrics, f, indent=2)

    async def process_with_retry(self,
                                 messages: List[Dict],
                                 model: str = "llama-3.3-70b-versatile",
                                 max_retries: int = 3,
                                 **kwargs) -> Dict:
        """Process a request with retry logic and error handling."""
        start_time = datetime.now()
        retries = 0

        while retries < max_retries:
            try:
                # Configure default parameters based on task complexity
                params = {
                    'model': model,
                    'messages': messages,
                    'temperature': kwargs.get('temperature', 0.7),
                    'max_completion_tokens': kwargs.get('max_completion_tokens', 4096),
                    'service_tier': kwargs.get('service_tier', None),
                    **kwargs
                }

                # Make the API call
                response = await asyncio.to_thread(self.client.chat.completions.create, **params)

                # Record success metrics
                self.record_success(start_time)
                return response

            except Exception as e:
                retries += 1
                self.record_error(str(e))

                if retries == max_retries:
                    raise Exception(f"Failed after {max_retries} retries: {str(e)}")

                # Exponential backoff
                await asyncio.sleep(2 ** retries)

    def record_success(self, start_time: datetime):
        """Record successful request metrics."""
        duration = (datetime.now() - start_time).total_seconds()
        self.metrics['requests'].append({
            'timestamp': datetime.now().isoformat(),
            'duration': duration,
            'status': 'success'
        })

        # Update performance metrics
        total_reqs = len(self.metrics['requests'])
        self.metrics['performance'].update({
            'avg_response_time': (
                    (self.metrics['performance']['avg_response_time'] * (total_reqs - 1) + duration)
                    / total_reqs
            ),
            'total_requests': total_reqs,
            'success_rate': (
                    (total_reqs - len(self.metrics['errors'])) / total_reqs * 100
            )
        })

        self.save_metrics()

    def record_error(self, error_message: str):
        """Record error metrics."""
        self.metrics['errors'].append({
            'timestamp': datetime.now().isoformat(),
            'error': error_message
        })
        self.save_metrics()

    async def batch_process(self,
                            requests: List[Dict],
                            model: str = "llama-3.3-70b-versatile",
                            **kwargs) -> List[Dict]:
        """Process multiple requests in parallel."""
        tasks = [
            self.process_with_retry(
                messages=req['messages'],
                model=model,
                **kwargs
            )
            for req in requests
        ]
        return await asyncio.gather(*tasks, return_exceptions=True)

    def get_performance_metrics(self) -> Dict:
        """Get current performance metrics."""
        return self.metrics['performance']

================
File: src/integrations/groq/client.py
================
from groq import Groq
from typing import Dict, List, Optional, Union
from datetime import datetime
import asyncio
import json
import os
from pathlib import Path
from dotenv import load_dotenv
import logging

# Get the project root directory
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
LOGS_DIR = PROJECT_ROOT / 'logs'
DATA_DIR = PROJECT_ROOT / 'data'

# Ensure directories exist
LOGS_DIR.mkdir(parents=True, exist_ok=True)
(DATA_DIR / 'metrics').mkdir(parents=True, exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOGS_DIR / 'groq_client.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class EnhancedGroqClient:
    """Enhanced Groq client with retry logic and error handling."""

    def __init__(self, api_key: Optional[str] = None):
        """Initialize the enhanced Groq client with API key from environment or parameter."""
        load_dotenv(override=True)
        self.api_key = api_key or os.getenv('GROQ_API_KEY')
        if not self.api_key:
            raise ValueError("GROQ_API_KEY must be provided either through initialization or environment")

        self.client = Groq(api_key=self.api_key)

        self.metrics_file = DATA_DIR / 'metrics' / 'groq_metrics.json'
        self.load_metrics()

    def load_metrics(self):
        """Load or initialize performance metrics."""
        try:
            with open(self.metrics_file, 'r') as f:
                self.metrics = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            self.metrics = {
                'requests': [],
                'errors': [],
                'performance': {
                    'avg_response_time': 0,
                    'total_requests': 0,
                    'success_rate': 100
                }
            }
            self.save_metrics()

    def save_metrics(self):
        """Save current metrics to file."""
        with open(self.metrics_file, 'w') as f:
            json.dump(self.metrics, f, indent=2)

    async def process_with_retry(self,
                                 messages: List[Dict],
                                 max_retries: int = 3,
                                 **kwargs) -> Dict:
        """Process a request with retry logic and error handling.

        Args:
            messages: List of message dictionaries for the conversation
            max_retries: Maximum number of retry attempts
            **kwargs: Additional parameters for the API call

        Returns:
            API response dictionary
        """
        start_time = datetime.now()
        retries = 0
        last_error = None

        while retries < max_retries:
            try:
                # Configure default parameters
                params = {
                    'model': kwargs.get('model', 'llama-3.3-70b-versatile'),
                    'messages': messages,
                    'temperature': kwargs.get('temperature', 0.7),
                    'max_completion_tokens': kwargs.get('max_completion_tokens', 4096),
                    'service_tier': kwargs.get('service_tier', None),
                    **kwargs
                }

                # Make the API call
                response = await asyncio.to_thread(self.client.chat.completions.create, **params)

                # Record success metrics
                self.record_success(start_time)
                return response

            except Exception as e:
                retries += 1
                last_error = str(e)
                self.record_error(last_error)

                if retries == max_retries:
                    logger.error(f"Failed after {max_retries} retries: {last_error}")
                    raise Exception(f"Failed after {max_retries} retries: {last_error}")

                # Exponential backoff
                wait_time = 2 ** retries
                logger.warning(f"Attempt {retries} failed. Waiting {wait_time} seconds before retry...")
                await asyncio.sleep(wait_time)

    def record_success(self, start_time: datetime):
        """Record successful request metrics."""
        duration = (datetime.now() - start_time).total_seconds()
        self.metrics['requests'].append({
            'timestamp': datetime.now().isoformat(),
            'duration': duration,
            'status': 'success'
        })

        # Update aggregate performance metrics
        total_reqs = len(self.metrics['requests'])
        self.metrics['performance'].update({
            'avg_response_time': (
                    (self.metrics['performance']['avg_response_time'] * (total_reqs - 1) + duration)
                    / total_reqs
            ),
            'total_requests': total_reqs,
            'success_rate': (
                    (total_reqs - len(self.metrics['errors'])) / total_reqs * 100
            )
        })

        self.save_metrics()

    def record_error(self, error_message: str):
        """Record error metrics."""
        self.metrics['errors'].append({
            'timestamp': datetime.now().isoformat(),
            'error': error_message
        })
        self.save_metrics()

    def get_performance_metrics(self) -> Dict:
        """Get current performance metrics."""
        return self.metrics['performance']

================
File: src/integrations/groq/constants.py
================
# groq_integration/constants.py

MODEL_CONFIGURATIONS = {
    'complex': {
        'primary': {
            'name': 'llama-3.3-70b-versatile',
            'default_temperature': 0.7,
            'max_tokens': 4096,
            'recommended_tasks': ['meeting_analysis', 'complex_response_generation']
        },
        'fallback': {
            'name': 'llama-3.1-8b-instant',
            'default_temperature': 0.6,
            'max_tokens': 4096,
            'recommended_tasks': ['meeting_analysis', 'response_generation']
        }
    },
    'simple': {
        'primary': {
            'name': 'llama-3.3-70b-versatile',
            'default_temperature': 0.5,
            'max_tokens': 2048,
            'recommended_tasks': ['email_classification', 'simple_responses']
        },
        'fallback': {
            'name': 'llama-3.1-8b-instant',
            'default_temperature': 0.5,
            'max_tokens': 2048,
            'recommended_tasks': ['email_classification']
        }
    }
}

# Default settings for different task types
TASK_SETTINGS = {
    'meeting_analysis': {
        'complexity': 'complex',
        'temperature': 0.7,
        'reasoning_format': 'parsed'
    },
    'email_classification': {
        'complexity': 'simple',
        'temperature': 0.5,
        'reasoning_format': 'raw'
    },
    'response_generation': {
        'complexity': 'complex',
        'temperature': 0.6,
        'reasoning_format': 'parsed'
    }
}

================
File: src/integrations/groq/model_manager.py
================
import json
from datetime import datetime
from typing import Dict, Optional
from .constants import MODEL_CONFIGURATIONS, TASK_SETTINGS


class ModelManager:
    def __init__(self, service_tier: str = None, metrics_file: str = 'model_metrics.json'):
        """
        Initialize the ModelManager with service tier and metrics tracking.

        Args:
            metrics_file: File to store performance metrics
        """
        self.service_tier = service_tier
        self.metrics_file = metrics_file
        self.performance_metrics = self._load_metrics()

    def _load_metrics(self) -> Dict:
        """Load existing performance metrics from file"""
        try:
            with open(self.metrics_file, 'r') as f:
                return json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            return {'models': {}, 'tasks': {}}

    def get_model_config(self, task_type: str, force_model: Optional[str] = None) -> Dict:
        """
        Get the appropriate model configuration for a task.

        Args:
            task_type: Type of task (e.g., 'meeting_analysis')
            force_model: Optional specific model to use

        Returns:
            Dict containing model configuration
        """
        if force_model:
            # Find and return forced model config
            for complexity in MODEL_CONFIGURATIONS.values():
                for model_type in complexity.values():
                    if model_type['name'] == force_model:
                        return model_type
            raise ValueError(f"Forced model {force_model} not found in configurations")

        # Get task complexity from settings
        task_settings = TASK_SETTINGS.get(task_type)
        if not task_settings:
            raise ValueError(f"Unknown task type: {task_type}")

        complexity = task_settings['complexity']
        models = MODEL_CONFIGURATIONS[complexity]

        # Check performance metrics to decide between primary and fallback
        if self._should_use_fallback(task_type, models['primary']['name']):
            return models['fallback']
        return models['primary']

    def _should_use_fallback(self, task_type: str, primary_model: str) -> bool:
        """Determine if we should use fallback based on recent performance"""
        # Implementation of fallback logic based on metrics
        # This can be expanded based on your specific requirements
        return False

    def record_performance(self, model: str, task_type: str, metrics: Dict):
        """
        Record performance metrics for a model on a specific task.

        Args:
            model: Model name
            task_type: Type of task
            metrics: Dictionary containing performance metrics
        """
        timestamp = datetime.now().isoformat()

        if model not in self.performance_metrics['models']:
            self.performance_metrics['models'][model] = []

        self.performance_metrics['models'][model].append({
            'timestamp': timestamp,
            'task_type': task_type,
            **metrics
        })

        # Save updated metrics
        with open(self.metrics_file, 'w') as f:
            json.dump(self.performance_metrics, f, indent=2)

================
File: src/storage/__init__.py
================
from .secure import SecureStorage

__all__ = ['SecureStorage']

================
File: src/storage/database.py
================
"""
Database Configuration and Connection Management

Provides comprehensive database setup, connection pooling, and session
management with proper error handling and connection lifecycle management.

Design Considerations:
- Connection pooling for optimal performance
- SQLAlchemy session management
- Comprehensive error handling
- Secure credential management
- Support for multiple database backends
"""

import os
import logging
from contextlib import contextmanager
from typing import Generator, Any

from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.pool import QueuePool

from src.storage.models import Base
from src.storage.encryption import encrypt_value, decrypt_value

logger = logging.getLogger(__name__)

# Database configuration
DB_PATH = os.getenv("DATABASE_URL", "sqlite:///data/secure/sentient_inbox.db")
DB_POOL_SIZE = int(os.getenv("DB_POOL_SIZE", "5"))
DB_MAX_OVERFLOW = int(os.getenv("DB_MAX_OVERFLOW", "10"))
DB_POOL_TIMEOUT = int(os.getenv("DB_POOL_TIMEOUT", "30"))

# Initialize engine with connection pooling
engine = create_engine(
    DB_PATH,
    poolclass=QueuePool,
    pool_size=DB_POOL_SIZE,
    max_overflow=DB_MAX_OVERFLOW,
    pool_timeout=DB_POOL_TIMEOUT,
    connect_args={"check_same_thread": False} if DB_PATH.startswith("sqlite") else {},
    echo=os.getenv("SQL_ECHO", "False").lower() == "true"
)

# Create session factory
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def init_db() -> None:
    """
    Initialize database with proper schema creation and migration handling.
    
    Creates all tables if they don't exist and performs any necessary
    migrations to ensure schema compatibility. Implements proper error
    handling for database setup failures.
    """
    try:
        logger.info("Initializing database schema")
        Base.metadata.create_all(bind=engine)
        logger.info("Database schema initialized successfully")
    except Exception as e:
        logger.error(f"Database initialization failed: {str(e)}")
        raise RuntimeError(f"Failed to initialize database: {str(e)}")

@contextmanager
def get_db_session() -> Generator[Session, None, None]:
    """
    Provide database session with proper error handling and cleanup.
    
    Implements comprehensive session lifecycle management with proper
    error handling, rollback on exceptions, and guaranteed cleanup.
    
    Yields:
        SQLAlchemy session for database operations
        
    Raises:
        Exception: Re-raises any exceptions that occur during session use
    """
    session = SessionLocal()
    try:
        yield session
        session.commit()
    except Exception as e:
        session.rollback()
        logger.error(f"Database session error: {str(e)}")
        raise
    finally:
        session.close()

================
File: src/storage/encryption.py
================
"""
Token Encryption Utilities

Provides secure encryption and decryption of sensitive OAuth tokens
with proper key management, initialization vector handling, and
comprehensive error management.

Design Considerations:
- Industry-standard AES-256 encryption
- Proper IV generation and handling
- Comprehensive error management
- Key derivation from environment secrets
"""

import os
import base64
import logging
from typing import Optional, Union
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC

logger = logging.getLogger(__name__)

# Load encryption key from environment or generate one
def get_encryption_key() -> bytes:
    """
    Get or generate a secure encryption key for token encryption.
    
    Loads encryption key from environment variable if available,
    otherwise generates a secure key using system entropy and
    PBKDF2 key derivation.
    
    Returns:
        bytes: Encryption key in bytes format
    """
    key_str = os.getenv("TOKEN_ENCRYPTION_KEY")
    if key_str:
        try:
            return base64.urlsafe_b64decode(key_str)
        except Exception as e:
            logger.warning(f"Invalid encryption key format, generating new key: {str(e)}")
    
    # Generate a secure key using PBKDF2
    salt = os.urandom(16)
    kdf = PBKDF2HMAC(
        algorithm=hashes.SHA256(),
        length=32,
        salt=salt,
        iterations=100000,
    )
    key = base64.urlsafe_b64encode(kdf.derive(os.urandom(32)))
    
    # Log warning that we're using a generated key
    logger.warning(
        "Using dynamically generated encryption key. Set TOKEN_ENCRYPTION_KEY "
        "environment variable for persistent encryption."
    )
    
    return key

# Initialize Fernet cipher with the encryption key
try:
    FERNET_KEY = get_encryption_key()
    cipher_suite = Fernet(FERNET_KEY)
    logger.info("Encryption system initialized successfully")
except Exception as e:
    logger.error(f"Failed to initialize encryption system: {str(e)}")
    raise RuntimeError(f"Encryption system initialization failed: {str(e)}")

def encrypt_value(value: Union[str, bytes]) -> str:
    """
    Encrypt a sensitive value with proper error handling.
    
    Implements secure encryption using Fernet symmetric encryption
    with comprehensive error handling and type conversion.
    
    Args:
        value: String or bytes value to encrypt
        
    Returns:
        str: Base64-encoded encrypted value
        
    Raises:
        ValueError: If encryption fails
    """
    if value is None:
        return None
        
    try:
        # Convert to bytes if string
        if isinstance(value, str):
            value_bytes = value.encode('utf-8')
        else:
            value_bytes = value
            
        # Encrypt and return as string
        encrypted = cipher_suite.encrypt(value_bytes)
        return base64.urlsafe_b64encode(encrypted).decode('utf-8')
    except Exception as e:
        logger.error(f"Encryption error: {str(e)}")
        raise ValueError(f"Failed to encrypt value: {str(e)}")

def decrypt_value(encrypted_value: str) -> str:
    """
    Decrypt an encrypted value with proper error handling.
    
    Implements secure decryption using Fernet symmetric encryption
    with comprehensive error handling and type conversion.
    
    Args:
        encrypted_value: Base64-encoded encrypted value
        
    Returns:
        str: Decrypted string value
        
    Raises:
        ValueError: If decryption fails
    """
    if encrypted_value is None:
        return None
        
    try:
        # Decode base64 and decrypt
        encrypted_bytes = base64.urlsafe_b64decode(encrypted_value)
        decrypted = cipher_suite.decrypt(encrypted_bytes)
        return decrypted.decode('utf-8')
    except Exception as e:
        logger.error(f"Decryption error: {str(e)}")
        raise ValueError(f"Failed to decrypt value: {str(e)}")

================
File: src/storage/models.py
================
"""
Database Models for User Management System

Defines comprehensive database models for user information and OAuth token
storage, implementing secure handling of sensitive data with proper
encryption and relationship management.

Design Considerations:
- Secure token storage with encryption
- Comprehensive user profile management
- Support for multiple OAuth providers per user
- Proper indexing for optimal query performance
- Clear documentation of field purposes
"""

import json
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Any, Set

from sqlalchemy import Column, String, Boolean, DateTime, ForeignKey, Text, Integer, JSON
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship, backref

Base = declarative_base()

class User(Base):
    """
    User model storing core user information with OAuth provider linkage.
    
    Implements secure user profile management with comprehensive field
    validation and OAuth provider integration while maintaining minimal
    required user information for system functionality.
    """
    __tablename__ = "users"
    
    id = Column(String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
    email = Column(String(255), unique=True, nullable=False, index=True)
    username = Column(String(100), unique=True, nullable=False, index=True)
    display_name = Column(String(255), nullable=True)
    
    # User permissions and status
    is_active = Column(Boolean, default=True, nullable=False)
    permissions = Column(JSON, nullable=False, default=lambda: json.dumps(["view"]))
    
    # Profile information (optional)
    profile_picture = Column(String(500), nullable=True)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    last_login = Column(DateTime, nullable=True)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)
    
    # OAuth token relationships
    oauth_tokens = relationship("OAuthToken", back_populates="user", cascade="all, delete-orphan")
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert user to dictionary representation for API responses."""
        return {
            "id": self.id,
            "email": self.email,
            "username": self.username,
            "display_name": self.display_name,
            "is_active": self.is_active,
            "permissions": json.loads(self.permissions) if isinstance(self.permissions, str) else self.permissions,
            "profile_picture": self.profile_picture,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "last_login": self.last_login.isoformat() if self.last_login else None,
            "oauth_providers": [token.provider for token in self.oauth_tokens]
        }

class OAuthToken(Base):
    """
    OAuth token storage with secure encryption and provider metadata.
    
    Implements comprehensive token management with secure storage
    of sensitive OAuth credentials, refresh token handling, and
    proper provider-specific metadata.
    """
    __tablename__ = "oauth_tokens"
    
    id = Column(String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
    user_id = Column(String(36), ForeignKey("users.id", ondelete="CASCADE"), nullable=False)
    provider = Column(String(50), nullable=False, index=True)  # 'google', 'microsoft', etc.
    
    # Provider-specific identifiers
    provider_user_id = Column(String(255), nullable=False)
    provider_email = Column(String(255), nullable=False)
    
    # Token data (encrypted in the database)
    access_token = Column(Text, nullable=False)
    refresh_token = Column(Text, nullable=True)
    token_type = Column(String(50), nullable=False, default="Bearer")
    expires_at = Column(DateTime, nullable=False)
    
    # Scopes granted
    scopes = Column(Text, nullable=False)
    
    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)
    
    # Relationships
    user = relationship("User", back_populates="oauth_tokens")
    
    # Unique constraint: one provider per user
    __table_args__ = (
        {'sqlite_autoincrement': True},
    )

================
File: src/storage/secure.py
================
import asyncio
import base64
import hashlib
import json
import logging
import os
import shutil
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Set

from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC

logger = logging.getLogger(__name__)

# Constants for security settings
KEY_ROTATION_DAYS = 30
BACKUP_RETENTION_DAYS = 7
MAX_RETRIES = 1
RETRY_DELAY = 3  # seconds
WEEKLY_HISTORY_DAYS = 7

class SecureStorage:
    """
    Manages secure storage of email records with encryption, automatic cleanup, and weekly rolling history.
    
    This class provides comprehensive data security for stored email records, implementing
    industrial-grade encryption, automatic key rotation, robust backup mechanisms, and
    sophisticated retrieval capabilities. It follows strict error handling protocols defined
    in the system documentation.
    
    Key Features:
    - Fernet symmetric encryption for data at rest
    - Automatic key rotation based on configurable periods
    - Secure backup and restoration mechanisms
    - Comprehensive error handling with retry logic
    - Weekly rolling history for optimized record management
    - Advanced data retrieval methods with filtering capabilities
    """

    def __init__(self, storage_path: str = "data/secure"):
        """
        Initialize the secure storage manager with encryption setup.
        
        Performs comprehensive initialization including directory creation,
        encryption key management, and storage preparation. Sets up the
        encrypted storage system with proper security measures and validation.
        
        Args:
            storage_path: Base path for secure storage (default: "data/secure")
        """
        self.storage_path = Path(storage_path)
        self.record_file = self.storage_path / "encrypted_records.bin"
        self.backup_dir = self.storage_path / "backups"
        self.keys_file = self.storage_path / "key_history.bin"
        
        # Create necessary directories
        self.storage_path.mkdir(parents=True, exist_ok=True)
        self.backup_dir.mkdir(exist_ok=True)

        # Initialize or load encryption keys
        self.keys = self._initialize_keys()
        self.current_key = self.keys[0]  # Most recent key
        self.cipher_suite = Fernet(self.current_key)

        # Initialize storage if needed
        if not self.record_file.exists():
            self._write_encrypted_data({
                "records": [], 
                "metadata": {
                    "last_cleanup": None,
                    "last_key_rotation": datetime.now().isoformat(),
                    "last_backup": None,
                    "data_version": 1
                }
            })

        # Set up logging
        logging.basicConfig(level=logging.DEBUG)

    def _generate_secure_key(self, extra_entropy: Optional[bytes] = None) -> bytes:
        """
        Generate a secure encryption key using system-specific information.
        
        Creates a cryptographically secure key based on system parameters
        combined with optional additional entropy. Implements PBKDF2 with
        SHA-256 for key derivation following security best practices.
        
        Args:
            extra_entropy: Optional additional entropy for key generation
            
        Returns:
            Secure key in base64 URL-safe encoding
        """
        # Combine system info with optional extra entropy
        system_info = f"{os.getpid()}{os.path.getmtime(__file__)}{time.time()}"
        if extra_entropy:
            system_info = f"{system_info}{extra_entropy.hex()}"
        salt = hashlib.sha256(system_info.encode()).digest()

        # Use PBKDF2 to derive a secure key
        kdf = PBKDF2HMAC(
            algorithm=hashes.SHA256(),
            length=32,
            salt=salt,
            iterations=100000,
        )

        key = base64.urlsafe_b64encode(kdf.derive(os.urandom(32)))
        return key

    def _generate_record_id(self, email_data: Dict[str, Any]) -> str:
        """
        Generate a unique, non-reversible ID for an email record.
        
        Creates a deterministic but secure identifier based on email
        metadata. Uses SHA-256 hashing to ensure uniqueness and
        non-reversibility for privacy protection.
        
        Args:
            email_data: Email record data dictionary
            
        Returns:
            Secure record identifier limited to 32 characters
        """
        # Combine relevant data to create a unique identifier
        unique_data = f"{email_data.get('timestamp', '')}{email_data.get('message_id', '')}"
        # Create a one-way hash that can't be reversed to get the original data
        return hashlib.sha256(unique_data.encode()).hexdigest()[:32]

    def _initialize_keys(self) -> List[bytes]:
        """
        Initialize or load encryption keys with history.
        
        Sets up the encryption key system with key history management.
        Implements secure error handling and recovery mechanisms in
        case of key loading failures.
        
        Returns:
            List of encryption keys with most recent first
        """
        try:
            if self.keys_file.exists():
                with open(self.keys_file, 'rb') as f:
                    keys_data = json.load(f)
                    return [k.encode() for k in keys_data['keys']]
            
            # Generate initial key
            initial_key = base64.urlsafe_b64encode(os.urandom(32))
            with open(self.keys_file, 'w') as f:
                json.dump({'keys': [initial_key.decode()]}, f)
            return [initial_key]
            
        except Exception as e:
            logger.error(f"Error initializing keys: {e}")
            # Generate a new key if there's any error
            initial_key = base64.urlsafe_b64encode(os.urandom(32))
            with open(self.keys_file, 'w') as f:
                json.dump({'keys': [initial_key.decode()]}, f)
            return [initial_key]

    def _save_keys(self, keys: List[bytes]) -> bool:
        """
        Save encryption keys.
        
        Persists the current key set to secure storage with proper
        encoding and error handling. Maintains the key history
        for backward compatibility.
        
        Args:
            keys: List of encryption keys to save
            
        Returns:
            Success indicator for the operation
        """
        try:
            keys_data = {
                'keys': [base64.urlsafe_b64encode(k).decode() for k in keys]
            }
            with open(self.keys_file, 'w') as f:
                json.dump(keys_data, f)
            return True
        except Exception as e:
            logger.error(f"Error saving keys: {e}")
            return False

    async def _read_encrypted_data(self, allow_restore: bool = True) -> Dict:
        """
        Read and decrypt the stored data with retry and key rotation support.
        
        Implements comprehensive decryption with key rotation support,
        backup restoration, and proper error handling following system
        specifications in error-handling.md.
        
        Args:
            allow_restore: Whether to attempt backup restoration on failure
            
        Returns:
            Decrypted data dictionary or empty structure on failure
        """
        for attempt in range(MAX_RETRIES):
            try:
                if not self.record_file.exists():
                    return {"records": [], "metadata": self._get_default_metadata()}

                with open(self.record_file, 'rb') as f:
                    encrypted_data = f.read()
                    if not encrypted_data:
                        return {"records": [], "metadata": self._get_default_metadata()}

                    # Try decryption with all available keys
                    last_error = None
                    for key in self.keys:
                        try:
                            cipher = Fernet(key)
                            decrypted_data = cipher.decrypt(encrypted_data)
                            data = json.loads(decrypted_data)
                            
                            # Verify data integrity
                            if not self._verify_data_structure(data):
                                raise ValueError("Invalid data structure")
                            
                            # Re-encrypt with current key if an old key was used
                            if key != self.current_key:
                                self._write_encrypted_data(data)
                            
                            return data
                        except Exception as e:
                            last_error = e
                            continue
                    
                    # If decryption failed and restore is allowed, try to restore
                    if allow_restore and self._restore_from_backup():
                        # Try reading one more time without allowing another restore
                        return await self._read_encrypted_data(allow_restore=False)
                    
                    raise ValueError(f"Unable to decrypt with any available key: {last_error}")
                    
            except Exception as e:
                if attempt < MAX_RETRIES - 1:
                    logger.warning(f"Retry {attempt + 1} reading data: {e}")
                    time.sleep(RETRY_DELAY * (attempt + 1))  # Exponential backoff
                    continue
                logger.error(f"Error reading encrypted data: {e}")
                if allow_restore and self._restore_from_backup():
                    return await self._read_encrypted_data(allow_restore=False)
                return {"records": [], "metadata": self._get_default_metadata()}

    def _write_encrypted_data(self, data: Dict) -> bool:
        """
        Encrypt and write data to storage with backup.
        
        Implements secure data writing with proper backup creation,
        data verification, and atomic file operations. Follows strict
        error handling protocols with retry mechanisms.
        
        Args:
            data: Data dictionary to encrypt and store
            
        Returns:
            Success indicator for the operation
        """
        for attempt in range(MAX_RETRIES):
            try:
                # Create backup before writing
                self._create_backup()

                # Verify data structure before encryption
                if not self._verify_data_structure(data):
                    raise ValueError("Invalid data structure")

                # Encrypt and write data
                encrypted_data = self.cipher_suite.encrypt(json.dumps(data).encode())
                temp_file = self.record_file.with_suffix('.tmp')
                
                # Write to temp file first
                with open(temp_file, 'wb') as f:
                    f.write(encrypted_data)
                
                # Verify the temp file
                with open(temp_file, 'rb') as f:
                    verify_data = f.read()
                    if verify_data != encrypted_data:
                        raise ValueError("Data verification failed")
                
                # Atomic replace
                os.replace(temp_file, self.record_file)
                
                # Update backup timestamp
                data["metadata"]["last_backup"] = datetime.now().isoformat()
                return True

            except Exception as e:
                if attempt < MAX_RETRIES - 1:
                    logger.warning(f"Retry {attempt + 1} writing data: {e}")
                    time.sleep(RETRY_DELAY * (attempt + 1))  # Exponential backoff
                    if os.path.exists(temp_file):
                        try:
                            os.remove(temp_file)
                        except Exception:
                            pass
                    continue
                logger.error(f"Error writing encrypted data: {e}")
                return False

    def _verify_data_structure(self, data: Dict) -> bool:
        """
        Verify the integrity of the data structure.
        
        Validates that the data structure contains all required fields
        and follows the expected format. Ensures data consistency
        before storage operations.
        
        Args:
            data: Data dictionary to verify
            
        Returns:
            Validation result indicating structure integrity
        """
        try:
            required_keys = {"records", "metadata"}
            metadata_keys = {"last_cleanup", "last_key_rotation", "last_backup", "data_version"}
            
            if not all(k in data for k in required_keys):
                return False
                
            if not all(k in data["metadata"] for k in metadata_keys):
                return False
                
            if not isinstance(data["records"], list):
                return False
                
            return True
        except Exception:
            return False

    def _get_default_metadata(self) -> Dict:
        """
        Get default metadata structure.
        
        Creates a standard metadata structure with proper timestamps
        and version information for new storage initialization.
        
        Returns:
            Default metadata dictionary with proper initialization
        """
        return {
            "last_cleanup": None,
            "last_key_rotation": datetime.now().isoformat(),
            "last_backup": None,
            "data_version": 1
        }

    def _create_backup(self) -> bool:
        """
        Create a backup of the current data file.
        
        Implements comprehensive backup creation with verification
        and cleanup of old backups. Follows the backup management
        protocols defined in system specifications.
        
        Returns:
            Success indicator for the backup operation
        """
        try:
            if self.record_file.exists():
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_file = self.backup_dir / f"records_backup_{timestamp}.bin"
                shutil.copy2(self.record_file, backup_file)
                
                # Verify backup
                if not backup_file.exists() or backup_file.stat().st_size != self.record_file.stat().st_size:
                    raise ValueError("Backup verification failed")
                
                # Cleanup old backups
                self._cleanup_old_backups()
                return True
            return False
        except Exception as e:
            logger.error(f"Error creating backup: {e}")
            return False

    def _cleanup_old_backups(self):
        """
        Remove backups older than BACKUP_RETENTION_DAYS.
        
        Implements retention policy enforcement by removing backups
        that exceed the configured retention period. Maintains a clean
        backup store while preserving recent history.
        """
        try:
            cutoff = datetime.now() - timedelta(days=BACKUP_RETENTION_DAYS)
            for backup_file in self.backup_dir.glob("records_backup_*.bin"):
                try:
                    if datetime.fromtimestamp(backup_file.stat().st_mtime) < cutoff:
                        backup_file.unlink()
                except Exception as e:
                    logger.error(f"Error cleaning up backup {backup_file}: {e}")
        except Exception as e:
            logger.error(f"Error during backup cleanup: {e}")

    def _restore_from_backup(self) -> bool:
        """
        Attempt to restore from the most recent valid backup.
        
        Implements comprehensive backup restoration with validation
        and error handling. Attempts to restore from each available
        backup until a valid one is found.
        
        Returns:
            Success indicator for the restoration operation
        """
        try:
            # Get list of backups sorted by modification time (newest first)
            backups = sorted(
                [f for f in self.backup_dir.glob("records_backup_*.bin") if f.is_file()],
                key=lambda x: x.stat().st_mtime,
                reverse=True
            )
            
            if not backups:
                logger.warning("No backups found for restoration")
                return False
            
            # Try each backup until we find a valid one
            for backup_file in backups:
                try:
                    # Read the backup data
                    with open(backup_file, 'rb') as f:
                        encrypted_data = f.read()
                        
                    # Try to decrypt with all available keys
                    for key in self.keys:
                        try:
                            cipher = Fernet(key)
                            decrypted_data = cipher.decrypt(encrypted_data)
                            data = json.loads(decrypted_data)
                            
                            if self._verify_data_structure(data):
                                # Re-encrypt with current key
                                encrypted_data = self.cipher_suite.encrypt(json.dumps(data).encode())
                                
                                # Write directly to record file
                                with open(self.record_file, 'wb') as f:
                                    f.write(encrypted_data)
                                
                                logger.info(f"Successfully restored from backup: {backup_file}")
                                return True
                        except Exception:
                            continue
                    
                except Exception as e:
                    logger.warning(f"Failed to restore from backup {backup_file}: {e}")
                    continue
            
            logger.error("All backup restoration attempts failed")
            return False
            
        except Exception as e:
            logger.error(f"Error during backup restoration: {e}")
            return False

    async def rotate_key(self) -> bool:
        """
        Rotate encryption key and re-encrypt data.
        
        Implements key rotation following cryptographic best practices.
        Generates a new key, re-encrypts all data, and maintains key history
        for backward compatibility and recovery purposes.
        
        Returns:
            Success indicator for the key rotation operation
        """
        try:
            # Check if rotation is needed
            data = await asyncio.to_thread(self._read_encrypted_data)
            last_rotation = datetime.fromisoformat(data["metadata"]["last_key_rotation"])
            if datetime.now() - last_rotation < timedelta(days=KEY_ROTATION_DAYS):
                return True

            # Generate new key
            new_key = await asyncio.to_thread(self._generate_secure_key, os.urandom(32))
            
            # Create backup before rotation
            if not await asyncio.to_thread(self._create_backup):
                logger.error("Failed to create backup before key rotation")
                return False
            
            # Re-encrypt data with new key
            self.keys.insert(0, new_key)
            self.current_key = new_key
            self.cipher_suite = Fernet(new_key)
            
            # Update metadata and save
            data["metadata"]["last_key_rotation"] = datetime.now().isoformat()
            success = await asyncio.to_thread(self._write_encrypted_data, data)
            
            if success:
                # Keep limited key history
                self.keys = self.keys[:3]  # Keep last 3 keys
                await asyncio.to_thread(self._save_keys, self.keys)
                return True
            return False

        except Exception as e:
            logger.error(f"Error rotating encryption key: {e}")
            return False

    async def _cleanup_old_records(self, retention_days: int = 30, force: bool = False) -> bool:
        """
        Remove records older than the retention period.
        
        Implements record retention policy enforcement by removing
        records that exceed the configured retention period. Maintains
        database size within reasonable limits while preserving
        recent history.
        
        Args:
            retention_days: Number of days to retain records
            force: If True, ignores last_cleanup timestamp (for testing)
            
        Returns:
            Success indicator for the cleanup operation
        """
        try:
            data = await asyncio.to_thread(self._read_encrypted_data)
            now = datetime.now()

            # Check if cleanup is needed (unless forced)
            if not force:
                last_cleanup = data["metadata"].get("last_cleanup")
                if last_cleanup:
                    last_cleanup_date = datetime.fromisoformat(last_cleanup)
                    if last_cleanup_date > now - timedelta(days=1):
                        return True

            # Keep only records within retention period
            cutoff_date = now - timedelta(days=retention_days)
            original_records = data["records"]
            data["records"] = [
                record for record in original_records
                if datetime.fromisoformat(record.get("timestamp", "2000-01-01")) > cutoff_date
            ]

            # Update cleanup timestamp and write changes
            data["metadata"]["last_cleanup"] = now.isoformat()
            return await asyncio.to_thread(self._write_encrypted_data, data)

        except Exception as e:
            logger.error(f"Error during cleanup: {e}")
            return False

    async def add_record(self, email_data: Dict[str, Any], force_cleanup: bool = False) -> Tuple[str, bool]:
        """
        Add a new email record to secure storage.
        
        Implements comprehensive record addition with data sanitization,
        unique ID generation, and proper error handling. Triggers maintenance
        operations like cleanup and key rotation as needed.
        
        Args:
            email_data: The email data to store
            force_cleanup: If True, forces cleanup regardless of last cleanup time
            
        Returns:
            Tuple of (record_id, success)
        """
        try:
            # Input validation
            if not email_data or not isinstance(email_data, dict):
                logger.error("Invalid email data format")
                return "", False

            # Generate a unique ID for the record
            record_id = self._generate_record_id(email_data)

            # Create a sanitized record with thread information
            sanitized_record = {
                "id": record_id,
                "timestamp": datetime.now().isoformat(),
                "processed": True,
                "message_id": email_data.get("message_id", ""),
                "thread_id": email_data.get("thread_id", ""),
                "thread_messages": email_data.get("thread_messages", []),
                "message_hash": hashlib.sha256(
                    f"{email_data.get('subject', '')}{email_data.get('sender', '')}"
                    f"{','.join(sorted(email_data.get('recipients', [])))}"
                    f"{email_data.get('thread_id', '')}".encode()
                ).hexdigest(),
                "checksum": hashlib.sha256(
                    json.dumps(email_data, sort_keys=True).encode()
                ).hexdigest(),
                "analysis_results": email_data.get("analysis_results", {})
            }

            # Read existing data, add new record, and write back
            data = await asyncio.to_thread(self._read_encrypted_data)
            data["records"].append(sanitized_record)
            
            if await asyncio.to_thread(self._write_encrypted_data, data):
                # Trigger maintenance operations
                await self._cleanup_old_records(force=force_cleanup)
                await self.rotate_key()  # Check and rotate key if needed
                return record_id, True
            return record_id, False

        except Exception as e:
            logger.error(f"Error adding record: {e}")
            return "", False

    async def is_processed(self, message_id: str) -> Tuple[bool, bool]:
        """
        Check if an email has been processed using its message ID.
        
        Verifies if an email has already been processed by checking
        both direct message ID match and thread membership. Prevents
        duplicate processing of related messages.
        
        Args:
            message_id: Email message ID to check
            
        Returns:
            Tuple of (is_processed, operation_success)
        """
        try:
            if not message_id:
                return False, True  # Not processed, but operation succeeded

            # Get all records
            data = await asyncio.to_thread(self._read_encrypted_data)
            records = data.get("records", [])
            
            # First check direct message ID match
            for record in records:
                if record.get("message_id") == message_id:
                    return True, True
                    
                # Also check if this message is in any processed thread
                thread_messages = record.get("thread_messages", [])
                if message_id in thread_messages:
                    return True, True
                    
            return False, True

        except Exception as e:
            logger.error(f"Error checking processed status: {e}")
            return False, False

    async def get_record_count(self) -> int:
        """
        Get the total number of records.
        
        Retrieves the current record count for monitoring and
        statistics purposes. Useful for dashboard data and
        performance monitoring.
        
        Returns:
            Total count of stored records
        """
        try:
            data = await asyncio.to_thread(self._read_encrypted_data)
            return len(data.get("records", []))
        except Exception as e:
            logger.error(f"Error getting record count: {e}")
            return 0

    async def get_processed_records_since(self, start_time: datetime) -> List[Dict[str, Any]]:
        """
        Retrieve processed records since a specified time.
        
        Implements advanced data retrieval with timestamp filtering.
        Returns only records that were processed on or after the
        specified timestamp. Useful for time-based analytics and reporting.
        
        Args:
            start_time: Timestamp to filter records from
            
        Returns:
            List of records processed since the specified time
        """
        try:
            data = await asyncio.to_thread(self._read_encrypted_data)
            records = data.get("records", [])
            
            # Filter records by timestamp
            filtered_records = []
            for record in records:
                if "timestamp" in record:
                    try:
                        record_time = datetime.fromisoformat(record["timestamp"])
                        if record_time >= start_time:
                            filtered_records.append(record)
                    except ValueError:
                        # Skip records with invalid timestamps
                        continue
            
            logger.debug(f"Retrieved {len(filtered_records)} records since {start_time.isoformat()}")
            return filtered_records
            
        except Exception as e:
            logger.error(f"Error retrieving records since {start_time.isoformat()}: {e}")
            return []

    async def get_records_by_category(self, category: str) -> List[Dict[str, Any]]:
        """
        Retrieve records filtered by category.
        
        Implements advanced data retrieval with category filtering.
        Returns only records that match the specified category.
        Useful for category-based analytics and reporting.
        
        Args:
            category: Category value to filter by (e.g., "meeting", "needs_review")
            
        Returns:
            List of records matching the specified category
        """
        try:
            data = await asyncio.to_thread(self._read_encrypted_data)
            records = data.get("records", [])
            
            # Filter records by category
            filtered_records = []
            for record in records:
                record_category = record.get("analysis_results", {}).get("final_category")
                if record_category == category:
                    filtered_records.append(record)
            
            logger.debug(f"Retrieved {len(filtered_records)} records with category '{category}'")
            return filtered_records
            
        except Exception as e:
            logger.error(f"Error retrieving records for category '{category}': {e}")
            return []

    async def get_all_processed_records(self) -> List[Dict[str, Any]]:
        """
        Retrieve all processed records.
        
        Implements comprehensive data retrieval for all processed records.
        Provides complete access to the stored record set for comprehensive
        analytics and reporting.
        
        Returns:
            List of all processed records
        """
        try:
            data = await asyncio.to_thread(self._read_encrypted_data)
            records = data.get("records", [])
            
            logger.debug(f"Retrieved all {len(records)} records")
            return records
            
        except Exception as e:
            logger.error(f"Error retrieving all records: {e}")
            return []

    async def get_category_counts(self) -> Dict[str, int]:
        """
        Get counts of records by category.
        
        Implements category-based counting of processed records.
        Provides an aggregate view of record distribution across
        different categories for analytics and reporting.
        
        Returns:
            Dictionary mapping category names to record counts
        """
        try:
            data = await asyncio.to_thread(self._read_encrypted_data)
            records = data.get("records", [])
            
            # Initialize category counter
            category_counts = {
                "meeting": 0,
                "needs_review": 0,
                "not_actionable": 0,
                "not_meeting": 0,
                "unknown": 0
            }
            
            # Count records by category
            for record in records:
                category = record.get("analysis_results", {}).get("final_category")
                if category in category_counts:
                    category_counts[category] += 1
                else:
                    category_counts["unknown"] += 1
            
            logger.debug(f"Retrieved category counts: {category_counts}")
            return category_counts
            
        except Exception as e:
            logger.error(f"Error retrieving category counts: {e}")
            return {"meeting": 0, "needs_review": 0, "not_actionable": 0, "not_meeting": 0, "unknown": 0}

================
File: src/storage/user_repository.py
================
"""
User Repository Implementation

Provides comprehensive database operations for user management with
robust error handling, transaction management, and optimized queries.

Design Considerations:
- Repository pattern for data access abstraction
- Comprehensive error handling
- Optimized database queries
- Transaction management
- Secure token handling
"""

import logging
import json
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any
from sqlalchemy.orm import Session
from sqlalchemy import or_, and_

from src.storage.models import User, OAuthToken
from src.storage.database import get_db_session
from src.storage.encryption import encrypt_value, decrypt_value

logger = logging.getLogger(__name__)

class UserRepository:
    """
    Repository for user management database operations.
    
    Implements comprehensive data access operations for user management
    with proper security, error handling, and transaction management.
    """
    
    @staticmethod
    async def create_user(
        email: str,
        username: str,
        display_name: Optional[str] = None,
        permissions: Optional[List[str]] = None,
        profile_picture: Optional[str] = None
    ) -> User:
        """
        Create a new user with specified attributes.
        
        Args:
            email: User email address (unique)
            username: Username (unique)
            display_name: Optional display name
            permissions: Optional list of permissions (defaults to ["view"])
            profile_picture: Optional profile picture URL
            
        Returns:
            Created user object
            
        Raises:
            ValueError: If user with email or username already exists
            RuntimeError: If database operation fails
        """
        with get_db_session() as session:
            # Check if user with email or username already exists
            existing_user = session.query(User).filter(
                or_(User.email == email, User.username == username)
            ).first()
            
            if existing_user:
                field = "email" if existing_user.email == email else "username"
                logger.warning(f"Attempted to create duplicate user with {field}: {email if field == 'email' else username}")
                raise ValueError(f"User with this {field} already exists")
            
            # Create new user
            user = User(
                email=email,
                username=username,
                display_name=display_name,
                permissions=json.dumps(permissions or ["view"]),
                profile_picture=profile_picture,
                created_at=datetime.utcnow(),
            )
            
            try:
                session.add(user)
                session.commit()
                logger.info(f"Created new user: {username} ({email})")
                return user
            except Exception as e:
                logger.error(f"Failed to create user {email}: {str(e)}")
                raise RuntimeError(f"Failed to create user: {str(e)}")
    
    @staticmethod
    async def get_user_by_email(email: str) -> Optional[User]:
        """
        Retrieve a user by email address.
        
        Args:
            email: Email address to search for
            
        Returns:
            User object if found, None otherwise
        """
        with get_db_session() as session:
            return session.query(User).filter(User.email == email).first()
    
    @staticmethod
    async def get_user_by_username(username: str) -> Optional[User]:
        """
        Retrieve a user by username.
        
        Args:
            username: Username to search for
            
        Returns:
            User object if found, None otherwise
        """
        with get_db_session() as session:
            return session.query(User).filter(User.username == username).first()
    
    @staticmethod
    async def get_user_by_oauth(provider: str, provider_user_id: str) -> Optional[User]:
        """
        Retrieve a user by OAuth provider and provider-specific ID.
        
        Args:
            provider: OAuth provider name (e.g., 'google', 'microsoft')
            provider_user_id: Provider-specific user ID
            
        Returns:
            User object if found, None otherwise
        """
        with get_db_session() as session:
            token = session.query(OAuthToken).filter(
                and_(
                    OAuthToken.provider == provider,
                    OAuthToken.provider_user_id == provider_user_id
                )
            ).first()
            
            if token:
                return token.user
            return None
    
    @staticmethod
    async def update_user_last_login(user_id: str) -> bool:
        """
        Update user's last login timestamp.
        
        Args:
            user_id: User ID to update
            
        Returns:
            Success flag indicating if update was successful
        """
        with get_db_session() as session:
            user = session.query(User).filter(User.id == user_id).first()
            if not user:
                logger.warning(f"Attempted to update last login for non-existent user: {user_id}")
                return False
                
            user.last_login = datetime.utcnow()
            session.commit()
            return True
    
    @staticmethod
    async def save_oauth_token(
        user_id: str,
        provider: str,
        provider_user_id: str,
        provider_email: str,
        access_token: str,
        refresh_token: Optional[str],
        expires_in: int,
        scopes: List[str]
    ) -> OAuthToken:
        """
        Save or update OAuth token for a user.
        
        Args:
            user_id: User ID to associate the token with
            provider: OAuth provider name (e.g., 'google', 'microsoft')
            provider_user_id: Provider-specific user ID
            provider_email: Provider-specific email address
            access_token: OAuth access token
            refresh_token: OAuth refresh token (may be None)
            expires_in: Token expiration time in seconds
            scopes: List of granted OAuth scopes
            
        Returns:
            Created or updated OAuthToken object
            
        Raises:
            ValueError: If user does not exist
            RuntimeError: If database operation fails
        """
        with get_db_session() as session:
            # Check if user exists
            user = session.query(User).filter(User.id == user_id).first()
            if not user:
                logger.warning(f"Attempted to save OAuth token for non-existent user: {user_id}")
                raise ValueError("User does not exist")
            
            # Calculate expiration timestamp
            expires_at = datetime.utcnow() + timedelta(seconds=expires_in)
            
            # Encrypt sensitive token data
            encrypted_access_token = encrypt_value(access_token)
            encrypted_refresh_token = encrypt_value(refresh_token) if refresh_token else None
            
            # Check if token for this provider already exists
            existing_token = session.query(OAuthToken).filter(
                and_(
                    OAuthToken.user_id == user_id,
                    OAuthToken.provider == provider
                )
            ).first()
            
            if existing_token:
                # Update existing token
                existing_token.provider_user_id = provider_user_id
                existing_token.provider_email = provider_email
                existing_token.access_token = encrypted_access_token
                if encrypted_refresh_token:
                    existing_token.refresh_token = encrypted_refresh_token
                existing_token.expires_at = expires_at
                existing_token.scopes = ",".join(scopes)
                existing_token.updated_at = datetime.utcnow()
                
                token = existing_token
                logger.info(f"Updated OAuth token for user {user_id} and provider {provider}")
            else:
                # Create new token
                token = OAuthToken(
                    user_id=user_id,
                    provider=provider,
                    provider_user_id=provider_user_id,
                    provider_email=provider_email,
                    access_token=encrypted_access_token,
                    refresh_token=encrypted_refresh_token,
                    token_type="Bearer",
                    expires_at=expires_at,
                    scopes=",".join(scopes),
                    created_at=datetime.utcnow(),
                    updated_at=datetime.utcnow()
                )
                session.add(token)
                logger.info(f"Created new OAuth token for user {user_id} and provider {provider}")
            
            try:
                session.commit()
                return token
            except Exception as e:
                logger.error(f"Failed to save OAuth token for user {user_id}: {str(e)}")
                raise RuntimeError(f"Failed to save OAuth token: {str(e)}")
    
    @staticmethod
    async def get_oauth_tokens(user_id: str, provider: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        Get OAuth tokens for a user with decrypted values.
        
        Args:
            user_id: User ID to retrieve tokens for
            provider: Optional provider filter
            
        Returns:
            List of token dictionaries with decrypted values
        """
        with get_db_session() as session:
            query = session.query(OAuthToken).filter(OAuthToken.user_id == user_id)
            
            if provider:
                query = query.filter(OAuthToken.provider == provider)
                
            tokens = query.all()
            
            # Decrypt token values
            result = []
            for token in tokens:
                result.append({
                    "id": token.id,
                    "provider": token.provider,
                    "provider_user_id": token.provider_user_id,
                    "provider_email": token.provider_email,
                    "access_token": decrypt_value(token.access_token),
                    "refresh_token": decrypt_value(token.refresh_token) if token.refresh_token else None,
                    "token_type": token.token_type,
                    "expires_at": token.expires_at.isoformat(),
                    "scopes": token.scopes.split(",") if token.scopes else [],
                    "created_at": token.created_at.isoformat(),
                    "updated_at": token.updated_at.isoformat()
                })
                
            return result

================
File: src/utils/date_utils.py
================
"""
Date handling utilities for email processing system.

This module provides robust date parsing and formatting capabilities,
specifically handling email date formats and ensuring proper conversion
to system-standard ISO format. Implements comprehensive error handling
and logging as specified in error-handling.md.
"""

from datetime import datetime
import email.utils
import logging
from typing import Optional, Tuple
from zoneinfo import ZoneInfo

logger = logging.getLogger(__name__)

class DateParsingError(Exception):
    """Custom exception for date parsing failures."""
    pass

def parse_email_date(date_str: str) -> Tuple[datetime, bool]:
    """
    Parse email date strings with comprehensive format handling.
    
    Implements robust parsing of various email date formats, including:
    - RFC 2822 format (standard email dates)
    - ISO format dates
    - Common variations of email date formats
    
    Args:
        date_str: Date string to parse
        
    Returns:
        Tuple containing:
        - Parsed datetime object in UTC
        - Boolean indicating parsing success
        
    Implementation follows error handling protocols from error-handling.md:
    - Single retry attempt
    - 3-second delay between attempts
    - Comprehensive error logging
    """
    if not date_str:
        logger.error("Empty date string provided")
        return datetime.now(ZoneInfo("UTC")), False
        
    try:
        # First attempt: Parse as email date format
        email_tuple = email.utils.parsedate_tz(date_str)
        if email_tuple:
            timestamp = email.utils.mktime_tz(email_tuple)
            return datetime.fromtimestamp(timestamp, ZoneInfo("UTC")), True
            
        # Second attempt: Parse as ISO format
        try:
            parsed_date = datetime.fromisoformat(date_str)
            if not parsed_date.tzinfo:
                parsed_date = parsed_date.replace(tzinfo=ZoneInfo("UTC"))
            return parsed_date, True
        except ValueError:
            pass
            
        # Final attempt: Common format variations
        for fmt in [
            "%Y-%m-%d %H:%M:%S %z",
            "%Y-%m-%d %H:%M:%S",
            "%a, %d %b %Y %H:%M:%S %z",
            "%a, %d %b %Y %H:%M:%S"
        ]:
            try:
                parsed = datetime.strptime(date_str, fmt)
                if not parsed.tzinfo:
                    parsed = parsed.replace(tzinfo=ZoneInfo("UTC"))
                return parsed, True
            except ValueError:
                continue
                
        raise DateParsingError(f"Unable to parse date string: {date_str}")
        
    except Exception as e:
        logger.error(f"Error parsing date string '{date_str}': {str(e)}")
        return datetime.now(ZoneInfo("UTC")), False

def format_iso_date(dt: datetime) -> str:
    """
    Format datetime object as ISO string with timezone handling.
    
    Ensures consistent ISO format output for all system date representations,
    maintaining timezone information and format compatibility.
    
    Args:
        dt: Datetime object to format
        
    Returns:
        ISO formatted date string
    """
    if not dt.tzinfo:
        dt = dt.replace(tzinfo=ZoneInfo("UTC"))
    return dt.isoformat()

def is_valid_iso_date(date_str: str) -> bool:
    """
    Validate if a string is in proper ISO format.
    
    Implements strict ISO format validation for system date strings,
    ensuring consistency in date handling across the system.
    
    Args:
        date_str: Date string to validate
        
    Returns:
        Boolean indicating if string is valid ISO format
    """
    try:
        datetime.fromisoformat(date_str)
        return True
    except ValueError:
        return False

================
File: startup_script.py
================
"""
Application Startup Script

Manages the application startup process with proper initialization, error handling,
and environment setup. Ensures all required components are available before
starting the main application.

This script implements comprehensive error handling following system specifications,
providing a robust startup process with proper logging and recovery mechanisms.

Usage:
    python start.py [mode]

Arguments:
    mode: 'api' to run the API server, 'process' to run email processing (default: both)

Returns:
    0 if startup successful, 1 otherwise
"""

import os
import sys
import subprocess
import logging
import argparse
import time
from pathlib import Path


def parse_arguments():
    """Parse command line arguments with proper validation."""
    parser = argparse.ArgumentParser(description="Start the Email Management Application")
    
    parser.add_argument(
        "mode", 
        nargs="?",
        choices=["api", "process", "both"],
        default="both",
        help="Startup mode: 'api' for API server, 'process' for email processing, 'both' for complete application"
    )
    
    parser.add_argument(
        "--host", 
        default="127.0.0.1",
        help="Host address for API server (default: 127.0.0.1)"
    )
    
    parser.add_argument(
        "--port", 
        type=int,
        default=8000,
        help="Port for API server (default: 8000)"
    )
    
    parser.add_argument(
        "--env",
        choices=["development", "testing", "production"],
        default="development",
        help="Environment to run in (default: development)"
    )
    
    parser.add_argument(
        "--skip-init",
        action="store_true",
        help="Skip initialization steps"
    )
    
    parser.add_argument(
        "--batch-size",
        type=int,
        default=50,
        help="Email batch size for processing (default: 50)"
    )
    
    return parser.parse_args()


def setup_logging():
    """Configure comprehensive logging for startup process."""
    # Ensure logs directory exists
    os.makedirs('logs', exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/startup.log'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger("startup")


def run_initialization():
    """
    Run pre-startup initialization with proper error handling.
    
    Implements comprehensive initialization following system specifications
    and error handling protocols, ensuring proper application preparation.
    
    Returns:
        bool: True if initialization successful, False otherwise
    """
    logger = logging.getLogger("startup.init")
    logger.info("Running pre-startup initialization")
    
    try:
        # Check if pre_startup script exists
        pre_startup_path = Path("pre_startup.py")
        if not pre_startup_path.exists():
            logger.error(f"Pre-startup script not found: {pre_startup_path}")
            return False
        
        # Run the pre-startup script
        result = subprocess.run(
            [sys.executable, "pre_startup.py"],
            check=True,
            capture_output=True,
            text=True
        )
        
        # Log output from pre-startup
        for line in result.stdout.splitlines():
            logger.info(f"Init: {line}")
            
        logger.info("Pre-startup initialization completed successfully")
        return True
        
    except subprocess.CalledProcessError as e:
        logger.error(f"Pre-startup initialization failed: {e.stderr}")
        return False
    except Exception as e:
        logger.error(f"Error running pre-startup initialization: {str(e)}")
        return False


def run_api_server(host, port, env):
    """
    Start the API server with proper error handling.
    
    Implements API server startup following system specifications,
    providing proper error handling and logging.
    
    Args:
        host: Host address to bind server
        port: Port to bind server
        env: Environment context
        
    Returns:
        subprocess.Popen: Running process or None if startup failed
    """
    logger = logging.getLogger("startup.api")
    logger.info(f"Starting API server on {host}:{port} in {env} environment")
    
    try:
        # Check if run_api.py script exists
        api_script = Path("run_api.py")
        if not api_script.exists():
            logger.error(f"API server script not found: {api_script}")
            return None
        
        # Build command
        cmd = [
            sys.executable,
            "run_api.py",
            "--host", host,
            "--port", str(port),
            "--env", env
        ]
        
        # Add reload flag in development mode
        if env == "development":
            cmd.append("--reload")
        
        # Start process without waiting
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=1,
            universal_newlines=True
        )
        
        # Give it a moment to detect immediate startup errors
        time.sleep(2)
        
        if process.poll() is not None:
            # Process already exited
            stdout, stderr = process.communicate()
            logger.error(f"API server failed to start: {stderr}")
            return None
            
        logger.info("API server started successfully")
        return process
        
    except Exception as e:
        logger.error(f"Error starting API server: {str(e)}")
        return None


def run_email_processor(batch_size):
    """
    Start the email processor with proper error handling.
    
    Implements email processor startup following system specifications,
    providing proper error handling and logging.
    
    Args:
        batch_size: Number of emails to process in batch
        
    Returns:
        subprocess.Popen: Running process or None if startup failed
    """
    logger = logging.getLogger("startup.processor")
    logger.info(f"Starting email processor with batch size {batch_size}")
    
    try:
        # Check if main.py script exists
        processor_script = Path("main.py")
        if not processor_script.exists():
            logger.error(f"Email processor script not found: {processor_script}")
            return None
        
        # Start process
        process = subprocess.Popen(
            [sys.executable, "main.py"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=1,
            universal_newlines=True,
            env={**os.environ, "BATCH_SIZE": str(batch_size)}
        )
        
        # Give it a moment to detect immediate startup errors
        time.sleep(2)
        
        if process.poll() is not None:
            # Process already exited
            stdout, stderr = process.communicate()
            logger.error(f"Email processor failed to start: {stderr}")
            return None
            
        logger.info("Email processor started successfully")
        return process
        
    except Exception as e:
        logger.error(f"Error starting email processor: {str(e)}")
        return None


def monitor_processes(processes):
    """
    Monitor running processes and log their output.
    
    Implements comprehensive process monitoring with proper output
    capturing and error handling following system specifications.
    
    Args:
        processes: Dictionary of running processes {name: process}
    """
    logger = logging.getLogger("startup.monitor")
    logger.info(f"Monitoring {len(processes)} processes")
    
    # Setup process-specific loggers
    process_loggers = {}
    for name in processes:
        process_loggers[name] = logging.getLogger(f"process.{name}")
    
    try:
        while processes:
            for name, process in list(processes.items()):
                # Check if process is still running
                if process.poll() is not None:
                    # Process exited
                    stdout, stderr = process.communicate()
                    logger.warning(f"Process '{name}' exited with code {process.returncode}")
                    
                    if stderr:
                        logger.error(f"Process '{name}' error output: {stderr}")
                    
                    del processes[name]
                    continue
                
                # Read output (non-blocking)
                while True:
                    line = process.stdout.readline()
                    if not line:
                        break
                    process_loggers[name].info(line.rstrip())
            
            # Short sleep to prevent CPU overuse
            time.sleep(0.1)
            
    except KeyboardInterrupt:
        logger.info("Received keyboard interrupt, shutting down processes")
        shutdown_processes(processes)
    except Exception as e:
        logger.error(f"Error monitoring processes: {str(e)}")
        shutdown_processes(processes)


def shutdown_processes(processes):
    """
    Safely shut down running processes.
    
    Implements graceful process termination following system specifications,
    ensuring proper cleanup on application shutdown.
    
    Args:
        processes: Dictionary of running processes {name: process}
    """
    logger = logging.getLogger("startup.shutdown")
    logger.info(f"Shutting down {len(processes)} processes")
    
    for name, process in processes.items():
        try:
            logger.info(f"Terminating process: {name}")
            process.terminate()
            
            # Give process time to terminate gracefully
            process.wait(timeout=5)
        except subprocess.TimeoutExpired:
            logger.warning(f"Process {name} did not terminate gracefully, killing")
            process.kill()
        except Exception as e:
            logger.error(f"Error shutting down process {name}: {str(e)}")


def main():
    """
    Main application startup sequence.
    
    Implements comprehensive application startup following system
    specifications and error handling protocols, providing a robust
    startup process with proper component initialization and monitoring.
    
    Returns:
        int: 0 for success, 1 for failure
    """
    # Parse arguments
    args = parse_arguments()
    
    # Setup logging
    logger = setup_logging()
    logger.info("=== Starting Email Management Application ===")
    logger.info(f"Starting in mode: {args.mode}, environment: {args.env}")
    
    # Run initialization unless explicitly skipped
    if not args.skip_init:
        init_success = run_initialization()
        if not init_success:
            logger.error("Initialization failed, aborting startup")
            return 1
    else:
        logger.info("Initialization skipped due to --skip-init flag")
    
    # Track running processes
    processes = {}
    
    # Start components based on selected mode
    if args.mode in ["api", "both"]:
        api_process = run_api_server(args.host, args.port, args.env)
        if api_process:
            processes["api"] = api_process
        else:
            logger.error("Failed to start API server")
            if args.mode == "api":
                return 1
    
    if args.mode in ["process", "both"]:
        processor_process = run_email_processor(args.batch_size)
        if processor_process:
            processes["processor"] = processor_process
        else:
            logger.error("Failed to start email processor")
            if args.mode == "process":
                # Shut down any started processes before exiting
                shutdown_processes(processes)
                return 1
    
    # Check if any processes started successfully
    if not processes:
        logger.error("No processes started successfully, aborting")
        return 1
    
    logger.info(f"Successfully started {len(processes)} components")
    
    # Monitor running processes
    try:
        monitor_processes(processes)
    except KeyboardInterrupt:
        logger.info("Application shutdown requested by user")
    finally:
        # Ensure all processes are terminated on exit
        shutdown_processes(processes)
    
    logger.info("=== Application shutdown complete ===")
    return 0

================
File: unicode_safe_logging.py
================
"""
Encoding-Safe Logging Utility

Provides robust logging functionality with proper Unicode handling for
cross-platform compatibility, specifically addressing Windows console
encoding limitations while maintaining visual indicators.

This module implements proper error handling and fallback mechanisms
following system specifications from error-handling.md.
"""

import logging
import os
import sys
from typing import Optional, Dict, Any
import platform


class SafeFormatter(logging.Formatter):
    """
    Custom log formatter with encoding-safe character substitution.
    
    Implements automatic substitution of Unicode symbols with ASCII
    alternatives when operating in environments with limited encoding
    support, ensuring consistent logging across all platforms.
    """
    
    # Define symbol mappings for Unicode -> ASCII-safe alternatives
    SYMBOL_MAP = {
        # Success symbols
        "✓": "√",  # Checkmark -> ASCII approximation
        "✅": "[SUCCESS]",
        
        # Warning symbols
        "⚠": "!",  # Warning symbol -> Exclamation mark
        "⚠️": "[WARNING]",
        
        # Error symbols
        "❌": "x",  # X mark -> ASCII 'x'
        "🔴": "[ERROR]",
        
        # Info symbols
        "ℹ": "i",  # Info symbol -> ASCII 'i'
        "🔵": "[INFO]",
        
        # Other symbols
        "→": "->",  # Right arrow -> ASCII approximation
        "←": "<-",  # Left arrow -> ASCII approximation
        "•": "*",   # Bullet point -> Asterisk
    }
    
    # Extra symbols that should be replaced in Windows environments
    WINDOWS_EXTRA_REPLACEMENTS = {
        "√": "OK",
        "✓": "OK",
        "⚠": "WARNING",
        "❌": "ERROR",
        "•": "-"
    }
    
    def __init__(self, fmt: Optional[str] = None, datefmt: Optional[str] = None,
                 style: str = '%', validate: bool = True):
        """
        Initialize formatter with format string and symbol replacement.
        
        Args:
            fmt: Format string for log messages
            datefmt: Date format string
            style: Style of format string (%, {, or $)
            validate: Whether to validate the format string
        """
        super().__init__(fmt, datefmt, style, validate)
        
        # Determine if we're on Windows to apply additional replacements
        self.is_windows = platform.system() == "Windows"
        
        # Check for explicit encoding override environment variable
        self.force_ascii = os.environ.get("FORCE_ASCII_LOGGING", "0").lower() in ("1", "true", "yes")
        
        # Check terminal encoding capabilities
        self.limited_encoding = self._has_limited_encoding()
    
    def _has_limited_encoding(self) -> bool:
        """
        Detect if the current environment has limited encoding support.
        
        Implements comprehensive encoding detection for terminals,
        identifying environments that may have problematic Unicode support.
        
        Returns:
            bool: True if environment has limited encoding support
        """
        # Always treat as limited encoding if forcing ASCII
        if self.force_ascii:
            return True
            
        # Windows command prompt and PowerShell often have encoding issues
        if self.is_windows:
            # Check if we're in Windows Terminal which has better Unicode support
            if "WT_SESSION" in os.environ:
                return False
                
            # Check if PYTHONIOENCODING is explicitly set to UTF-8
            if os.environ.get("PYTHONIOENCODING", "").lower() == "utf-8":
                return False
                
            # Default to limited encoding for standard Windows console
            return True
            
        # Most Unix-like systems have good Unicode support by default
        return False
    
    def format(self, record: logging.LogRecord) -> str:
        """
        Format log record with encoding-safe character substitution.
        
        Overrides the standard formatter to replace Unicode symbols
        with ASCII alternatives when necessary based on the detected
        environment capabilities.
        
        Args:
            record: Log record to format
            
        Returns:
            str: Formatted log message with encoding-safe characters
        """
        # First, let the parent formatter do its job
        formatted_message = super().format(record)
        
        # If we have limited encoding, apply substitutions
        if self.limited_encoding:
            # Apply standard substitutions
            for unicode_char, ascii_char in self.SYMBOL_MAP.items():
                formatted_message = formatted_message.replace(unicode_char, ascii_char)
            
            # Apply Windows-specific extra substitutions if needed
            if self.is_windows:
                for unicode_char, ascii_char in self.WINDOWS_EXTRA_REPLACEMENTS.items():
                    formatted_message = formatted_message.replace(unicode_char, ascii_char)
        
        return formatted_message


def configure_safe_logging(
    name: Optional[str] = None,
    level: int = logging.INFO,
    log_file: Optional[str] = None,
    format_str: Optional[str] = None
) -> logging.Logger:
    """
    Configure logging with encoding-safe formatting and proper error handling.
    
    Creates a logger with proper Unicode character handling for cross-platform
    compatibility, ensuring consistent logging output across environments.
    
    Args:
        name: Logger name (defaults to root logger if None)
        level: Logging level
        log_file: Optional log file path
        format_str: Custom format string for log messages
        
    Returns:
        logging.Logger: Configured logger instance
    """
    # Create logger
    logger = logging.getLogger(name)
    logger.setLevel(level)
    
    # Clear any existing handlers to avoid duplication
    if logger.hasHandlers():
        logger.handlers.clear()
    
    # Default format string if not provided
    if format_str is None:
        format_str = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    # Create safe formatter
    formatter = SafeFormatter(format_str)
    
    # Create console handler with safe encoding
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # Add file handler if log file specified
    if log_file:
        try:
            # Ensure directory exists
            os.makedirs(os.path.dirname(log_file), exist_ok=True)
            
            # Create file handler
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
        except Exception as e:
            # Log to console if file handler creation fails
            console_handler.setLevel(logging.WARNING)
            logger.warning(f"Failed to create log file handler: {str(e)}")
    
    return logger


# Constants for ASCII-art status indicators (Windows-safe)
STATUS_INDICATORS = {
    "success": "[+] ",
    "error": "[-] ",
    "warning": "[!] ",
    "info": "[*] "
}


def get_status_prefix(status_type: str) -> str:
    """
    Get encoding-safe status prefix for log messages.
    
    Provides consistent status indicators that work across platforms,
    including environments with limited encoding support.
    
    Args:
        status_type: Type of status (success, error, warning, info)
        
    Returns:
        str: Encoding-safe status prefix
    """
    return STATUS_INDICATORS.get(status_type.lower(), "")

================
File: updated_pre_startup.py
================
"""
Pre-Startup Initialization Script (Unicode-Safe Version)

Performs comprehensive application initialization before main application startup,
ensuring proper directory structure, dependency validation, and environment setup.

This script implements proper error handling and logging following system specifications,
with enhanced Unicode-safe logging to ensure compatibility across all platforms.

Usage:
    python pre_startup.py

Returns:
    0 if initialization successful, 1 otherwise
"""

import os
import sys
import logging
import subprocess
import platform
from pathlib import Path
import json
from datetime import datetime
import re
from typing import Dict, List, Optional, Tuple

# Import safe logging utility if available, otherwise define basic version
try:
    from unicode_safe_logging import configure_safe_logging, get_status_prefix
    HAS_SAFE_LOGGING = True
except ImportError:
    HAS_SAFE_LOGGING = False


def setup_basic_logging():
    """Configure basic logging as fallback if safe logging is unavailable."""
    # Ensure logs directory exists
    os.makedirs('logs', exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/initialization.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger("pre_startup")


def setup_logging():
    """Configure comprehensive logging with Unicode safety."""
    # Ensure logs directory exists
    os.makedirs('logs', exist_ok=True)
    
    if HAS_SAFE_LOGGING:
        return configure_safe_logging(
            name="pre_startup",
            level=logging.INFO,
            log_file="logs/initialization.log",
            format_str='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
    else:
        return setup_basic_logging()


def get_safe_prefix(status_type):
    """Get platform-safe status prefix for logging."""
    if HAS_SAFE_LOGGING:
        return get_status_prefix(status_type)
        
    # Fallback simple indicators
    prefixes = {
        "success": "[OK] ",
        "error": "[ERROR] ",
        "warning": "[WARNING] ",
        "info": "[INFO] "
    }
    return prefixes.get(status_type.lower(), "")


def run_directory_setup():
    """
    Run directory setup with proper error handling.
    
    Ensures all required directories exist for application functionality,
    implementing proper error handling and recovery.
    
    Returns:
        bool: True if setup successful, False otherwise
    """
    logger = logging.getLogger("pre_startup.directories")
    logger.info("Setting up required directories")
    
    # Define required directories based on system documentation
    required_directories = [
        # Core system directories
        "logs",                    # Logging directory for all components
        "data",                    # Base data directory
        "data/config",             # Configuration storage
        "data/secure",             # Encrypted data storage
        "data/secure/backups",     # Backup storage for secure data
        "data/metrics",            # Performance metrics storage
        "data/cache",              # Temporary cache storage
    ]
    
    success = True
    created_dirs = []
    
    # Create each directory with proper error handling
    for directory in required_directories:
        try:
            dir_path = Path(directory)
            if not dir_path.exists():
                dir_path.mkdir(parents=True, exist_ok=True)
                logger.info(f"{get_safe_prefix('success')}Created directory: {directory}")
                created_dirs.append(directory)
            else:
                logger.info(f"{get_safe_prefix('info')}Directory already exists: {directory}")
        except Exception as e:
            logger.error(f"{get_safe_prefix('error')}Error creating directory {directory}: {str(e)}")
            success = False
    
    # Summary output
    if created_dirs:
        logger.info(f"Created {len(created_dirs)} directories: {', '.join(created_dirs)}")
    else:
        logger.info("No new directories needed to be created")
        
    return success


def validate_dependencies():
    """
    Validate critical dependencies with version checks.
    
    Implements comprehensive dependency validation, checking
    for known problematic versions and compatibility issues.
    
    Returns:
        bool: True if all dependencies valid, False otherwise
    """
    logger = logging.getLogger("pre_startup.dependencies")
    logger.info("Validating critical dependencies")
    
    # Critical dependencies to validate
    critical_deps = ["bcrypt", "cryptography", "fastapi", "pydantic"]
    all_valid = True
    
    try:
        # Create a subprocess to check installed packages
        result = subprocess.run(
            [sys.executable, "-m", "pip", "list", "--format=json"],
            check=True,
            capture_output=True,
            text=True
        )
        
        # Parse installed packages
        installed_packages = json.loads(result.stdout)
        installed_dict = {pkg["name"].lower(): pkg["version"] for pkg in installed_packages}
        
        # Check critical dependencies
        for dep in critical_deps:
            if dep.lower() in installed_dict:
                logger.info(f"Found {dep} version {installed_dict[dep.lower()]}")
                
                # Specific version checks
                if dep.lower() == "bcrypt":
                    version = installed_dict[dep.lower()]
                    if version.startswith("4.0."):
                        logger.info(f"{get_safe_prefix('success')}{dep} version {version} is compatible")
                    else:
                        logger.warning(f"{get_safe_prefix('warning')}{dep} version {version} may have compatibility issues")
                        logger.warning(f"  Recommended version: 4.0.1")
                        all_valid = False
            else:
                logger.error(f"{get_safe_prefix('error')}Required dependency {dep} not found")
                all_valid = False
                
        if all_valid:
            logger.info(f"{get_safe_prefix('success')}All critical dependencies validated successfully")
        
        return all_valid
        
    except subprocess.CalledProcessError as e:
        logger.error(f"Dependency validation failed: {e.stderr}")
        return False
    except Exception as e:
        logger.error(f"Error validating dependencies: {str(e)}")
        return False


def create_default_configs():
    """
    Create default configuration files if they don't exist.
    
    Implements configuration initialization following system 
    specifications, ensuring required configuration is available.
    
    Returns:
        bool: True if configuration setup successful, False otherwise
    """
    logger = logging.getLogger("pre_startup.config")
    logger.info("Setting up default configurations")
    
    # Ensure config directory exists
    config_dir = Path("data/config")
    config_dir.mkdir(parents=True, exist_ok=True)
    
    # Default configurations to create
    default_configs = {
        "email_settings.json": {
            "batch_size": 50,
            "auto_respond_enabled": True,
            "confidence_threshold": 0.7,
            "processing_interval_minutes": 15,
            "max_tokens_per_analysis": 4000,
            "models": {
                "classification": "llama-3.3-70b-versatile",
                "analysis": "deepseek-reasoner",
                "response": "llama-3.3-70b-versatile"
            }
        }
    }
    
    success = True
    for filename, config in default_configs.items():
        file_path = config_dir / filename
        
        if not file_path.exists():
            try:
                with open(file_path, 'w') as f:
                    json.dump(config, f, indent=2)
                logger.info(f"{get_safe_prefix('success')}Created default configuration: {filename}")
            except Exception as e:
                logger.error(f"{get_safe_prefix('error')}Error creating default configuration {filename}: {str(e)}")
                success = False
        else:
            logger.info(f"{get_safe_prefix('info')}Configuration file already exists: {filename}")
            
    return success


def initialize_secure_storage():
    """
    Initialize secure storage with proper error handling.
    
    Implements secure storage initialization following error handling
    protocols, ensuring critical storage systems are ready.
    
    Returns:
        bool: True if initialization successful, False otherwise
    """
    logger = logging.getLogger("pre_startup.storage")
    logger.info("Initializing secure storage")
    
    try:
        # Ensure secure storage directory exists
        storage_path = Path("data/secure")
        storage_path.mkdir(parents=True, exist_ok=True)
        
        # Placeholder for actual secure storage initialization
        # This would integrate with the SecureStorage class in a real implementation
        
        # For now, just touch required files to prevent startup errors
        required_files = [
            storage_path / ".initialized",
            storage_path / "backups" / ".initialized"
        ]
        
        for file_path in required_files:
            file_path.parent.mkdir(parents=True, exist_ok=True)
            if not file_path.exists():
                file_path.touch()
                logger.info(f"{get_safe_prefix('success')}Created initialization marker: {file_path}")
        
        return True
    except Exception as e:
        logger.error(f"{get_safe_prefix('error')}Error initializing secure storage: {str(e)}")
        return False


def initialize_metrics():
    """
    Initialize metrics storage with proper error handling.
    
    Implements metrics storage initialization following system
    specifications, ensuring metrics systems are available.
    
    Returns:
        bool: True if initialization successful, False otherwise
    """
    logger = logging.getLogger("pre_startup.metrics")
    logger.info("Initializing metrics storage")
    
    try:
        # Ensure metrics directory exists
        metrics_path = Path("data/metrics")
        metrics_path.mkdir(parents=True, exist_ok=True)
        
        # Initialize groq_metrics.json if it doesn't exist
        groq_metrics_file = metrics_path / "groq_metrics.json"
        if not groq_metrics_file.exists():
            default_metrics = {
                "requests": [],
                "errors": [],
                "performance": {
                    "avg_response_time": 0,
                    "total_requests": 0,
                    "success_rate": 100
                },
                "initialized_at": datetime.now().isoformat()
            }
            
            with open(groq_metrics_file, 'w') as f:
                json.dump(default_metrics, f, indent=2)
            logger.info(f"{get_safe_prefix('success')}Created default metrics file: {groq_metrics_file}")
        
        # Initialize email_stats.json if it doesn't exist
        email_stats_file = metrics_path / "email_stats.json"
        if not email_stats_file.exists():
            default_stats = {
                "total_emails_processed": 0,
                "emails_by_category": {
                    "meeting": 0,
                    "needs_review": 0,
                    "not_actionable": 0,
                    "not_meeting": 0
                },
                "average_processing_time_ms": 0,
                "success_rate": 0,
                "stats_period_days": 30,
                "last_updated": datetime.now().isoformat()
            }
            
            with open(email_stats_file, 'w') as f:
                json.dump(default_stats, f, indent=2)
            logger.info(f"{get_safe_prefix('success')}Created default email stats file: {email_stats_file}")
            
        return True
    except Exception as e:
        logger.error(f"{get_safe_prefix('error')}Error initializing metrics storage: {str(e)}")
        return False


def validate_environment():
    """
    Validate environment settings and dependencies.
    
    Implements environment validation following system specifications,
    ensuring required environment variables and settings are available.
    
    Returns:
        bool: True if environment valid, False otherwise
    """
    logger = logging.getLogger("pre_startup.environment")
    logger.info("Validating environment")
    
    # System requirements
    min_python_version = (3, 9)
    
    # Check Python version
    python_version = tuple(map(int, platform.python_version_tuple()[:2]))
    python_valid = python_version >= min_python_version
    
    if python_valid:
        logger.info(f"{get_safe_prefix('success')}Python version: {platform.python_version()}")
    else:
        logger.error(f"{get_safe_prefix('error')}Python version {platform.python_version()} is below minimum {'.'.join(map(str, min_python_version))}")
    
    # Check for .env file
    env_file = Path(".env")
    if env_file.exists():
        logger.info(f"{get_safe_prefix('success')}Found .env file")
        
        # Check for critical environment variables
        critical_env_vars = ["GROQ_API_KEY"]
        missing_vars = []
        
        for var in critical_env_vars:
            if not os.getenv(var):
                missing_vars.append(var)
                
        if missing_vars:
            logger.warning(f"{get_safe_prefix('warning')}Missing critical environment variables: {', '.join(missing_vars)}")
            logger.warning("  Application may fail at runtime if these are required")
    else:
        logger.warning(f"{get_safe_prefix('warning')}No .env file found - environment variables must be set manually")
    
    return python_valid


def main():
    """
    Main initialization sequence for application pre-startup.
    
    Implements comprehensive initialization following system specifications
    and error handling protocols, ensuring proper application startup.
    
    Returns:
        int: 0 for success, 1 for failure
    """
    # Setup logging with encoding-safe formatting
    logger = setup_logging()
    logger.info("=== Starting pre-startup initialization ===")
    
    # Track initialization steps
    steps = [
        {"name": "Run directory setup", "func": run_directory_setup},
        {"name": "Validate dependencies", "func": validate_dependencies},
        {"name": "Create default configurations", "func": create_default_configs},
        {"name": "Initialize secure storage", "func": initialize_secure_storage},
        {"name": "Initialize metrics", "func": initialize_metrics},
        {"name": "Validate environment", "func": validate_environment}
    ]
    
    # Run all initialization steps
    success = True
    for step in steps:
        logger.info(f"Running step: {step['name']}")
        step_success = step["func"]()
        
        if step_success:
            logger.info(f"{get_safe_prefix('success')}Step completed successfully: {step['name']}")
        else:
            logger.error(f"{get_safe_prefix('error')}Step failed: {step['name']}")
            success = False
    
    # Finalize
    if success:
        logger.info("=== Pre-startup initialization completed successfully ===")
        return 0
    else:
        logger.error("=== Pre-startup initialization completed with errors ===")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)



================================================================
End of Codebase
================================================================
